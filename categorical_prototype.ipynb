{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import QuestionAnsweringPipeline, AutoAdapterModel, AutoModelWithHeads, AutoTokenizer, AutoConfig\n",
    "from transformers.onnx import OnnxConfig, validate_model_outputs, export\n",
    "from transformers.models.bert import BertOnnxConfig\n",
    "\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from onnxruntime import InferenceSession\n",
    "import onnxruntime\n",
    "\n",
    "from onnx_opcounter import calculate_params\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_metric, load_dataset\n",
    "\n",
    "from typing import Mapping, OrderedDict\n",
    "from pathlib import Path\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4034.28it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "adapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-boolq\", source=\"hf\")\n",
    "model.active_adapters = adapter_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_model_inference(question, context):\n",
    "    \n",
    "    raw_input = [[context, question]]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_idx = torch.argmax(outputs.logits)\n",
    "\n",
    "    return bool(answer_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset boolq (/Users/michaelhermann/.cache/huggingface/datasets/boolq/default/0.1.0/bf0dd57da941c50de94ae3ce3cef7fea48c08f337a4b7aac484e9dddc5aa24e5)\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"boolq\", split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is house tax and property tax are same\n",
      "Correct answer: True\n",
      "Given answer: True\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 100):\n",
    "    if data[i][\"answer\"] == True:\n",
    "        break\n",
    "# print(f\"using {i}\")\n",
    "test_no = i\n",
    "\n",
    "question = data[test_no][\"question\"]\n",
    "correct_answer = data[test_no][\"answer\"]\n",
    "context = data[test_no][\"passage\"]\n",
    "\n",
    "\n",
    "answer = categorical_model_inference(question, context)\n",
    "\n",
    "print(question)\n",
    "print(f\"Correct answer: {correct_answer}\")\n",
    "print(f\"Given answer: {answer}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 4\n",
      "tensor([[ 1.1532, -1.1786]], grad_fn=<AddmmBackward0>)\n",
      "Answer: False\n",
      "Base Answer: True\n",
      "Question: is there a difference between hydroxyzine hcl and hydroxyzine pam\n",
      "Context: Hydroxyzine preparations require a doctor's prescription. The drug is available in two formulations, the pamoate and the dihydrochloride or hydrochloride salts. Vistaril, Equipose, Masmoran, and Paxistil are preparations of the pamoate salt, while Atarax, Alamon, Aterax, Durrax, Tran-Q, Orgatrax, Quiess, and Tranquizine are of the hydrochloride salt.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    i =  4\n",
    "\n",
    "    question = data[i][\"question\"]\n",
    "    correct_answer = data[i][\"answer\"]\n",
    "    context = data[i][\"passage\"]\n",
    "\n",
    "    raw_input = [[context, question]]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_idx = torch.argmax(outputs.logits)\n",
    "    answer = bool(answer_idx)\n",
    "    \n",
    "    if answer != correct_answer:\n",
    "        print(f\"no: {i}\")\n",
    "        print(outputs.logits)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(f\"Base Answer: {correct_answer}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Context: {context}\")\n",
    "\n",
    "        break  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a little test - base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 out of 200 -> 0.735%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 200\n",
    "\n",
    "for i in range(total):\n",
    "    test_no = i\n",
    "\n",
    "    question = data[test_no][\"question\"]\n",
    "    correct_answer = data[test_no][\"answer\"]\n",
    "    context = data[test_no][\"passage\"]\n",
    "\n",
    "    answer = categorical_model_inference(question, context)\n",
    "    if answer == correct_answer:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"{correct} out of {total} -> {correct/total}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/layer.py:49: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if context.output_adapter_gating_scores:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/composition.py:202: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor is not None and hidden_states.shape[0] != tensor.shape[0]:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/context.py:117: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if getattr(ctx, \"output_\" + attr, False):\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\") # bert-base-uncased-pf-boolq\",\n",
    "onnx_config = BertOnnxConfig(config)\n",
    "\n",
    "onnx_path = Path(\"onnx/boolq/model.onnx\")\n",
    "\n",
    "onnx_inputs, onnx_outputs = export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_path)\n",
    "\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnx_inference(onnx_model, question, context):\n",
    "\n",
    "    inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors=\"np\")\n",
    "    inputs = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\n",
    "\n",
    "    outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\n",
    "\n",
    "    return bool(np.argmax(outputs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"onnx/boolq/model.onnx\"\n",
    "onnx_model  = onnxruntime.InferenceSession(\n",
    "        str(onnx_path), providers=[\"CPUExecutionProvider\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Property tax or 'house tax' is a local tax on buildings, along with appurtenant land. It is and imposed on the Possessor (not the custodian of property as per 1978, 44th amendment of constitution). It resembles the US-type wealth tax and differs from the excise-type UK rate. The tax power is vested in the states and is delegated to local bodies, specifying the valuation method, rate band, and collection procedures. The tax base is the annual rental value (ARV) or area-based rating. Owner-occupied and other properties not producing rent are assessed on cost and then converted into ARV by applying a percentage of cost, usually four percent. Vacant land is generally exempt. Central government properties are exempt. Instead a 'service charge' is permissible under executive order. Properties of foreign missions also enjoy tax exemption without requiring reciprocity. The tax is usually accompanied by service taxes, e.g., water tax, drainage tax, conservancy (sanitation) tax, lighting tax, all using the same tax base. The rate structure is flat on rural (panchayat) properties, but in the urban (municipal) areas it is mildly progressive with about 80% of assessments falling in the first two brackets.\"\n",
    "question = \"is house tax and property tax are same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "answer = onnx_inference(onnx_model, question, context)\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 4\n",
      "[ 1.3443897 -1.2073131]\n",
      "Answer: False\n",
      "Base Answer: True\n",
      "Question: is there a difference between hydroxyzine hcl and hydroxyzine pam\n",
      "Context: Hydroxyzine preparations require a doctor's prescription. The drug is available in two formulations, the pamoate and the dihydrochloride or hydrochloride salts. Vistaril, Equipose, Masmoran, and Paxistil are preparations of the pamoate salt, while Atarax, Alamon, Aterax, Durrax, Tran-Q, Orgatrax, Quiess, and Tranquizine are of the hydrochloride salt.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "\n",
    "    question = data[i][\"question\"]\n",
    "    correct_answer = data[i][\"answer\"]\n",
    "    context = data[i][\"passage\"]\n",
    "\n",
    "    raw_input = [[context, question]]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "    inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors=\"np\")\n",
    "    inputs = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\n",
    "\n",
    "    outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\n",
    "    output_prep = outputs[0][0]\n",
    "    answer = bool(np.argmax(output_prep))\n",
    "    \n",
    "    if answer != correct_answer:\n",
    "        print(f\"no: {i}\")\n",
    "        print(output_prep)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(f\"Base Answer: {correct_answer}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Context: {context}\")\n",
    "\n",
    "        break "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a little test - onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 out of 200 -> 0.635%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 200\n",
    "\n",
    "for i in range(total):\n",
    "    test_no = i\n",
    "\n",
    "    question = data[test_no][\"question\"]\n",
    "    correct_answer = data[test_no][\"answer\"]\n",
    "    context = data[test_no][\"passage\"]\n",
    "\n",
    "    answer = onnx_inference(onnx_model, question, context)\n",
    "    if answer == correct_answer:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"{correct} out of {total} -> {correct/total}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare base and onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_base = []\n",
    "list_onnx = []\n",
    "list_correct = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_no in range(total):\n",
    "\n",
    "    question = data[test_no][\"question\"]\n",
    "    correct_answer = data[test_no][\"answer\"]\n",
    "    context = data[test_no][\"passage\"]\n",
    "\n",
    "    answer_base = categorical_model_inference(question, context)\n",
    "    answer_onnx = onnx_inference(onnx_model, question, context)\n",
    "    \n",
    "    list_correct.append(correct_answer)\n",
    "    list_base.append(answer_base)\n",
    "    list_onnx.append(answer_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_indexes = []\n",
    "for i in range(len(list_correct)):\n",
    "    if list_base[i] != list_onnx[i]:\n",
    "        diff_indexes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 0\n",
      "Base: tensor([[0.0363, 0.1560]], grad_fn=<AddmmBackward0>)\n",
      "Onnx: [ 1.0141207  -0.70817924]\n",
      "Base answer: True\n",
      "Onnx answer: False\n",
      "Correct Answer: False\n",
      "no: 5\n",
      "Base: tensor([[-0.3566,  0.5935]], grad_fn=<AddmmBackward0>)\n",
      "Onnx: [ 0.48343384 -0.32177168]\n",
      "Base answer: True\n",
      "Onnx answer: False\n",
      "Correct Answer: False\n",
      "no: 7\n",
      "Base: tensor([[-1.6825,  1.6893]], grad_fn=<AddmmBackward0>)\n",
      "Onnx: [ 0.69819057 -0.52735263]\n",
      "Base answer: True\n",
      "Onnx answer: False\n",
      "Correct Answer: True\n",
      "no: 9\n",
      "Base: tensor([[-2.6897,  2.7745]], grad_fn=<AddmmBackward0>)\n",
      "Onnx: [ 0.15282367 -0.0086502 ]\n",
      "Base answer: True\n",
      "Onnx answer: False\n",
      "Correct Answer: True\n",
      "no: 10\n",
      "Base: tensor([[-0.7401,  0.9367]], grad_fn=<AddmmBackward0>)\n",
      "Onnx: [0.17055601 0.15920028]\n",
      "Base answer: True\n",
      "Onnx answer: False\n",
      "Correct Answer: True\n",
      "no: 11\n",
      "Base: tensor([[ 2.2703, -2.2002]], grad_fn=<AddmmBackward0>)\n",
      "Onnx: [-1.0790286  1.2619745]\n",
      "Base answer: False\n",
      "Onnx answer: True\n",
      "Correct Answer: False\n",
      "no: 15\n",
      "Base: tensor([[ 0.5078, -0.3660]], grad_fn=<AddmmBackward0>)\n",
      "Onnx: [-1.0672904  1.2439492]\n",
      "Base answer: False\n",
      "Onnx answer: True\n",
      "Correct Answer: True\n",
      "no: 22\n",
      "Base: tensor([[ 0.5552, -0.5039]], grad_fn=<AddmmBackward0>)\n",
      "Onnx: [0.06004597 0.10225756]\n",
      "Base answer: False\n",
      "Onnx answer: True\n",
      "Correct Answer: True\n",
      "no: 24\n",
      "Base: tensor([[-0.5260,  0.5809]], grad_fn=<AddmmBackward0>)\n",
      "Onnx: [ 0.7348168  -0.52081984]\n",
      "Base answer: True\n",
      "Onnx answer: False\n",
      "Correct Answer: True\n",
      "no: 36\n",
      "Base: tensor([[ 1.1273, -0.8541]], grad_fn=<AddmmBackward0>)\n",
      "Onnx: [-0.09191322  0.36754894]\n",
      "Base answer: False\n",
      "Onnx answer: True\n",
      "Correct Answer: False\n"
     ]
    }
   ],
   "source": [
    "for ind in range(len(diff_indexes[:10])):\n",
    "\n",
    "    i = diff_indexes[ind]\n",
    "\n",
    "    question = data[i][\"question\"]\n",
    "    correct_answer = data[i][\"answer\"]\n",
    "    context = data[i][\"passage\"]\n",
    "\n",
    "    #base \n",
    "    raw_input = [[context, question]]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs_base = model(**inputs)\n",
    "    answer_idx = torch.argmax(outputs_base.logits)\n",
    "    answer_base = bool(answer_idx)\n",
    "\n",
    "    #onnx\n",
    "    raw_input = [[context, question]]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors=\"np\")\n",
    "    inputs = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\n",
    "\n",
    "    outputs_onnx = onnx_model.run(input_feed=dict(inputs), output_names=None)\n",
    "    output_prep = outputs_onnx[0][0]\n",
    "    answer_onnx = bool(np.argmax(output_prep))\n",
    "\n",
    "    \n",
    "    print(f\"no: {i}\")\n",
    "    print(f\"Base: {outputs_base.logits}\")\n",
    "    print(f\"Onnx: {output_prep}\")\n",
    "    print(f\"Base answer: {answer_base}\")\n",
    "    print(f\"Onnx answer: {answer_onnx}\")\n",
    "    print(f\"Correct Answer: {correct_answer}\")\n",
    "    # print(f\"Question: {question}\")\n",
    "    # print(f\"Context: {context}\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 24,\n",
       " 36,\n",
       " 41,\n",
       " 42,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 49,\n",
       " 56,\n",
       " 71,\n",
       " 77,\n",
       " 79,\n",
       " 85,\n",
       " 94,\n",
       " 95,\n",
       " 98,\n",
       " 99]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapterhub_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dff476688330005cfc33f1ee0f15c13ae533c265ccd041ab146cdb98ecc6219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
