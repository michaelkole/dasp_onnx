{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/model_sharing\n",
    "# https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort\n",
    "# https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModelForQuestionAnswering\n",
    "\n",
    "# https://github.com/huggingface/optimum/blob/ed95b9fa8019af29ce1904ac3cfef8729eb4f4be/optimum/modeling_base.py#L12\n",
    "# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/hf_api.py#L1458\n",
    "# https://github.com/huggingface/huggingface_hub/blob/664cfdd25adfb69f429decf19e2d65ed5599f9fd/src/huggingface_hub/utils/_deprecation.py#L7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Mapping, OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import HfApi, HfFolder, hf_hub_download\n",
    "from transformers import AutoModelWithHeads, AutoTokenizer, AutoConfig\n",
    "from transformers.onnx import OnnxConfig, export\n",
    "\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers.models.bert import BertOnnxConfig\n",
    "from transformers.models.roberta import RobertaOnnxConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hub(save_dir, repository_id):\n",
    "    huggingface_token = HfFolder.get_token()\n",
    "    api = HfApi()\n",
    "\n",
    "    api.create_repo(\n",
    "        token=huggingface_token,\n",
    "        repo_id=f'UKP-SQuARE/{repository_id}',\n",
    "        exist_ok=True,\n",
    "        private=False\n",
    "    )\n",
    "\n",
    "    for path, subdirs, files in os.walk(save_dir):\n",
    "        for name in files:\n",
    "            local_file_path = os.path.join(path, name)\n",
    "            _, hub_file_path = os.path.split(local_file_path)\n",
    "            try:\n",
    "                api.upload_file(\n",
    "                    token=huggingface_token,\n",
    "                    repo_id=f\"UKP-SQuARE/{repository_id}\",\n",
    "                    path_or_fileobj=os.path.join(os.getcwd(), local_file_path),\n",
    "                    path_in_repo=hub_file_path,\n",
    "                )\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except NameError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readme(directory_path, base_model, adapter):\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "\n",
    "    readme_path = hf_hub_download(repo_id=adapter, filename=\"README.md\")\n",
    "\n",
    "    onnx_readme = \"{}/README.md\".format(directory_path)\n",
    "\n",
    "    skip = False\n",
    "    with open(readme_path, 'r') as src, open(onnx_readme, 'w') as dst:\n",
    "        for line in src:\n",
    "            # Insert onnx tag\n",
    "            if line == 'tags:\\n':\n",
    "                dst.write(line)\n",
    "                dst.write('- onnx\\n')\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"# Adapter\"):\n",
    "                skip = True\n",
    "\n",
    "                # Insert custom README\n",
    "                dst.write(\"# ONNX export of \" + line[2:])\n",
    "                dst.write(f\"## Conversion of [{adapter}](https://huggingface.co/{adapter}) for UKP SQuARE\\n\\n\\n\")\n",
    "                dst.write(\"## Usage\\n\")\n",
    "                dst.write(\"```python\\n\")\n",
    "                dst.write(f\"onnx_path = hf_hub_download(repo_id='UKP-SQuARE/{model_id}', filename='model.onnx') # or model_quant.onnx for quantization\\n\")\n",
    "                dst.write(\"onnx_model = InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\\n\\n\")\n",
    "                dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('{base_model}')\\n\\n\")\n",
    "                dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                dst.write(\"inputs_int64 = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\\n\")\n",
    "                dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs_int64), output_names=None)\\n\")\n",
    "                dst.write(\"```\\n\\n\")\n",
    "\n",
    "            # Continue with normal model card\n",
    "            if line.startswith(\"## Architecture & Training\"): \n",
    "                skip = False\n",
    "\n",
    "            if not skip: \n",
    "                dst.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOnnxConfig(OnnxConfig):\n",
    "    # Inspired by BertONNXConfig, can be extended to support other QA tasks\n",
    "    @property\n",
    "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "        if self.task == \"multiple-choice\":\n",
    "            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n",
    "        else:\n",
    "            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n",
    "\n",
    "        return OrderedDict(\n",
    "                [\n",
    "                    (\"input_ids\", dynamic_axis),\n",
    "                    (\"attention_mask\", dynamic_axis),\n",
    "                    (\"token_type_ids\", dynamic_axis), # Roberta doesn't use this\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "def onnx_export(base_model, adapter_id, quantize_model=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    model = AutoModelWithHeads.from_pretrained(base_model)\n",
    "    \n",
    "    adapter = f\"AdapterHub/{base_model}-pf-{adapter_id}\"\n",
    "    adapter_name = model.load_adapter(adapter, source=\"hf\")\n",
    "    model.active_adapters = adapter_name\n",
    "\n",
    "    config = AutoConfig.from_pretrained(base_model)\n",
    "\n",
    "    if base_model.startswith(\"bert\"):\n",
    "        onnx_config = BertOnnxConfig(config, task=\"question-answering\")\n",
    "    elif base_model.startswith(\"roberta\"):\n",
    "        onnx_config = RobertaOnnxConfig(config, task=\"question-answering\")\n",
    "    else:\n",
    "        onnx_config = CustomOnnxConfig(config, task=\"question-answering\")\n",
    "        \n",
    "\n",
    "    # Generate the local directory in onnx/\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "    directory_path = Path(\"onnx/{}\".format(model_id))\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_model_path = Path(\"{}/model.onnx\".format(directory_path))\n",
    "\n",
    "    # Export ONNX model\n",
    "    export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_model_path)\n",
    "\n",
    "    # Create config.json of vanilla model\n",
    "    model.save_pretrained(directory_path)\n",
    "    os.remove(directory_path / \"pytorch_model.bin\")\n",
    "\n",
    "    # Create README.md\n",
    "    generate_readme(directory_path, base_model, adapter)\n",
    "\n",
    "    if quantize_model:\n",
    "        quantized_model_path = \"{}/model_quant.onnx\".format(directory_path)\n",
    "        quantize_dynamic(onnx_model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "\n",
    "    print(\"Uploading model to hub... (may take a few minutes)\")\n",
    "    push_to_hub(\n",
    "        save_dir = directory_path,\n",
    "        repository_id = model_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting: bert-base-uncased drop\n",
      "Uploading model to hub... (may take a few minutes)\n",
      "Exporting: roberta-base drop\n",
      "Uploading model to hub... (may take a few minutes)\n",
      "Exporting: bert-base-uncased hotpotqa\n",
      "Uploading model to hub... (may take a few minutes)\n"
     ]
    }
   ],
   "source": [
    "skills = pd.read_csv('square_skills/extractive_qa_skills.csv')[0:3]\n",
    "\n",
    "for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "    print(\"Exporting: {} {}\".format(reader, adapter))\n",
    "    onnx_export(reader, adapter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('adapter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a7f5a2c07603db35bc4e52cfd5b475adbf202ae824ea4c5e531d495460257f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
