{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/model_sharing\n",
    "# https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort\n",
    "# https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModelForQuestionAnswering\n",
    "\n",
    "# https://github.com/huggingface/optimum/blob/ed95b9fa8019af29ce1904ac3cfef8729eb4f4be/optimum/modeling_base.py#L12\n",
    "# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/hf_api.py#L1458\n",
    "# https://github.com/huggingface/huggingface_hub/blob/664cfdd25adfb69f429decf19e2d65ed5599f9fd/src/huggingface_hub/utils/_deprecation.py#L7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "# from onnxruntime import InferenceSession\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering, ORTModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_id = \"midav\" # square?\n",
    "onnx_model_name = \"model\"\n",
    "onnx_path = \"onnx/dropbert\"\n",
    "onnx_file = f\"{onnx_path}/{onnx_model_name}.onnx\"\n",
    "\n",
    "base_model = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 161kB/s]\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 12.0kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 528kB/s] \n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 851kB/s] \n",
      "Downloading: 100%|██████████| 440M/440M [01:03<00:00, 6.97MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ORTModelForQuestionAnswering.from_pretrained(base_model,from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('onnx/test/tokenizer_config.json',\n",
       " 'onnx/test/special_tokens_map.json',\n",
       " 'onnx/test/vocab.txt',\n",
       " 'onnx/test/added_tokens.json',\n",
       " 'onnx/test/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"onnx/test\")\n",
    "tokenizer.save_pretrained(\"onnx/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = \"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\"\n",
    "# import huggingface_hub\n",
    "# huggingface_hub.login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '62d9c1b8cfed764363b3ac11', 'name': 'midav', 'fullname': 'Michael Hermann', 'email': 'michael.koles@googlemail.com', 'emailVerified': True, 'plan': 'NO_PLAN', 'canPay': False, 'periodEnd': None, 'avatarUrl': '/avatars/dea04f3d857c8d3e8be2b0d65870df44.svg', 'orgs': []}\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import HfApi, HfFolder, hf_hub_download\n",
    "# huggingface_token = HfFolder.get_token()\n",
    "# api = HfApi()\n",
    "\n",
    "# user = api.whoami(huggingface_token)\n",
    "# print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import create_repo\n",
    "# create_repo(\"midav/testbsf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_onnx_model(onnx_path):\n",
    "#     return InferenceSession(\n",
    "#         str(onnx_path), providers=[\"CPUExecutionProvider\"]\n",
    "#     )\n",
    "# onnx_model = get_onnx_model(onnx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_repo() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [55], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# onnx_model.push_to_hub(\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#     f\"{onnx_path}\",\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#     repository_id= f\"{organization_id}\",\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#     use_auth_token=True\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model\u001b[39m.\u001b[39;49mpush_to_hub(\n\u001b[1;32m      8\u001b[0m     save_directory \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39monnx/test\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     repository_id \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtesting\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     private \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     11\u001b[0m     use_auth_token \u001b[39m=\u001b[39;49m token\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/optimum/modeling_base.py:133\u001b[0m, in \u001b[0;36mOptimizedModel.push_to_hub\u001b[0;34m(self, save_directory, repository_id, private, use_auth_token)\u001b[0m\n\u001b[1;32m    130\u001b[0m user \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mwhoami(huggingface_token)\n\u001b[1;32m    131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgit_config_username_and_email(git_email\u001b[39m=\u001b[39muser[\u001b[39m\"\u001b[39m\u001b[39memail\u001b[39m\u001b[39m\"\u001b[39m], git_user\u001b[39m=\u001b[39muser[\u001b[39m\"\u001b[39m\u001b[39mfullname\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 133\u001b[0m api\u001b[39m.\u001b[39;49mcreate_repo(\n\u001b[1;32m    134\u001b[0m     token\u001b[39m=\u001b[39;49mhuggingface_token,\n\u001b[1;32m    135\u001b[0m     name\u001b[39m=\u001b[39;49mrepository_id,\n\u001b[1;32m    136\u001b[0m     organization\u001b[39m=\u001b[39;49muser[\u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    137\u001b[0m     exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    138\u001b[0m     private\u001b[39m=\u001b[39;49mprivate,\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    140\u001b[0m \u001b[39mfor\u001b[39;00m path, subdirs, files \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mwalk(save_directory):\n\u001b[1;32m    141\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m files:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:31\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     32\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     33\u001b[0m args_msg \u001b[39m=\u001b[39m [\n\u001b[1;32m     34\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00marg\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00marg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m     \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[39m-\u001b[39mextra_args:])\n\u001b[1;32m     36\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: create_repo() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "source": [
    "# onnx_model.push_to_hub(\n",
    "#     f\"{onnx_path}\",\n",
    "#     repository_id= f\"{organization_id}\",\n",
    "#     use_auth_token=True\n",
    "# )\n",
    "\n",
    "model.push_to_hub(\n",
    "    save_directory = \"onnx/test\",\n",
    "    repository_id = \"testing\",\n",
    "    # private = False,\n",
    "    # use_auth_token = token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines https://huggingface.co/docs/optimum/onnxruntime/usage_guides/pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification, ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from optimum.pipelines import pipeline\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:213: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    }
   ],
   "source": [
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "save_dir = \"tmp/onnx/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('tmp/onnx')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
    "quantizer = ORTQuantizer.from_pretrained(model)\n",
    "# Apply dynamic quantization and save the resulting model\n",
    "quantizer.quantize(save_dir=save_dir, quantization_config=qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "config.json not found in tmp/onnx/ local folder",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m ORTModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(save_dir, file_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodel_quantized.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/optimum/onnxruntime/modeling_ort.py:269\u001b[0m, in \u001b[0;36mORTModel.from_pretrained\u001b[0;34m(cls, model_id, from_transformers, force_download, use_auth_token, cache_dir, subfolder, provider, session_options, provider_options, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[39m@add_start_docstrings\u001b[39m(FROM_PRETRAINED_START_DOCSTRING)\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    255\u001b[0m ):\n\u001b[1;32m    256\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m    provider (`str`, *optional*):\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m        ONNX Runtime providers to use for loading the model. See https://onnxruntime.ai/docs/execution-providers/ for\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39m        `ORTModel`: The loaded ORTModel model.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    270\u001b[0m         model_id,\n\u001b[1;32m    271\u001b[0m         from_transformers,\n\u001b[1;32m    272\u001b[0m         force_download,\n\u001b[1;32m    273\u001b[0m         use_auth_token,\n\u001b[1;32m    274\u001b[0m         cache_dir,\n\u001b[1;32m    275\u001b[0m         subfolder,\n\u001b[1;32m    276\u001b[0m         provider\u001b[39m=\u001b[39;49mprovider,\n\u001b[1;32m    277\u001b[0m         session_options\u001b[39m=\u001b[39;49msession_options,\n\u001b[1;32m    278\u001b[0m         provider_options\u001b[39m=\u001b[39;49mprovider_options,\n\u001b[1;32m    279\u001b[0m         \u001b[39m*\u001b[39;49margs,\n\u001b[1;32m    280\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    281\u001b[0m     )\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/optimum/modeling_base.py:225\u001b[0m, in \u001b[0;36mOptimizedModel.from_pretrained\u001b[0;34m(cls, model_id, from_transformers, force_download, use_auth_token, cache_dir, subfolder, **model_kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    222\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfig.json not found in the specified subfolder \u001b[39m\u001b[39m{\u001b[39;00msubfolder\u001b[39m}\u001b[39;00m\u001b[39m. Using the top level config.json.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m         )\n\u001b[1;32m    224\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfig.json not found in \u001b[39m\u001b[39m{\u001b[39;00mmodel_id\u001b[39m}\u001b[39;00m\u001b[39m local folder\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    226\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: config.json not found in tmp/onnx/ local folder"
     ]
    }
   ],
   "source": [
    "model = ORTModelForSequenceClassification.from_pretrained(save_dir, file_name=\"model_quantized.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:213: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9973195195198059}]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_repo() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [64], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m tokenizer\u001b[39m.\u001b[39msave_pretrained(\u001b[39m\"\u001b[39m\u001b[39mnew_path_for_directory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m\"\u001b[39m\u001b[39mnew_path_for_directory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m model\u001b[39m.\u001b[39;49mpush_to_hub(\u001b[39m\"\u001b[39;49m\u001b[39mnew_path_for_directory\u001b[39;49m\u001b[39m\"\u001b[39;49m, repository_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmy-onnx-repo\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_auth_token\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/optimum/modeling_base.py:133\u001b[0m, in \u001b[0;36mOptimizedModel.push_to_hub\u001b[0;34m(self, save_directory, repository_id, private, use_auth_token)\u001b[0m\n\u001b[1;32m    130\u001b[0m user \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mwhoami(huggingface_token)\n\u001b[1;32m    131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgit_config_username_and_email(git_email\u001b[39m=\u001b[39muser[\u001b[39m\"\u001b[39m\u001b[39memail\u001b[39m\u001b[39m\"\u001b[39m], git_user\u001b[39m=\u001b[39muser[\u001b[39m\"\u001b[39m\u001b[39mfullname\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 133\u001b[0m api\u001b[39m.\u001b[39;49mcreate_repo(\n\u001b[1;32m    134\u001b[0m     token\u001b[39m=\u001b[39;49mhuggingface_token,\n\u001b[1;32m    135\u001b[0m     name\u001b[39m=\u001b[39;49mrepository_id,\n\u001b[1;32m    136\u001b[0m     organization\u001b[39m=\u001b[39;49muser[\u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    137\u001b[0m     exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    138\u001b[0m     private\u001b[39m=\u001b[39;49mprivate,\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    140\u001b[0m \u001b[39mfor\u001b[39;00m path, subdirs, files \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mwalk(save_directory):\n\u001b[1;32m    141\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m files:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:31\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     32\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     33\u001b[0m args_msg \u001b[39m=\u001b[39m [\n\u001b[1;32m     34\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00marg\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00marg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m     \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[39m-\u001b[39mextra_args:])\n\u001b[1;32m     36\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: create_repo() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification, ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "from optimum.pipelines import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "save_dir = \"tmp/onnx/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)\n",
    "\n",
    "optimization_config = OptimizationConfig(optimization_level=2)\n",
    "optimizer = ORTOptimizer.from_pretrained(model)\n",
    "optimizer.optimize(save_dir=save_dir, optimization_config=optimization_config)\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(save_dir, file_name=\"model_optimized.onnx\")\n",
    "\n",
    "onnx_clx = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "text = \"I like the new ORT pipeline\"\n",
    "pred = onnx_clx(text)\n",
    "print(pred)\n",
    "\n",
    "tokenizer.save_pretrained(\"new_path_for_directory\")\n",
    "model.save_pretrained(\"new_path_for_directory\")\n",
    "model.push_to_hub(\"new_path_for_directory\", repository_id=\"my-onnx-repo\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('adapterhub_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dff476688330005cfc33f1ee0f15c13ae533c265ccd041ab146cdb98ecc6219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
