{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/model_sharing\n",
    "# https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort\n",
    "# https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModelForQuestionAnswering\n",
    "\n",
    "# https://github.com/huggingface/optimum/blob/ed95b9fa8019af29ce1904ac3cfef8729eb4f4be/optimum/modeling_base.py#L12\n",
    "# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/hf_api.py#L1458\n",
    "# https://github.com/huggingface/huggingface_hub/blob/664cfdd25adfb69f429decf19e2d65ed5599f9fd/src/huggingface_hub/utils/_deprecation.py#L7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Mapping, OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import HfApi, HfFolder, hf_hub_download\n",
    "from transformers import AutoModelWithHeads, AutoTokenizer, AutoConfig\n",
    "from transformers.onnx import OnnxConfig, export\n",
    "\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers.models.bert import BertOnnxConfig\n",
    "from transformers.models.roberta import RobertaOnnxConfig\n",
    "from transformers.models.bart import BartOnnxConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hub(save_dir, repository_id):\n",
    "    huggingface_token = HfFolder.get_token()\n",
    "    api = HfApi()\n",
    "\n",
    "    api.create_repo(\n",
    "        token=huggingface_token,\n",
    "        repo_id=f'UKP-SQuARE/{repository_id}',\n",
    "        exist_ok=True,\n",
    "        private=False\n",
    "    )\n",
    "\n",
    "    for path, subdirs, files in os.walk(save_dir):\n",
    "        for name in files:\n",
    "            local_file_path = os.path.join(path, name)\n",
    "            _, hub_file_path = os.path.split(local_file_path)\n",
    "            try:\n",
    "                api.upload_file(\n",
    "                    token=huggingface_token,\n",
    "                    repo_id=f\"UKP-SQuARE/{repository_id}\",\n",
    "                    path_or_fileobj=os.path.join(os.getcwd(), local_file_path),\n",
    "                    path_in_repo=hub_file_path,\n",
    "                )\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except NameError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readme(directory_path, base_model, adapter, skill):\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "\n",
    "    readme_path = hf_hub_download(repo_id=adapter, filename=\"README.md\")\n",
    "\n",
    "    onnx_readme = \"{}/README.md\".format(directory_path)\n",
    "\n",
    "    skip = False\n",
    "    with open(readme_path, 'r') as src, open(onnx_readme, 'w') as dst:\n",
    "        for line in src:\n",
    "            # Insert onnx tag\n",
    "            if line == 'tags:\\n':\n",
    "                dst.write(\"inference: false\\n\")\n",
    "                dst.write(line)\n",
    "                dst.write('- onnx\\n')\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"# Adapter\"):\n",
    "                skip = True\n",
    "\n",
    "                # Insert custom README\n",
    "                dst.write(\"# ONNX export of \" + line[2:])\n",
    "                dst.write(f\"## Conversion of [{adapter}](https://huggingface.co/{adapter}) for UKP SQuARE\\n\\n\\n\")\n",
    "                dst.write(\"## Usage\\n\")\n",
    "                dst.write(\"```python\\n\")\n",
    "                dst.write(f\"onnx_path = hf_hub_download(repo_id='UKP-SQuARE/{model_id}', filename='model.onnx') # or model_quant.onnx for quantization\\n\")\n",
    "                dst.write(\"onnx_model = InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\\n\\n\")\n",
    "\n",
    "                if (skill == \"span-extraction\"):\n",
    "                    dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                    dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                    dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                    dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                    dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "                    dst.write(\"```\\n\\n\")\n",
    "\n",
    "                elif (skill == \"categorical\"):\n",
    "                    dst.write(\"context = 'English orthography typically represents vowel sounds with the five conventional vowel letters ⟨a, e, i, o, u⟩, as well as ⟨y⟩, which may also be a consonant depending on context. However, outside of abbreviations, there are a handful of words in English that do not have vowels, either because the vowel sounds are not written with vowel letters or because the words themselves are pronounced without vowel sounds'.\\n\")\n",
    "                    dst.write(\"question = 'can there be a word without a vowel'\\n\")\n",
    "                    dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                    dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                    dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "                    dst.write(\"```\\n\\n\")\n",
    "\n",
    "                elif skill == \"multiple-choice\":\n",
    "                    dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                    dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                    dst.write('choices = [\"Cat\", \"Horse\", \"Tiger\", \"Fish\"]')\n",
    "\n",
    "                    dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "\n",
    "                    dst.write(\"raw_input = [[context, question + \" \" + choice] for choice in choices]\\n\")\n",
    "                    dst.write('inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"np\")\\n')\n",
    "\n",
    "                    dst.write(\"inputs['token_type_ids'] = np.expand_dims(inputs['token_type_ids'], axis=0)\\n\")\n",
    "                    dst.write(\"inputs['input_ids'] =  np.expand_dims(inputs['input_ids'], axis=0)\\n\")\n",
    "                    dst.write(\"inputs['attention_mask'] =  np.expand_dims(inputs['attention_mask'], axis=0)\\n\")\n",
    "                    dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "\n",
    "                    dst.write(\"```\\n\\n\")\n",
    "\n",
    "                elif skill == \"abstractive\":\n",
    "                    dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                    dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                    dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                    dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                    dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "                    dst.write(\"```\\n\\n\")\n",
    "\n",
    "\n",
    "            # Continue with normal model card\n",
    "            if line.startswith(\"## Architecture & Training\"): \n",
    "                skip = False\n",
    "\n",
    "            if not skip: \n",
    "                dst.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOnnxConfig(OnnxConfig):\n",
    "    # Inspired by BertONNXConfig, can be extended to support other QA tasks\n",
    "    @property\n",
    "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "        if self.task == \"multiple-choice\":\n",
    "            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n",
    "        else:\n",
    "            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n",
    "\n",
    "        return OrderedDict(\n",
    "                [\n",
    "                    (\"input_ids\", dynamic_axis),\n",
    "                    (\"attention_mask\", dynamic_axis),\n",
    "                    (\"token_type_ids\", dynamic_axis), # Roberta doesn't use this\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "def onnx_export(base_model, adapter_id, skill, quantize_model=True):\n",
    "    if adapter_id == \"narrativeqa\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "        model = AutoModelWithHeads.from_pretrained(\"facebook/bart-base\")\n",
    "        adapter_name = model.load_adapter(\"AdapterHub/narrativeqa\", source=\"hf\", set_active=True)\n",
    "        model.set_active_adapters(adapter_name)\n",
    "        adapter = \"AdapterHub/narrativeqa\"\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\"facebook/bart-base\")\n",
    "    else:\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "        model = AutoModelWithHeads.from_pretrained(base_model)\n",
    "        adapter = f\"AdapterHub/{base_model}-pf-{adapter_id}\"\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(base_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if (skill == \"span-extraction\") | (skill == \"categorical\"):\n",
    "        if base_model.startswith(\"bert\"):\n",
    "            onnx_config = BertOnnxConfig(config)\n",
    "        elif base_model.startswith(\"roberta\"):\n",
    "            onnx_config = RobertaOnnxConfig(config)\n",
    "        else:\n",
    "            onnx_config = CustomOnnxConfig(config)\n",
    "            \n",
    "    elif skill == \"multiple-choice\":\n",
    "        if base_model.startswith(\"bert\"):\n",
    "            onnx_config = BertOnnxConfig(config, task=\"multiple-choice\")\n",
    "        elif base_model.startswith(\"roberta\"):\n",
    "            onnx_config = RobertaOnnxConfig(config, task=\"multiple-choice\")\n",
    "        else:\n",
    "            onnx_config = CustomOnnxConfig(config, task=\"multiple-choice\")\n",
    "            \n",
    "    elif skill == \"abstractive\":\n",
    "        if base_model.startswith(\"facebook/bart-base\") or base_model.startswith(\"bart\"):\n",
    "            onnx_config = BartOnnxConfig(config, task=\"causal-lm\")\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only BART is supported for abstractive qa\")\n",
    "\n",
    "    \n",
    "    # Generate the local directory in onnx/\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "    directory_path = Path(\"onnx/{}\".format(model_id))\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_model_path = Path(\"{}/model.onnx\".format(directory_path))\n",
    "\n",
    "    # Export ONNX model\n",
    "    export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_model_path)\n",
    "\n",
    "\n",
    "    # Create config.json of vanilla model\n",
    "    model.save_pretrained(directory_path)\n",
    "    os.remove(directory_path / \"pytorch_model.bin\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(directory_path)\n",
    "\n",
    "    # Create README.md\n",
    "    generate_readme(directory_path, base_model, adapter, skill)\n",
    "\n",
    "    if quantize_model:\n",
    "        quantized_model_path = \"{}/model_quant.onnx\".format(directory_path)\n",
    "        quantize_dynamic(onnx_model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "\n",
    "    print(\"Uploading model to hub... (may take a few minutes)\")\n",
    "    push_to_hub(\n",
    "        save_dir = directory_path,\n",
    "        repository_id = model_id,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Models and Uplaoding them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting abstractive\n",
      "_________________________\n",
      "\n",
      "Exporting: bart-base narrativeqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bart/adapter_model.py:246: FutureWarning: This class has been renamed to `BartAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bart/adapter_model.py:224: FutureWarning: This class has been renamed to `BartAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3016.40it/s]\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:250: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:257: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:289: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/layer.py:49: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if context.output_adapter_gating_scores:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/composition.py:202: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor is not None and hidden_states.shape[0] != tensor.shape[0]:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:944: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:107: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/context.py:117: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if getattr(ctx, \"output_\" + attr, False):\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bart/adapter_model.py:95: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_ids is not None and x.shape[1] == input_ids.shape[1]:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bart/adapter_model.py:98: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  if len(torch.unique(eos_mask.sum(1))) > 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.0/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.0/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.0/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.0/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.1/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.1/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.2/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.2/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.3/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.3/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.4/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.4/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.5/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.5/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.0/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.0/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.1/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.1/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.1/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.1/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.2/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.2/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.2/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.2/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.3/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.3/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.3/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.3/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.4/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.4/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.4/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.4/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.5/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.5/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.5/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.5/encoder_attn/MatMul_1]\n",
      "Uploading model to hub... (may take a few minutes)\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://huggingface.co/api/models/UKP-SQuARE/narrativeqa-onnx/commit/main (Request ID: Root=1-63c7d840-4959f0a56726503558a116b8)\n\nForbidden: pass `create_pr=1` as a query parameter to create a Pull Request",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:239\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    240\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/models/UKP-SQuARE/narrativeqa-onnx/commit/main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m available_adapters) \u001b[39mor\u001b[39;00m (adapter \u001b[39min\u001b[39;00m available_adapters): \u001b[39m#if no adapter is specified -> loop over all. OR: is some are specified-> one do these. \u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExporting: \u001b[39m\u001b[39m{\u001b[39;00mreader\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00madapter\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     onnx_export(reader, adapter, skill)\n\u001b[1;32m     22\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUps. \u001b[39m\u001b[39m{\u001b[39;00mreader\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00madapter\u001b[39m}\u001b[39;00m\u001b[39m not available yet.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [16], line 87\u001b[0m, in \u001b[0;36monnx_export\u001b[0;34m(base_model, adapter_id, skill, quantize_model)\u001b[0m\n\u001b[1;32m     84\u001b[0m     quantize_dynamic(onnx_model_path, quantized_model_path, weight_type\u001b[39m=\u001b[39mQuantType\u001b[39m.\u001b[39mQInt8)\n\u001b[1;32m     86\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUploading model to hub... (may take a few minutes)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m push_to_hub(\n\u001b[1;32m     88\u001b[0m     save_dir \u001b[39m=\u001b[39;49m directory_path,\n\u001b[1;32m     89\u001b[0m     repository_id \u001b[39m=\u001b[39;49m model_id,\n\u001b[1;32m     90\u001b[0m )\n",
      "Cell \u001b[0;32mIn [2], line 17\u001b[0m, in \u001b[0;36mpush_to_hub\u001b[0;34m(save_dir, repository_id)\u001b[0m\n\u001b[1;32m     15\u001b[0m _, hub_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplit(local_file_path)\n\u001b[1;32m     16\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     api\u001b[39m.\u001b[39;49mupload_file(\n\u001b[1;32m     18\u001b[0m         token\u001b[39m=\u001b[39;49mhuggingface_token,\n\u001b[1;32m     19\u001b[0m         repo_id\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mUKP-SQuARE/\u001b[39;49m\u001b[39m{\u001b[39;49;00mrepository_id\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m         path_or_fileobj\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(os\u001b[39m.\u001b[39;49mgetcwd(), local_file_path),\n\u001b[1;32m     21\u001b[0m         path_in_repo\u001b[39m=\u001b[39;49mhub_file_path,\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/huggingface_hub/hf_api.py:2073\u001b[0m, in \u001b[0;36mHfApi.upload_file\u001b[0;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit)\u001b[0m\n\u001b[1;32m   2063\u001b[0m commit_message \u001b[39m=\u001b[39m (\n\u001b[1;32m   2064\u001b[0m     commit_message\n\u001b[1;32m   2065\u001b[0m     \u001b[39mif\u001b[39;00m commit_message \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2066\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUpload \u001b[39m\u001b[39m{\u001b[39;00mpath_in_repo\u001b[39m}\u001b[39;00m\u001b[39m with huggingface_hub\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2067\u001b[0m )\n\u001b[1;32m   2068\u001b[0m operation \u001b[39m=\u001b[39m CommitOperationAdd(\n\u001b[1;32m   2069\u001b[0m     path_or_fileobj\u001b[39m=\u001b[39mpath_or_fileobj,\n\u001b[1;32m   2070\u001b[0m     path_in_repo\u001b[39m=\u001b[39mpath_in_repo,\n\u001b[1;32m   2071\u001b[0m )\n\u001b[0;32m-> 2073\u001b[0m commit_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_commit(\n\u001b[1;32m   2074\u001b[0m     repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[1;32m   2075\u001b[0m     repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m   2076\u001b[0m     operations\u001b[39m=\u001b[39;49m[operation],\n\u001b[1;32m   2077\u001b[0m     commit_message\u001b[39m=\u001b[39;49mcommit_message,\n\u001b[1;32m   2078\u001b[0m     commit_description\u001b[39m=\u001b[39;49mcommit_description,\n\u001b[1;32m   2079\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2080\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2081\u001b[0m     create_pr\u001b[39m=\u001b[39;49mcreate_pr,\n\u001b[1;32m   2082\u001b[0m     parent_commit\u001b[39m=\u001b[39;49mparent_commit,\n\u001b[1;32m   2083\u001b[0m )\n\u001b[1;32m   2085\u001b[0m \u001b[39mif\u001b[39;00m commit_info\u001b[39m.\u001b[39mpr_url \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2086\u001b[0m     revision \u001b[39m=\u001b[39m quote(_parse_revision_from_pr_url(commit_info\u001b[39m.\u001b[39mpr_url), safe\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/huggingface_hub/hf_api.py:1921\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit)\u001b[0m\n\u001b[1;32m   1914\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1915\u001b[0m     commit_resp \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[1;32m   1916\u001b[0m         url\u001b[39m=\u001b[39mcommit_url,\n\u001b[1;32m   1917\u001b[0m         headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m   1918\u001b[0m         data\u001b[39m=\u001b[39m_payload_as_ndjson(),  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m         params\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mcreate_pr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m} \u001b[39mif\u001b[39;00m create_pr \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1920\u001b[0m     )\n\u001b[0;32m-> 1921\u001b[0m     hf_raise_for_status(commit_resp, endpoint_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcommit\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1922\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1923\u001b[0m     e\u001b[39m.\u001b[39mappend_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:280\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(message, response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m \u001b[39mraise\u001b[39;00m HfHubHTTPError(\u001b[39mstr\u001b[39m(e), response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/models/UKP-SQuARE/narrativeqa-onnx/commit/main (Request ID: Root=1-63c7d840-4959f0a56726503558a116b8)\n\nForbidden: pass `create_pr=1` as a query parameter to create a Pull Request"
     ]
    }
   ],
   "source": [
    "# available_skills = [\"span-extraction\", \"categorical\", \"multiple-choice\", \"abstractive\"]\n",
    "# available_skills = [\"span-extraction\"]\n",
    "#available_skills = [\"categorical\"]\n",
    "# available_skills = [\"multiple-choice\"]\n",
    "available_skills = [\"abstractive\"]\n",
    "\n",
    "# available_adapters = [\"cosmos_qa\", \"multirc\", \"quartz\", \"race\", \"quail\"]\n",
    "available_adapters = [] # all \n",
    "\n",
    "all_skills = pd.read_csv(f'square_skills/all_skills.csv')\n",
    "\n",
    "for skill in available_skills:\n",
    "    skills = all_skills[all_skills[\"Type\"] == skill]\n",
    "\n",
    "    print(f\"Exporting {skill}\")\n",
    "    print(\"_________________________\\n\")\n",
    "    for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "        \n",
    "        if (not available_adapters) or (adapter in available_adapters): #if no adapter is specified -> loop over all. OR: is some are specified-> one do these. \n",
    "            print(f\"Exporting: {reader} {adapter}\")\n",
    "            onnx_export(reader, adapter, skill)\n",
    "        else:\n",
    "            print(f\"Ups. {reader} {adapter} not available yet.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readme(directory_path, base_model, adapter):\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "\n",
    "    readme_path = hf_hub_download(repo_id=adapter, filename=\"README.md\")\n",
    "\n",
    "    onnx_readme = \"{}/README.md\".format(directory_path)\n",
    "\n",
    "    skip = False\n",
    "    with open(readme_path, 'r') as src, open(onnx_readme, 'w') as dst:\n",
    "        for line in src:\n",
    "            # Insert onnx tag\n",
    "            if line == 'tags:\\n':\n",
    "                dst.write(\"inference: false\\n\")\n",
    "                dst.write(line)\n",
    "                dst.write('- onnx\\n')\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"# Adapter\"):\n",
    "                skip = True\n",
    "\n",
    "                # Insert custom README\n",
    "                dst.write(\"# ONNX export of \" + line[2:])\n",
    "                dst.write(f\"## Conversion of [{adapter}](https://huggingface.co/{adapter}) for UKP SQuARE\\n\\n\\n\")\n",
    "                dst.write(\"## Usage\\n\")\n",
    "                dst.write(\"```python\\n\")\n",
    "                dst.write(f\"onnx_path = hf_hub_download(repo_id='UKP-SQuARE/{model_id}', filename='model.onnx') # or model_quant.onnx for quantization\\n\")\n",
    "                dst.write(\"onnx_model = InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\\n\\n\")\n",
    "                dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "                dst.write(\"```\\n\\n\")\n",
    "\n",
    "            # Continue with normal model card\n",
    "            if line.startswith(\"## Architecture & Training\"): \n",
    "                skip = False\n",
    "\n",
    "            if not skip: \n",
    "                dst.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOnnxConfig(OnnxConfig):\n",
    "    # Inspired by BertONNXConfig, can be extended to support other QA tasks\n",
    "    @property\n",
    "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "        if self.task == \"multiple-choice\":\n",
    "            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n",
    "        else:\n",
    "            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n",
    "\n",
    "        return OrderedDict(\n",
    "                [\n",
    "                    (\"input_ids\", dynamic_axis),\n",
    "                    (\"attention_mask\", dynamic_axis),\n",
    "                    (\"token_type_ids\", dynamic_axis), # Roberta doesn't use this\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "def onnx_export(base_model, adapter_id, quantize_model=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    model = AutoModelWithHeads.from_pretrained(base_model)\n",
    "    \n",
    "    adapter = f\"AdapterHub/{base_model}-pf-{adapter_id}\"\n",
    "    adapter_name = model.load_adapter(adapter, source=\"hf\")\n",
    "    model.active_adapters = adapter_name\n",
    "\n",
    "    config = AutoConfig.from_pretrained(base_model)\n",
    "\n",
    "    if base_model.startswith(\"bert\"):\n",
    "        onnx_config = BertOnnxConfig(config, task=\"question-answering\")\n",
    "    elif base_model.startswith(\"roberta\"):\n",
    "        onnx_config = RobertaOnnxConfig(config, task=\"question-answering\")\n",
    "    else:\n",
    "        onnx_config = CustomOnnxConfig(config, task=\"question-answering\")\n",
    "        \n",
    "\n",
    "    # Generate the local directory in onnx/\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "    directory_path = Path(\"onnx/{}\".format(model_id))\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_model_path = Path(\"{}/model.onnx\".format(directory_path))\n",
    "\n",
    "    # Export ONNX model\n",
    "    export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_model_path)\n",
    "\n",
    "    # Create config.json of vanilla model\n",
    "    model.save_pretrained(directory_path)\n",
    "    os.remove(directory_path / \"pytorch_model.bin\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(directory_path)\n",
    "\n",
    "    # Create README.md\n",
    "    generate_readme(directory_path, base_model, adapter)\n",
    "\n",
    "    if quantize_model:\n",
    "        quantized_model_path = \"{}/model_quant.onnx\".format(directory_path)\n",
    "        quantize_dynamic(onnx_model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "\n",
    "    print(\"Uploading model to hub... (may take a few minutes)\")\n",
    "    push_to_hub(\n",
    "        save_dir = directory_path,\n",
    "        repository_id = model_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapterhub_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, May 24 2022, 21:28:31) \n[Clang 13.1.6 (clang-1316.0.21.2)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dff476688330005cfc33f1ee0f15c13ae533c265ccd041ab146cdb98ecc6219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
