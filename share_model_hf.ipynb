{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/model_sharing\n",
    "# https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort\n",
    "# https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModelForQuestionAnswering\n",
    "\n",
    "# https://github.com/huggingface/optimum/blob/ed95b9fa8019af29ce1904ac3cfef8729eb4f4be/optimum/modeling_base.py#L12\n",
    "# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/hf_api.py#L1458\n",
    "# https://github.com/huggingface/huggingface_hub/blob/664cfdd25adfb69f429decf19e2d65ed5599f9fd/src/huggingface_hub/utils/_deprecation.py#L7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Mapping, OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import HfApi, HfFolder, hf_hub_download\n",
    "from transformers import AutoModelWithHeads, AutoTokenizer, AutoConfig\n",
    "from transformers.onnx import OnnxConfig, export\n",
    "\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers.models.bert import BertOnnxConfig\n",
    "from transformers.models.roberta import RobertaOnnxConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hub(save_dir, repository_id):\n",
    "    huggingface_token = HfFolder.get_token()\n",
    "    api = HfApi()\n",
    "\n",
    "    api.create_repo(\n",
    "        token=huggingface_token,\n",
    "        repo_id=f'UKP-SQuARE/{repository_id}',\n",
    "        exist_ok=True,\n",
    "        private=False\n",
    "    )\n",
    "\n",
    "    for path, subdirs, files in os.walk(save_dir):\n",
    "        for name in files:\n",
    "            local_file_path = os.path.join(path, name)\n",
    "            _, hub_file_path = os.path.split(local_file_path)\n",
    "            try:\n",
    "                api.upload_file(\n",
    "                    token=huggingface_token,\n",
    "                    repo_id=f\"UKP-SQuARE/{repository_id}\",\n",
    "                    path_or_fileobj=os.path.join(os.getcwd(), local_file_path),\n",
    "                    path_in_repo=hub_file_path,\n",
    "                )\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except NameError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readme(directory_path, base_model, adapter, skill):\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "\n",
    "    readme_path = hf_hub_download(repo_id=adapter, filename=\"README.md\")\n",
    "\n",
    "    onnx_readme = \"{}/README.md\".format(directory_path)\n",
    "\n",
    "    skip = False\n",
    "    with open(readme_path, 'r') as src, open(onnx_readme, 'w') as dst:\n",
    "        for line in src:\n",
    "            # Insert onnx tag\n",
    "            if line == 'tags:\\n':\n",
    "                dst.write(\"inference: false\\n\")\n",
    "                dst.write(line)\n",
    "                dst.write('- onnx\\n')\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"# Adapter\"):\n",
    "                skip = True\n",
    "\n",
    "                # Insert custom README\n",
    "                dst.write(\"# ONNX export of \" + line[2:])\n",
    "                dst.write(f\"## Conversion of [{adapter}](https://huggingface.co/{adapter}) for UKP SQuARE\\n\\n\\n\")\n",
    "                dst.write(\"## Usage\\n\")\n",
    "                dst.write(\"```python\\n\")\n",
    "                dst.write(f\"onnx_path = hf_hub_download(repo_id='UKP-SQuARE/{model_id}', filename='model.onnx') # or model_quant.onnx for quantization\\n\")\n",
    "                dst.write(\"onnx_model = InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\\n\\n\")\n",
    "\n",
    "                if (skill == \"span-extraction\") | (skill == \"categorical\"):\n",
    "                    dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                    dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                    dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                    dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                    dst.write(\"inputs_int64 = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\\n\")\n",
    "                    dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs_int64), output_names=None)\\n\")\n",
    "                    dst.write(\"```\\n\\n\")\n",
    "\n",
    "                elif skill == \"multiple-choice\":\n",
    "                    dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                    dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                    dst.write('choices = [\"Cat\", \"Horse\", \"Tiger\", \"Fish\"]')\n",
    "\n",
    "                    dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "\n",
    "                    dst.write(\"raw_input = [[context, question + \" \" + choice] for choice in choices]\\n\")\n",
    "                    dst.write('inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"np\")\\n')\n",
    "\n",
    "                    dst.write(\"inputs['token_type_ids'] = np.expand_dims(inputs['token_type_ids'], axis=0)\\n\")\n",
    "                    dst.write(\"inputs['input_ids'] =  np.expand_dims(inputs['input_ids'], axis=0)\\n\")\n",
    "                    dst.write(\"inputs['attention_mask'] =  np.expand_dims(inputs['attention_mask'], axis=0)\\n\")\n",
    "                    dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "\n",
    "                    dst.write(\"```\\n\\n\")\n",
    "\n",
    "\n",
    "            # Continue with normal model card\n",
    "            if line.startswith(\"## Architecture & Training\"): \n",
    "                skip = False\n",
    "\n",
    "            if not skip: \n",
    "                dst.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOnnxConfig(OnnxConfig):\n",
    "    # Inspired by BertONNXConfig, can be extended to support other QA tasks\n",
    "    @property\n",
    "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "        if self.task == \"multiple-choice\":\n",
    "            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n",
    "        else:\n",
    "            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n",
    "\n",
    "        return OrderedDict(\n",
    "                [\n",
    "                    (\"input_ids\", dynamic_axis),\n",
    "                    (\"attention_mask\", dynamic_axis),\n",
    "                    (\"token_type_ids\", dynamic_axis), # Roberta doesn't use this\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "def onnx_export(base_model, adapter_id, skill, quantize_model=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    model = AutoModelWithHeads.from_pretrained(base_model)\n",
    "    \n",
    "    adapter = f\"AdapterHub/{base_model}-pf-{adapter_id}\"\n",
    "    adapter_name = model.load_adapter(adapter, source=\"hf\")\n",
    "    model.active_adapters = adapter_name\n",
    "\n",
    "    config = AutoConfig.from_pretrained(base_model)\n",
    "\n",
    "\n",
    "    if (skill == \"span-extraction\") | (skill == \"categorical\"):\n",
    "        if base_model.startswith(\"bert\"):\n",
    "            onnx_config = BertOnnxConfig(config)\n",
    "        elif base_model.startswith(\"roberta\"):\n",
    "            onnx_config = RobertaOnnxConfig(config)\n",
    "        else:\n",
    "            onnx_config = CustomOnnxConfig(config)\n",
    "            \n",
    "    elif skill == \"multiple-choice\":\n",
    "        if base_model.startswith(\"bert\"):\n",
    "            onnx_config = BertOnnxConfig(config, task=\"multiple-choice\")\n",
    "        elif base_model.startswith(\"roberta\"):\n",
    "            onnx_config = RobertaOnnxConfig(config, task=\"multiple-choice\")\n",
    "        else:\n",
    "            onnx_config = CustomOnnxConfig(config, task=\"multiple-choice\")\n",
    "        \n",
    "\n",
    "    print(skill)\n",
    "    \n",
    "    # Generate the local directory in onnx/\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "    directory_path = Path(\"onnx/{}\".format(model_id))\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_model_path = Path(\"{}/model.onnx\".format(directory_path))\n",
    "\n",
    "    # Export ONNX model\n",
    "    export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_model_path)\n",
    "\n",
    "\n",
    "    # Create config.json of vanilla model\n",
    "    model.save_pretrained(directory_path)\n",
    "    os.remove(directory_path / \"pytorch_model.bin\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(directory_path)\n",
    "\n",
    "    # Create README.md\n",
    "    generate_readme(directory_path, base_model, adapter, skill)\n",
    "\n",
    "    if quantize_model:\n",
    "        quantized_model_path = \"{}/model_quant.onnx\".format(directory_path)\n",
    "        quantize_dynamic(onnx_model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "\n",
    "    print(\"Uploading model to hub... (may take a few minutes)\")\n",
    "    push_to_hub(\n",
    "        save_dir = directory_path,\n",
    "        repository_id = model_id,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting multiple-choice\n",
      "_________________________\n",
      "\n",
      "Ups. bert-base-uncased commonsense_qa not available yet.\n",
      "Ups. roberta-base commonsense_qa not available yet.\n",
      "Ups. bert-base-uncased cosmos_qa not available yet.\n",
      "Ups. roberta-base cosmos_qa not available yet.\n",
      "Exporting: bert-base-uncased multirc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3225.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple-choice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/layer.py:49: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if context.output_adapter_gating_scores:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/composition.py:202: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor is not None and hidden_states.shape[0] != tensor.shape[0]:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/context.py:117: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if getattr(ctx, \"output_\" + attr, False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.11/attention/self/MatMul_1]\n",
      "Uploading model to hub... (may take a few minutes)\n",
      "Exporting: roberta-base multirc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 490kB/s]\n",
      "Downloading: 100%|██████████| 2.23k/2.23k [00:00<00:00, 921kB/s]\n",
      "Downloading: 100%|██████████| 584/584 [00:00<00:00, 320kB/s]/s]\n",
      "Downloading: 100%|██████████| 385/385 [00:00<00:00, 216kB/s]it]\n",
      "Downloading: 100%|██████████| 3.60M/3.60M [00:02<00:00, 1.31MB/s]\n",
      "Downloading: 100%|██████████| 2.37M/2.37M [00:01<00:00, 2.13MB/s]\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:10<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple-choice\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.11/attention/self/MatMul_1]\n",
      "Uploading model to hub... (may take a few minutes)\n",
      "Ups. bart-base narrativeqa not available yet.\n",
      "Exporting: bert-base-uncased quail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4357.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple-choice\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.11/attention/self/MatMul_1]\n",
      "Uploading model to hub... (may take a few minutes)\n",
      "Exporting: roberta-base quail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 653kB/s]\n",
      "Downloading: 100%|██████████| 2.13k/2.13k [00:00<00:00, 1.05MB/s]\n",
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 319kB/s]it]\n",
      "Downloading: 100%|██████████| 377/377 [00:00<00:00, 217kB/s]it]\n",
      "Downloading: 100%|██████████| 3.59M/3.59M [00:00<00:00, 4.57MB/s]\n",
      "Downloading: 100%|██████████| 2.37M/2.37M [00:00<00:00, 3.65MB/s]\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:07<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple-choice\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.11/attention/self/MatMul_1]\n",
      "Uploading model to hub... (may take a few minutes)\n",
      "Exporting: roberta-base quartz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 425kB/s]\n",
      "Downloading: 100%|██████████| 2.13k/2.13k [00:00<00:00, 1.18MB/s]\n",
      "Downloading: 100%|██████████| 572/572 [00:00<00:00, 266kB/s]it]\n",
      "Downloading: 100%|██████████| 350/350 [00:00<00:00, 171kB/s]it]\n",
      "Downloading: 100%|██████████| 3.59M/3.59M [00:00<00:00, 4.84MB/s]\n",
      "Downloading: 100%|██████████| 2.37M/2.37M [00:00<00:00, 3.24MB/s]\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:09<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple-choice\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.11/attention/self/MatMul_1]\n",
      "Uploading model to hub... (may take a few minutes)\n",
      "Exporting: bert-base-uncased race\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 2839.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple-choice\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.11/attention/self/MatMul_1]\n",
      "Uploading model to hub... (may take a few minutes)\n",
      "Exporting: roberta-base race\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 536kB/s]\n",
      "Downloading: 100%|██████████| 2.15k/2.15k [00:00<00:00, 1.01MB/s]\n",
      "Downloading: 100%|██████████| 574/574 [00:00<00:00, 292kB/s]it]\n",
      "Downloading: 100%|██████████| 380/380 [00:00<00:00, 217kB/s]it]\n",
      "Downloading: 100%|██████████| 3.59M/3.59M [00:00<00:00, 4.77MB/s]\n",
      "Downloading: 100%|██████████| 2.37M/2.37M [00:00<00:00, 3.15MB/s]\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:08<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple-choice\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.11/attention/self/MatMul_1]\n",
      "Uploading model to hub... (may take a few minutes)\n",
      "Ups. bert-base-uncased social_i_qa not available yet.\n",
      "Ups. roberta-base social_i_qa not available yet.\n"
     ]
    }
   ],
   "source": [
    "# available_skills = [\"span-extraction\", \"categorical\", \"multiple-choice\"]\n",
    "available_skills = [\"multiple-choice\"]\n",
    "available_adapters = [\"cosmos_qa\", \"multirc\", \"quartz\", \"race\", \"quail\"]\n",
    "\n",
    "all_skills = pd.read_csv(f'square_skills/all_skills.csv')\n",
    "\n",
    "for skill in available_skills:\n",
    "    skills = all_skills[all_skills[\"Type\"] == skill]\n",
    "\n",
    "    print(f\"Exporting {skill}\")\n",
    "    print(\"_________________________\\n\")\n",
    "    for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "    \n",
    "        if adapter in available_adapters:\n",
    "            print(f\"Exporting: {reader} {adapter}\")\n",
    "            onnx_export(reader, adapter, skill)\n",
    "        else:\n",
    "            print(f\"Ups. {reader} {adapter} not available yet.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readme(directory_path, base_model, adapter):\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "\n",
    "    readme_path = hf_hub_download(repo_id=adapter, filename=\"README.md\")\n",
    "\n",
    "    onnx_readme = \"{}/README.md\".format(directory_path)\n",
    "\n",
    "    skip = False\n",
    "    with open(readme_path, 'r') as src, open(onnx_readme, 'w') as dst:\n",
    "        for line in src:\n",
    "            # Insert onnx tag\n",
    "            if line == 'tags:\\n':\n",
    "                dst.write(\"inference: false\\n\")\n",
    "                dst.write(line)\n",
    "                dst.write('- onnx\\n')\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"# Adapter\"):\n",
    "                skip = True\n",
    "\n",
    "                # Insert custom README\n",
    "                dst.write(\"# ONNX export of \" + line[2:])\n",
    "                dst.write(f\"## Conversion of [{adapter}](https://huggingface.co/{adapter}) for UKP SQuARE\\n\\n\\n\")\n",
    "                dst.write(\"## Usage\\n\")\n",
    "                dst.write(\"```python\\n\")\n",
    "                dst.write(f\"onnx_path = hf_hub_download(repo_id='UKP-SQuARE/{model_id}', filename='model.onnx') # or model_quant.onnx for quantization\\n\")\n",
    "                dst.write(\"onnx_model = InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\\n\\n\")\n",
    "                dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                dst.write(\"inputs_int64 = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\\n\")\n",
    "                dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs_int64), output_names=None)\\n\")\n",
    "                dst.write(\"```\\n\\n\")\n",
    "\n",
    "            # Continue with normal model card\n",
    "            if line.startswith(\"## Architecture & Training\"): \n",
    "                skip = False\n",
    "\n",
    "            if not skip: \n",
    "                dst.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOnnxConfig(OnnxConfig):\n",
    "    # Inspired by BertONNXConfig, can be extended to support other QA tasks\n",
    "    @property\n",
    "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "        if self.task == \"multiple-choice\":\n",
    "            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n",
    "        else:\n",
    "            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n",
    "\n",
    "        return OrderedDict(\n",
    "                [\n",
    "                    (\"input_ids\", dynamic_axis),\n",
    "                    (\"attention_mask\", dynamic_axis),\n",
    "                    (\"token_type_ids\", dynamic_axis), # Roberta doesn't use this\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "def onnx_export(base_model, adapter_id, quantize_model=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    model = AutoModelWithHeads.from_pretrained(base_model)\n",
    "    \n",
    "    adapter = f\"AdapterHub/{base_model}-pf-{adapter_id}\"\n",
    "    adapter_name = model.load_adapter(adapter, source=\"hf\")\n",
    "    model.active_adapters = adapter_name\n",
    "\n",
    "    config = AutoConfig.from_pretrained(base_model)\n",
    "\n",
    "    if base_model.startswith(\"bert\"):\n",
    "        onnx_config = BertOnnxConfig(config, task=\"question-answering\")\n",
    "    elif base_model.startswith(\"roberta\"):\n",
    "        onnx_config = RobertaOnnxConfig(config, task=\"question-answering\")\n",
    "    else:\n",
    "        onnx_config = CustomOnnxConfig(config, task=\"question-answering\")\n",
    "        \n",
    "\n",
    "    # Generate the local directory in onnx/\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "    directory_path = Path(\"onnx/{}\".format(model_id))\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_model_path = Path(\"{}/model.onnx\".format(directory_path))\n",
    "\n",
    "    # Export ONNX model\n",
    "    export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_model_path)\n",
    "\n",
    "    # Create config.json of vanilla model\n",
    "    model.save_pretrained(directory_path)\n",
    "    os.remove(directory_path / \"pytorch_model.bin\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(directory_path)\n",
    "\n",
    "    # Create README.md\n",
    "    generate_readme(directory_path, base_model, adapter)\n",
    "\n",
    "    if quantize_model:\n",
    "        quantized_model_path = \"{}/model_quant.onnx\".format(directory_path)\n",
    "        quantize_dynamic(onnx_model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "\n",
    "    print(\"Uploading model to hub... (may take a few minutes)\")\n",
    "    push_to_hub(\n",
    "        save_dir = directory_path,\n",
    "        repository_id = model_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('adapter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a7f5a2c07603db35bc4e52cfd5b475adbf202ae824ea4c5e531d495460257f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
