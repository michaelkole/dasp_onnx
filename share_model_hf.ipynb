{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/model_sharing\n",
    "# https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort\n",
    "# https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModelForQuestionAnswering\n",
    "\n",
    "# https://github.com/huggingface/optimum/blob/ed95b9fa8019af29ce1904ac3cfef8729eb4f4be/optimum/modeling_base.py#L12\n",
    "# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/hf_api.py#L1458\n",
    "# https://github.com/huggingface/huggingface_hub/blob/664cfdd25adfb69f429decf19e2d65ed5599f9fd/src/huggingface_hub/utils/_deprecation.py#L7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Mapping, OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import HfApi, HfFolder, hf_hub_download\n",
    "from transformers import AutoModelWithHeads, AutoTokenizer, AutoConfig\n",
    "from transformers.onnx import OnnxConfig, export\n",
    "\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers.models.bert import BertOnnxConfig\n",
    "from transformers.models.roberta import RobertaOnnxConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hub(save_dir, repository_id):\n",
    "    huggingface_token = HfFolder.get_token()\n",
    "    api = HfApi()\n",
    "\n",
    "    api.create_repo(\n",
    "        token=huggingface_token,\n",
    "        repo_id=f'UKP-SQuARE/{repository_id}',\n",
    "        exist_ok=True,\n",
    "        private=False\n",
    "    )\n",
    "\n",
    "    for path, subdirs, files in os.walk(save_dir):\n",
    "        for name in files:\n",
    "            local_file_path = os.path.join(path, name)\n",
    "            _, hub_file_path = os.path.split(local_file_path)\n",
    "            try:\n",
    "                api.upload_file(\n",
    "                    token=huggingface_token,\n",
    "                    repo_id=f\"UKP-SQuARE/{repository_id}\",\n",
    "                    path_or_fileobj=os.path.join(os.getcwd(), local_file_path),\n",
    "                    path_in_repo=hub_file_path,\n",
    "                )\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except NameError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readme(directory_path, base_model, adapter, skill):\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "\n",
    "    readme_path = hf_hub_download(repo_id=adapter, filename=\"README.md\")\n",
    "\n",
    "    onnx_readme = \"{}/README.md\".format(directory_path)\n",
    "\n",
    "    skip = False\n",
    "    with open(readme_path, 'r') as src, open(onnx_readme, 'w') as dst:\n",
    "        for line in src:\n",
    "            # Insert onnx tag\n",
    "            if line == 'tags:\\n':\n",
    "                dst.write(\"inference: false\\n\")\n",
    "                dst.write(line)\n",
    "                dst.write('- onnx\\n')\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"# Adapter\"):\n",
    "                skip = True\n",
    "\n",
    "                # Insert custom README\n",
    "                dst.write(\"# ONNX export of \" + line[2:])\n",
    "                dst.write(f\"## Conversion of [{adapter}](https://huggingface.co/{adapter}) for UKP SQuARE\\n\\n\\n\")\n",
    "                dst.write(\"## Usage\\n\")\n",
    "                dst.write(\"```python\\n\")\n",
    "                dst.write(f\"onnx_path = hf_hub_download(repo_id='UKP-SQuARE/{model_id}', filename='model.onnx') # or model_quant.onnx for quantization\\n\")\n",
    "                dst.write(\"onnx_model = InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\\n\\n\")\n",
    "\n",
    "                if (skill == \"span-extraction\"):\n",
    "                    dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                    dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                    dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                    dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                    dst.write(\"inputs_int64 = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\\n\")\n",
    "                    dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs_int64), output_names=None)\\n\")\n",
    "                    dst.write(\"```\\n\\n\")\n",
    "\n",
    "                elif (skill == \"categorical\"):\n",
    "                    dst.write(\"context = 'English orthography typically represents vowel sounds with the five conventional vowel letters ⟨a, e, i, o, u⟩, as well as ⟨y⟩, which may also be a consonant depending on context. However, outside of abbreviations, there are a handful of words in English that do not have vowels, either because the vowel sounds are not written with vowel letters or because the words themselves are pronounced without vowel sounds'.\\n\")\n",
    "                    dst.write(\"question = 'can there be a word without a vowel'\\n\")\n",
    "                    dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                    dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                    dst.write(\"inputs = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\\n\")\n",
    "                    dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "                    dst.write(\"```\\n\\n\")\n",
    "\n",
    "                elif skill == \"multiple-choice\":\n",
    "                    dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                    dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                    dst.write('choices = [\"Cat\", \"Horse\", \"Tiger\", \"Fish\"]')\n",
    "\n",
    "                    dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "\n",
    "                    dst.write(\"raw_input = [[context, question + \" \" + choice] for choice in choices]\\n\")\n",
    "                    dst.write('inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"np\")\\n')\n",
    "\n",
    "                    dst.write(\"inputs['token_type_ids'] = np.expand_dims(inputs['token_type_ids'], axis=0)\\n\")\n",
    "                    dst.write(\"inputs['input_ids'] =  np.expand_dims(inputs['input_ids'], axis=0)\\n\")\n",
    "                    dst.write(\"inputs['attention_mask'] =  np.expand_dims(inputs['attention_mask'], axis=0)\\n\")\n",
    "                    dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "\n",
    "                    dst.write(\"```\\n\\n\")\n",
    "\n",
    "\n",
    "            # Continue with normal model card\n",
    "            if line.startswith(\"## Architecture & Training\"): \n",
    "                skip = False\n",
    "\n",
    "            if not skip: \n",
    "                dst.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOnnxConfig(OnnxConfig):\n",
    "    # Inspired by BertONNXConfig, can be extended to support other QA tasks\n",
    "    @property\n",
    "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "        if self.task == \"multiple-choice\":\n",
    "            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n",
    "        else:\n",
    "            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n",
    "\n",
    "        return OrderedDict(\n",
    "                [\n",
    "                    (\"input_ids\", dynamic_axis),\n",
    "                    (\"attention_mask\", dynamic_axis),\n",
    "                    (\"token_type_ids\", dynamic_axis), # Roberta doesn't use this\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "def onnx_export(base_model, adapter_id, skill, quantize_model=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    model = AutoModelWithHeads.from_pretrained(base_model)\n",
    "    \n",
    "    adapter = f\"AdapterHub/{base_model}-pf-{adapter_id}\"\n",
    "    adapter_name = model.load_adapter(adapter, source=\"hf\")\n",
    "    model.active_adapters = adapter_name\n",
    "\n",
    "    config = AutoConfig.from_pretrained(base_model)\n",
    "\n",
    "\n",
    "    if (skill == \"span-extraction\") | (skill == \"categorical\"):\n",
    "        if base_model.startswith(\"bert\"):\n",
    "            onnx_config = BertOnnxConfig(config)\n",
    "        elif base_model.startswith(\"roberta\"):\n",
    "            onnx_config = RobertaOnnxConfig(config)\n",
    "        else:\n",
    "            onnx_config = CustomOnnxConfig(config)\n",
    "            \n",
    "    elif skill == \"multiple-choice\":\n",
    "        if base_model.startswith(\"bert\"):\n",
    "            onnx_config = BertOnnxConfig(config, task=\"multiple-choice\")\n",
    "        elif base_model.startswith(\"roberta\"):\n",
    "            onnx_config = RobertaOnnxConfig(config, task=\"multiple-choice\")\n",
    "        else:\n",
    "            onnx_config = CustomOnnxConfig(config, task=\"multiple-choice\")\n",
    "            \n",
    "    \n",
    "    # Generate the local directory in onnx/\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "    directory_path = Path(\"onnx/{}\".format(model_id))\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_model_path = Path(\"{}/model.onnx\".format(directory_path))\n",
    "\n",
    "    # Export ONNX model\n",
    "    export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_model_path)\n",
    "\n",
    "\n",
    "    # Create config.json of vanilla model\n",
    "    model.save_pretrained(directory_path)\n",
    "    os.remove(directory_path / \"pytorch_model.bin\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(directory_path)\n",
    "\n",
    "    # Create README.md\n",
    "    generate_readme(directory_path, base_model, adapter, skill)\n",
    "\n",
    "    if quantize_model:\n",
    "        quantized_model_path = \"{}/model_quant.onnx\".format(directory_path)\n",
    "        quantize_dynamic(onnx_model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "\n",
    "    print(\"Uploading model to hub... (may take a few minutes)\")\n",
    "    push_to_hub(\n",
    "        save_dir = directory_path,\n",
    "        repository_id = model_id,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Models and Uplaoding them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting categorical\n",
      "_________________________\n",
      "\n",
      "Exporting: bert-base-uncased boolq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3291.37it/s]\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/layer.py:49: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if context.output_adapter_gating_scores:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/composition.py:202: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor is not None and hidden_states.shape[0] != tensor.shape[0]:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/context.py:117: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if getattr(ctx, \"output_\" + attr, False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.11/attention/self/MatMul_1]\n",
      "Uploading model to hub... (may take a few minutes)\n",
      "Exporting: roberta-base boolq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3065.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.11/attention/self/MatMul_1]\n",
      "Uploading model to hub... (may take a few minutes)\n"
     ]
    }
   ],
   "source": [
    "# available_skills = [\"span-extraction\", \"categorical\", \"multiple-choice\"]\n",
    "# available_skills = [\"span-extraction\"]\n",
    "available_skills = [\"categorical\"]\n",
    "# available_skills = [\"multiple-choice\"]\n",
    "\n",
    "\n",
    "# available_adapters = [\"cosmos_qa\", \"multirc\", \"quartz\", \"race\", \"quail\"]\n",
    "available_adapters = [] # all \n",
    "\n",
    "all_skills = pd.read_csv(f'square_skills/all_skills.csv')\n",
    "\n",
    "for skill in available_skills:\n",
    "    skills = all_skills[all_skills[\"Type\"] == skill]\n",
    "\n",
    "    print(f\"Exporting {skill}\")\n",
    "    print(\"_________________________\\n\")\n",
    "    for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "        \n",
    "        if (not available_adapters) or (adapter in available_adapters): #if no adapter is specified -> loop over all. OR: is some are specified-> one do these. \n",
    "            print(f\"Exporting: {reader} {adapter}\")\n",
    "            onnx_export(reader, adapter, skill)\n",
    "        else:\n",
    "            print(f\"Ups. {reader} {adapter} not available yet.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readme(directory_path, base_model, adapter):\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "\n",
    "    readme_path = hf_hub_download(repo_id=adapter, filename=\"README.md\")\n",
    "\n",
    "    onnx_readme = \"{}/README.md\".format(directory_path)\n",
    "\n",
    "    skip = False\n",
    "    with open(readme_path, 'r') as src, open(onnx_readme, 'w') as dst:\n",
    "        for line in src:\n",
    "            # Insert onnx tag\n",
    "            if line == 'tags:\\n':\n",
    "                dst.write(\"inference: false\\n\")\n",
    "                dst.write(line)\n",
    "                dst.write('- onnx\\n')\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"# Adapter\"):\n",
    "                skip = True\n",
    "\n",
    "                # Insert custom README\n",
    "                dst.write(\"# ONNX export of \" + line[2:])\n",
    "                dst.write(f\"## Conversion of [{adapter}](https://huggingface.co/{adapter}) for UKP SQuARE\\n\\n\\n\")\n",
    "                dst.write(\"## Usage\\n\")\n",
    "                dst.write(\"```python\\n\")\n",
    "                dst.write(f\"onnx_path = hf_hub_download(repo_id='UKP-SQuARE/{model_id}', filename='model.onnx') # or model_quant.onnx for quantization\\n\")\n",
    "                dst.write(\"onnx_model = InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\\n\\n\")\n",
    "                dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                dst.write(\"inputs_int64 = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\\n\")\n",
    "                dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs_int64), output_names=None)\\n\")\n",
    "                dst.write(\"```\\n\\n\")\n",
    "\n",
    "            # Continue with normal model card\n",
    "            if line.startswith(\"## Architecture & Training\"): \n",
    "                skip = False\n",
    "\n",
    "            if not skip: \n",
    "                dst.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOnnxConfig(OnnxConfig):\n",
    "    # Inspired by BertONNXConfig, can be extended to support other QA tasks\n",
    "    @property\n",
    "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "        if self.task == \"multiple-choice\":\n",
    "            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n",
    "        else:\n",
    "            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n",
    "\n",
    "        return OrderedDict(\n",
    "                [\n",
    "                    (\"input_ids\", dynamic_axis),\n",
    "                    (\"attention_mask\", dynamic_axis),\n",
    "                    (\"token_type_ids\", dynamic_axis), # Roberta doesn't use this\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "def onnx_export(base_model, adapter_id, quantize_model=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    model = AutoModelWithHeads.from_pretrained(base_model)\n",
    "    \n",
    "    adapter = f\"AdapterHub/{base_model}-pf-{adapter_id}\"\n",
    "    adapter_name = model.load_adapter(adapter, source=\"hf\")\n",
    "    model.active_adapters = adapter_name\n",
    "\n",
    "    config = AutoConfig.from_pretrained(base_model)\n",
    "\n",
    "    if base_model.startswith(\"bert\"):\n",
    "        onnx_config = BertOnnxConfig(config, task=\"question-answering\")\n",
    "    elif base_model.startswith(\"roberta\"):\n",
    "        onnx_config = RobertaOnnxConfig(config, task=\"question-answering\")\n",
    "    else:\n",
    "        onnx_config = CustomOnnxConfig(config, task=\"question-answering\")\n",
    "        \n",
    "\n",
    "    # Generate the local directory in onnx/\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "    directory_path = Path(\"onnx/{}\".format(model_id))\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_model_path = Path(\"{}/model.onnx\".format(directory_path))\n",
    "\n",
    "    # Export ONNX model\n",
    "    export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_model_path)\n",
    "\n",
    "    # Create config.json of vanilla model\n",
    "    model.save_pretrained(directory_path)\n",
    "    os.remove(directory_path / \"pytorch_model.bin\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(directory_path)\n",
    "\n",
    "    # Create README.md\n",
    "    generate_readme(directory_path, base_model, adapter)\n",
    "\n",
    "    if quantize_model:\n",
    "        quantized_model_path = \"{}/model_quant.onnx\".format(directory_path)\n",
    "        quantize_dynamic(onnx_model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "\n",
    "    print(\"Uploading model to hub... (may take a few minutes)\")\n",
    "    push_to_hub(\n",
    "        save_dir = directory_path,\n",
    "        repository_id = model_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapterhub_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dff476688330005cfc33f1ee0f15c13ae533c265ccd041ab146cdb98ecc6219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
