{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/model_sharing\n",
    "# https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort\n",
    "# https://huggingface.co/docs/optimum/v1.2.1/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModelForQuestionAnswering\n",
    "\n",
    "# https://github.com/huggingface/optimum/blob/ed95b9fa8019af29ce1904ac3cfef8729eb4f4be/optimum/modeling_base.py#L12\n",
    "# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/hf_api.py#L1458\n",
    "# https://github.com/huggingface/huggingface_hub/blob/664cfdd25adfb69f429decf19e2d65ed5599f9fd/src/huggingface_hub/utils/_deprecation.py#L7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Mapping, OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import HfApi, HfFolder, hf_hub_download\n",
    "from transformers import AutoModelWithHeads, AutoTokenizer, AutoConfig\n",
    "from transformers.onnx import OnnxConfig, export\n",
    "\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers.models.bert import BertOnnxConfig\n",
    "from transformers.models.roberta import RobertaOnnxConfig\n",
    "from transformers.models.bart import BartOnnxConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hub(save_dir, repository_id):\n",
    "    huggingface_token = HfFolder.get_token()\n",
    "    api = HfApi()\n",
    "\n",
    "    api.create_repo(\n",
    "        token=huggingface_token,\n",
    "        repo_id=f'UKP-SQuARE/{repository_id}',\n",
    "        exist_ok=True,\n",
    "        private=False\n",
    "    )\n",
    "\n",
    "    for path, subdirs, files in os.walk(save_dir):\n",
    "        for name in files:\n",
    "            local_file_path = os.path.join(path, name)\n",
    "            _, hub_file_path = os.path.split(local_file_path)\n",
    "            try:\n",
    "                api.upload_file(\n",
    "                    token=huggingface_token,\n",
    "                    repo_id=f\"UKP-SQuARE/{repository_id}\",\n",
    "                    path_or_fileobj=os.path.join(os.getcwd(), local_file_path),\n",
    "                    path_in_repo=hub_file_path,\n",
    "                )\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except NameError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readme(directory_path, base_model, adapter, skill):\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "\n",
    "    readme_path = hf_hub_download(repo_id=adapter, filename=\"README.md\")\n",
    "\n",
    "    onnx_readme = \"{}/README.md\".format(directory_path)\n",
    "\n",
    "    skip = False\n",
    "    with open(readme_path, 'r') as src, open(onnx_readme, 'w') as dst:\n",
    "        for line in src:\n",
    "            # Insert onnx tag\n",
    "            if line == 'tags:\\n':\n",
    "                dst.write(\"inference: false\\n\")\n",
    "                dst.write(line)\n",
    "                dst.write('- onnx\\n')\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"# Adapter\"):\n",
    "                skip = True\n",
    "\n",
    "                # Insert custom README\n",
    "                dst.write(\"# ONNX export of \" + line[2:])\n",
    "                dst.write(f\"## Conversion of [{adapter}](https://huggingface.co/{adapter}) for UKP SQuARE\\n\\n\\n\")\n",
    "                dst.write(\"## Usage\\n\")\n",
    "                dst.write(\"```python\\n\")\n",
    "                dst.write(f\"onnx_path = hf_hub_download(repo_id='UKP-SQuARE/{model_id}', filename='model.onnx') # or model_quant.onnx for quantization\\n\")\n",
    "                dst.write(\"onnx_model = InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\\n\\n\")\n",
    "\n",
    "                if (skill == \"span-extraction\"):\n",
    "                    dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                    dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                    dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                    dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                    dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "                    dst.write(\"```\\n\\n\")\n",
    "\n",
    "                elif (skill == \"categorical\"):\n",
    "                    dst.write(\"context = 'English orthography typically represents vowel sounds with the five conventional vowel letters ⟨a, e, i, o, u⟩, as well as ⟨y⟩, which may also be a consonant depending on context. However, outside of abbreviations, there are a handful of words in English that do not have vowels, either because the vowel sounds are not written with vowel letters or because the words themselves are pronounced without vowel sounds'.\\n\")\n",
    "                    dst.write(\"question = 'can there be a word without a vowel'\\n\")\n",
    "                    dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                    dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                    dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "                    dst.write(\"```\\n\\n\")\n",
    "\n",
    "                elif skill == \"multiple-choice\":\n",
    "                    dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                    dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                    dst.write('choices = [\"Cat\", \"Horse\", \"Tiger\", \"Fish\"]')\n",
    "\n",
    "                    dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "\n",
    "                    dst.write(\"raw_input = [[context, question + \" \" + choice] for choice in choices]\\n\")\n",
    "                    dst.write('inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"np\")\\n')\n",
    "\n",
    "                    dst.write(\"inputs['token_type_ids'] = np.expand_dims(inputs['token_type_ids'], axis=0)\\n\")\n",
    "                    dst.write(\"inputs['input_ids'] =  np.expand_dims(inputs['input_ids'], axis=0)\\n\")\n",
    "                    dst.write(\"inputs['attention_mask'] =  np.expand_dims(inputs['attention_mask'], axis=0)\\n\")\n",
    "                    dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "\n",
    "                    dst.write(\"```\\n\\n\")\n",
    "\n",
    "                elif skill == \"abstractive\":\n",
    "                    dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                    dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                    dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                    dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                    dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "                    dst.write(\"```\\n\\n\")\n",
    "\n",
    "\n",
    "            # Continue with normal model card\n",
    "            if line.startswith(\"## Architecture & Training\"): \n",
    "                skip = False\n",
    "\n",
    "            if not skip: \n",
    "                dst.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOnnxConfig(OnnxConfig):\n",
    "    # Inspired by BertONNXConfig, can be extended to support other QA tasks\n",
    "    @property\n",
    "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "        if self.task == \"multiple-choice\":\n",
    "            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n",
    "        else:\n",
    "            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n",
    "\n",
    "        return OrderedDict(\n",
    "                [\n",
    "                    (\"input_ids\", dynamic_axis),\n",
    "                    (\"attention_mask\", dynamic_axis),\n",
    "                    (\"token_type_ids\", dynamic_axis), # Roberta doesn't use this\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "def onnx_export(base_model, adapter_id, skill, quantize_model=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    model = AutoModelWithHeads.from_pretrained(base_model)\n",
    "    \n",
    "    #adapter = f\"AdapterHub/{base_model}-pf-{adapter_id}\"\n",
    "    adapter = \"AdapterHub/narrativeqa\" # not named consistently\n",
    "    adapter_name = model.load_adapter(adapter, source=\"hf\")\n",
    "    model.active_adapters = adapter_name\n",
    "\n",
    "    config = AutoConfig.from_pretrained(base_model)\n",
    "\n",
    "\n",
    "    if (skill == \"span-extraction\") | (skill == \"categorical\"):\n",
    "        if base_model.startswith(\"bert\"):\n",
    "            onnx_config = BertOnnxConfig(config)\n",
    "        elif base_model.startswith(\"roberta\"):\n",
    "            onnx_config = RobertaOnnxConfig(config)\n",
    "        else:\n",
    "            onnx_config = CustomOnnxConfig(config)\n",
    "            \n",
    "    elif skill == \"multiple-choice\":\n",
    "        if base_model.startswith(\"bert\"):\n",
    "            onnx_config = BertOnnxConfig(config, task=\"multiple-choice\")\n",
    "        elif base_model.startswith(\"roberta\"):\n",
    "            onnx_config = RobertaOnnxConfig(config, task=\"multiple-choice\")\n",
    "        else:\n",
    "            onnx_config = CustomOnnxConfig(config, task=\"multiple-choice\")\n",
    "            \n",
    "    elif skill == \"abstractive\":\n",
    "        if base_model.startswith(\"bart\"):\n",
    "            onnx_config = BartOnnxConfig(config, task=\"seq2seq-lm\")\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only BART is supported for abstractive qa\")\n",
    "\n",
    "    \n",
    "    # Generate the local directory in onnx/\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "    directory_path = Path(\"onnx/{}\".format(model_id))\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_model_path = Path(\"{}/model.onnx\".format(directory_path))\n",
    "\n",
    "    # Export ONNX model\n",
    "    export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_model_path)\n",
    "\n",
    "\n",
    "    # Create config.json of vanilla model\n",
    "    model.save_pretrained(directory_path)\n",
    "    os.remove(directory_path / \"pytorch_model.bin\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(directory_path)\n",
    "\n",
    "    # Create README.md\n",
    "    generate_readme(directory_path, base_model, adapter, skill)\n",
    "\n",
    "    if quantize_model:\n",
    "        quantized_model_path = \"{}/model_quant.onnx\".format(directory_path)\n",
    "        quantize_dynamic(onnx_model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "\n",
    "    print(\"Uploading model to hub... (may take a few minutes)\")\n",
    "    push_to_hub(\n",
    "        save_dir = directory_path,\n",
    "        repository_id = model_id,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Models and Uplaoding them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting abstractive\n",
      "_________________________\n",
      "\n",
      "Exporting: facebook/bart-base narrativeqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /home/daedalus/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/vocab.json from cache at /home/daedalus/.cache/huggingface/transformers/43978bdeaa326572886b44fcfed82f932f76571095ce31973e51c3da8ccade7f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/merges.txt from cache at /home/daedalus/.cache/huggingface/transformers/3c167ed8af56e6605eeb794b63a79d65d85e6708c9b04408d41946337030f5cd.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer.json from cache at /home/daedalus/.cache/huggingface/transformers/a878fcd69bba037c9b1b227f4213579ae43d0aaa9374e167bc6c5f41b1cfeb30.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /home/daedalus/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /home/daedalus/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/adapters/models/bart/adapter_model.py:248: FutureWarning: This class has been renamed to `BartAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /home/daedalus/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/adapters/models/bart/adapter_model.py:226: FutureWarning: This class has been renamed to `BartAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "All model checkpoint weights were used when initializing BartModelWithHeads.\n",
      "\n",
      "All the weights of BartModelWithHeads were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartModelWithHeads for predictions without further training.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 8194.67it/s]\n",
      "Loading module configuration from /home/daedalus/.cache/huggingface/hub/models--AdapterHub--narrativeqa/snapshots/3777ff41e4d7c954f6132bb38ad542e1c9c71e17/adapter_config.json\n",
      "Adding adapter 'narrativeqa'.\n",
      "Loading module weights from /home/daedalus/.cache/huggingface/hub/models--AdapterHub--narrativeqa/snapshots/3777ff41e4d7c954f6132bb38ad542e1c9c71e17/pytorch_adapter.bin\n",
      "Loading module configuration from /home/daedalus/.cache/huggingface/hub/models--AdapterHub--narrativeqa/snapshots/3777ff41e4d7c954f6132bb38ad542e1c9c71e17/head_config.json\n",
      "Adding head 'narrativeqa' with config {'head_type': 'seq2seq_lm', 'vocab_size': 50265, 'layers': 1, 'activation_function': None, 'layer_norm': False, 'bias': False, 'shift_labels': False, 'label2id': None}.\n",
      "Loading module weights from /home/daedalus/.cache/huggingface/hub/models--AdapterHub--narrativeqa/snapshots/3777ff41e4d7c954f6132bb38ad542e1c9c71e17/pytorch_model_head.bin\n",
      "Some weights of the state_dict could not be loaded into model: final_logits_bias\n",
      "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /home/daedalus/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Using framework PyTorch: 1.13.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:250: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:257: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:289: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/adapters/layer.py:49: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if context.output_adapter_gating_scores:\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/adapters/composition.py:203: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor is not None and hidden_states.shape[0] != tensor.shape[0]:\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:944: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:107: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/adapters/context.py:117: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if getattr(ctx, \"output_\" + attr, False):\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/adapters/models/bart/adapter_model.py:96: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_ids is not None and x.shape[1] == input_ids.shape[1]:\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/adapters/models/bart/adapter_model.py:99: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  if len(torch.unique(eos_mask.sum(1))) > 1:\n",
      "Configuration saved in onnx/narrativeqa-onnx/config.json\n",
      "Model weights saved in onnx/narrativeqa-onnx/pytorch_model.bin\n",
      "tokenizer config file saved in onnx/narrativeqa-onnx/tokenizer_config.json\n",
      "Special tokens file saved in onnx/narrativeqa-onnx/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.0/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.0/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.0/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.0/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.1/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.1/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.2/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.2/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.3/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.3/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.4/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.4/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.5/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/encoder/layers.5/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.0/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.0/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.1/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.1/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.1/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.1/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.2/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.2/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.2/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.2/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.3/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.3/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.3/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.3/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.4/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.4/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.4/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.4/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.5/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.5/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.5/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.5/encoder_attn/MatMul_1]\n",
      "Uploading model to hub... (may take a few minutes)\n"
     ]
    }
   ],
   "source": [
    "available_skills = [\"span-extraction\", \"categorical\", \"multiple-choice\", \"abstractive\"]\n",
    "# available_skills = [\"span-extraction\"]\n",
    "#available_skills = [\"categorical\"]\n",
    "# available_skills = [\"multiple-choice\"]\n",
    "#available_skills = [\"abstractive\"]\n",
    "\n",
    "# available_adapters = [\"cosmos_qa\", \"multirc\", \"quartz\", \"race\", \"quail\"]\n",
    "available_adapters = [] # all \n",
    "\n",
    "all_skills = pd.read_csv(f'square_skills/all_skills.csv')\n",
    "\n",
    "for skill in available_skills:\n",
    "    skills = all_skills[all_skills[\"Type\"] == skill]\n",
    "\n",
    "    print(f\"Exporting {skill}\")\n",
    "    print(\"_________________________\\n\")\n",
    "    for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "        \n",
    "        if (not available_adapters) or (adapter in available_adapters): #if no adapter is specified -> loop over all. OR: is some are specified-> one do these. \n",
    "            print(f\"Exporting: {reader} {adapter}\")\n",
    "            onnx_export(reader, adapter, skill)\n",
    "        else:\n",
    "            print(f\"Ups. {reader} {adapter} not available yet.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readme(directory_path, base_model, adapter):\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "\n",
    "    readme_path = hf_hub_download(repo_id=adapter, filename=\"README.md\")\n",
    "\n",
    "    onnx_readme = \"{}/README.md\".format(directory_path)\n",
    "\n",
    "    skip = False\n",
    "    with open(readme_path, 'r') as src, open(onnx_readme, 'w') as dst:\n",
    "        for line in src:\n",
    "            # Insert onnx tag\n",
    "            if line == 'tags:\\n':\n",
    "                dst.write(\"inference: false\\n\")\n",
    "                dst.write(line)\n",
    "                dst.write('- onnx\\n')\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"# Adapter\"):\n",
    "                skip = True\n",
    "\n",
    "                # Insert custom README\n",
    "                dst.write(\"# ONNX export of \" + line[2:])\n",
    "                dst.write(f\"## Conversion of [{adapter}](https://huggingface.co/{adapter}) for UKP SQuARE\\n\\n\\n\")\n",
    "                dst.write(\"## Usage\\n\")\n",
    "                dst.write(\"```python\\n\")\n",
    "                dst.write(f\"onnx_path = hf_hub_download(repo_id='UKP-SQuARE/{model_id}', filename='model.onnx') # or model_quant.onnx for quantization\\n\")\n",
    "                dst.write(\"onnx_model = InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\\n\\n\")\n",
    "                dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "                dst.write(\"```\\n\\n\")\n",
    "\n",
    "            # Continue with normal model card\n",
    "            if line.startswith(\"## Architecture & Training\"): \n",
    "                skip = False\n",
    "\n",
    "            if not skip: \n",
    "                dst.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOnnxConfig(OnnxConfig):\n",
    "    # Inspired by BertONNXConfig, can be extended to support other QA tasks\n",
    "    @property\n",
    "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "        if self.task == \"multiple-choice\":\n",
    "            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n",
    "        else:\n",
    "            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n",
    "\n",
    "        return OrderedDict(\n",
    "                [\n",
    "                    (\"input_ids\", dynamic_axis),\n",
    "                    (\"attention_mask\", dynamic_axis),\n",
    "                    (\"token_type_ids\", dynamic_axis), # Roberta doesn't use this\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "def onnx_export(base_model, adapter_id, quantize_model=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    model = AutoModelWithHeads.from_pretrained(base_model)\n",
    "    \n",
    "    adapter = f\"AdapterHub/{base_model}-pf-{adapter_id}\"\n",
    "    adapter_name = model.load_adapter(adapter, source=\"hf\")\n",
    "    model.active_adapters = adapter_name\n",
    "\n",
    "    config = AutoConfig.from_pretrained(base_model)\n",
    "\n",
    "    if base_model.startswith(\"bert\"):\n",
    "        onnx_config = BertOnnxConfig(config, task=\"question-answering\")\n",
    "    elif base_model.startswith(\"roberta\"):\n",
    "        onnx_config = RobertaOnnxConfig(config, task=\"question-answering\")\n",
    "    else:\n",
    "        onnx_config = CustomOnnxConfig(config, task=\"question-answering\")\n",
    "        \n",
    "\n",
    "    # Generate the local directory in onnx/\n",
    "    model_id = adapter.split(\"/\")[1]+\"-onnx\"\n",
    "    directory_path = Path(\"onnx/{}\".format(model_id))\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_model_path = Path(\"{}/model.onnx\".format(directory_path))\n",
    "\n",
    "    # Export ONNX model\n",
    "    export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_model_path)\n",
    "\n",
    "    # Create config.json of vanilla model\n",
    "    model.save_pretrained(directory_path)\n",
    "    os.remove(directory_path / \"pytorch_model.bin\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(directory_path)\n",
    "\n",
    "    # Create README.md\n",
    "    generate_readme(directory_path, base_model, adapter)\n",
    "\n",
    "    if quantize_model:\n",
    "        quantized_model_path = \"{}/model_quant.onnx\".format(directory_path)\n",
    "        quantize_dynamic(onnx_model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "\n",
    "    print(\"Uploading model to hub... (may take a few minutes)\")\n",
    "    push_to_hub(\n",
    "        save_dir = directory_path,\n",
    "        repository_id = model_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a7f5a2c07603db35bc4e52cfd5b475adbf202ae824ea4c5e531d495460257f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
