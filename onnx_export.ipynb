{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import QuestionAnsweringPipeline, AutoAdapterModel, AutoModelWithHeads, AutoTokenizer, AutoConfig\n",
    "from transformers.onnx import OnnxConfig, validate_model_outputs, export\n",
    "\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "from huggingface_hub import HfApi, HfFolder, hf_hub_download\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Mapping, OrderedDict, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "\n",
    "from importlib import import_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappings from HF AutoConfig (prepended Onnx to get AutoOnnxConfig)\n",
    "CONFIG_MAPPING_NAMES = OrderedDict(\n",
    "    [\n",
    "        # Add configs here\n",
    "        (\"albert\", \"AlbertOnnxConfig\"),\n",
    "        (\"audio-spectrogram-transformer\", \"ASTOnnxConfig\"),\n",
    "        (\"bart\", \"BartOnnxConfig\"),\n",
    "        (\"beit\", \"BeitOnnxConfig\"),\n",
    "        (\"bert\", \"BertOnnxConfig\"),\n",
    "        (\"bert-generation\", \"BertGenerationOnnxConfig\"),\n",
    "        (\"big_bird\", \"BigBirdOnnxConfig\"),\n",
    "        (\"bigbird_pegasus\", \"BigBirdPegasusOnnxConfig\"),\n",
    "        (\"blenderbot\", \"BlenderbotOnnxConfig\"),\n",
    "        (\"blenderbot-small\", \"BlenderbotSmallOnnxConfig\"),\n",
    "        (\"bloom\", \"BloomOnnxConfig\"),\n",
    "        (\"camembert\", \"CamembertOnnxConfig\"),\n",
    "        (\"canine\", \"CanineOnnxConfig\"),\n",
    "        (\"chinese_clip\", \"ChineseCLIPOnnxConfig\"),\n",
    "        (\"clip\", \"CLIPOnnxConfig\"),\n",
    "        (\"clipseg\", \"CLIPSegOnnxConfig\"),\n",
    "        (\"codegen\", \"CodeGenOnnxConfig\"),\n",
    "        (\"conditional_detr\", \"ConditionalDetrOnnxConfig\"),\n",
    "        (\"convbert\", \"ConvBertOnnxConfig\"),\n",
    "        (\"convnext\", \"ConvNextOnnxConfig\"),\n",
    "        (\"ctrl\", \"CTRLOnnxConfig\"),\n",
    "        (\"cvt\", \"CvtOnnxConfig\"),\n",
    "        (\"data2vec-audio\", \"Data2VecAudioOnnxConfig\"),\n",
    "        (\"data2vec-text\", \"Data2VecTextOnnxConfig\"),\n",
    "        (\"data2vec-vision\", \"Data2VecVisionOnnxConfig\"),\n",
    "        (\"deberta\", \"DebertaOnnxConfig\"),\n",
    "        (\"deberta-v2\", \"DebertaV2OnnxConfig\"),\n",
    "        (\"decision_transformer\", \"DecisionTransformerOnnxConfig\"),\n",
    "        (\"deformable_detr\", \"DeformableDetrOnnxConfig\"),\n",
    "        (\"deit\", \"DeiTOnnxConfig\"),\n",
    "        (\"detr\", \"DetrOnnxConfig\"),\n",
    "        (\"dinat\", \"DinatOnnxConfig\"),\n",
    "        (\"distilbert\", \"DistilBertOnnxConfig\"),\n",
    "        (\"donut-swin\", \"DonutSwinOnnxConfig\"),\n",
    "        (\"dpr\", \"DPROnnxConfig\"),\n",
    "        (\"dpt\", \"DPTOnnxConfig\"),\n",
    "        (\"electra\", \"ElectraOnnxConfig\"),\n",
    "        (\"encoder-decoder\", \"EncoderDecoderOnnxConfig\"),\n",
    "        (\"ernie\", \"ErnieOnnxConfig\"),\n",
    "        (\"esm\", \"EsmOnnxConfig\"),\n",
    "        (\"flaubert\", \"FlaubertOnnxConfig\"),\n",
    "        (\"flava\", \"FlavaOnnxConfig\"),\n",
    "        (\"fnet\", \"FNetOnnxConfig\"),\n",
    "        (\"fsmt\", \"FSMTOnnxConfig\"),\n",
    "        (\"funnel\", \"FunnelOnnxConfig\"),\n",
    "        (\"glpn\", \"GLPNOnnxConfig\"),\n",
    "        (\"gpt2\", \"GPT2OnnxConfig\"),\n",
    "        (\"gpt_neo\", \"GPTNeoOnnxConfig\"),\n",
    "        (\"gpt_neox\", \"GPTNeoXOnnxConfig\"),\n",
    "        (\"gpt_neox_japanese\", \"GPTNeoXJapaneseOnnxConfig\"),\n",
    "        (\"gptj\", \"GPTJOnnxConfig\"),\n",
    "        (\"groupvit\", \"GroupViTOnnxConfig\"),\n",
    "        (\"hubert\", \"HubertOnnxConfig\"),\n",
    "        (\"ibert\", \"IBertOnnxConfig\"),\n",
    "        (\"imagegpt\", \"ImageGPTOnnxConfig\"),\n",
    "        (\"jukebox\", \"JukeboxOnnxConfig\"),\n",
    "        (\"layoutlm\", \"LayoutLMOnnxConfig\"),\n",
    "        (\"layoutlmv2\", \"LayoutLMv2OnnxConfig\"),\n",
    "        (\"layoutlmv3\", \"LayoutLMv3OnnxConfig\"),\n",
    "        (\"led\", \"LEDOnnxConfig\"),\n",
    "        (\"levit\", \"LevitOnnxConfig\"),\n",
    "        (\"lilt\", \"LiltOnnxConfig\"),\n",
    "        (\"longformer\", \"LongformerOnnxConfig\"),\n",
    "        (\"longt5\", \"LongT5OnnxConfig\"),\n",
    "        (\"luke\", \"LukeOnnxConfig\"),\n",
    "        (\"lxmert\", \"LxmertOnnxConfig\"),\n",
    "        (\"m2m_100\", \"M2M100OnnxConfig\"),\n",
    "        (\"marian\", \"MarianOnnxConfig\"),\n",
    "        (\"markuplm\", \"MarkupLMOnnxConfig\"),\n",
    "        (\"maskformer\", \"MaskFormerOnnxConfig\"),\n",
    "        (\"maskformer-swin\", \"MaskFormerSwinOnnxConfig\"),\n",
    "        (\"mbart\", \"MBartOnnxConfig\"),\n",
    "        (\"mctct\", \"MCTCTOnnxConfig\"),\n",
    "        (\"megatron-bert\", \"MegatronBertOnnxConfig\"),\n",
    "        (\"mobilebert\", \"MobileBertOnnxConfig\"),\n",
    "        (\"mobilenet_v1\", \"MobileNetV1OnnxConfig\"),\n",
    "        (\"mobilenet_v2\", \"MobileNetV2OnnxConfig\"),\n",
    "        (\"mobilevit\", \"MobileViTOnnxConfig\"),\n",
    "        (\"mpnet\", \"MPNetOnnxConfig\"),\n",
    "        (\"mt5\", \"MT5OnnxConfig\"),\n",
    "        (\"mvp\", \"MvpOnnxConfig\"),\n",
    "        (\"nat\", \"NatOnnxConfig\"),\n",
    "        (\"nezha\", \"NezhaOnnxConfig\"),\n",
    "        (\"nystromformer\", \"NystromformerOnnxConfig\"),\n",
    "        (\"openai-gpt\", \"OpenAIGPTOnnxConfig\"),\n",
    "        (\"opt\", \"OPTOnnxConfig\"),\n",
    "        (\"owlvit\", \"OwlViTOnnxConfig\"),\n",
    "        (\"pegasus\", \"PegasusOnnxConfig\"),\n",
    "        (\"pegasus_x\", \"PegasusXOnnxConfig\"),\n",
    "        (\"perceiver\", \"PerceiverOnnxConfig\"),\n",
    "        (\"plbart\", \"PLBartOnnxConfig\"),\n",
    "        (\"poolformer\", \"PoolFormerOnnxConfig\"),\n",
    "        (\"prophetnet\", \"ProphetNetOnnxConfig\"),\n",
    "        (\"qdqbert\", \"QDQBertOnnxConfig\"),\n",
    "        (\"rag\", \"RagOnnxConfig\"),\n",
    "        (\"realm\", \"RealmOnnxConfig\"),\n",
    "        (\"reformer\", \"ReformerOnnxConfig\"),\n",
    "        (\"regnet\", \"RegNetOnnxConfig\"),\n",
    "        (\"rembert\", \"RemBertOnnxConfig\"),\n",
    "        (\"resnet\", \"ResNetOnnxConfig\"),\n",
    "        (\"retribert\", \"RetriBertOnnxConfig\"),\n",
    "        (\"roberta\", \"RobertaOnnxConfig\"),\n",
    "        (\"roc_bert\", \"RoCBertOnnxConfig\"),\n",
    "        (\"roformer\", \"RoFormerOnnxConfig\"),\n",
    "        (\"segformer\", \"SegformerOnnxConfig\"),\n",
    "        (\"sew\", \"SEWOnnxConfig\"),\n",
    "        (\"sew-d\", \"SEWDOnnxConfig\"),\n",
    "        (\"speech-encoder-decoder\", \"SpeechEncoderDecoderOnnxConfig\"),\n",
    "        (\"speech_to_text\", \"Speech2TextOnnxConfig\"),\n",
    "        (\"speech_to_text_2\", \"Speech2Text2OnnxConfig\"),\n",
    "        (\"splinter\", \"SplinterOnnxConfig\"),\n",
    "        (\"squeezebert\", \"SqueezeBertOnnxConfig\"),\n",
    "        (\"swin\", \"SwinOnnxConfig\"),\n",
    "        (\"swinv2\", \"Swinv2OnnxConfig\"),\n",
    "        (\"switch_transformers\", \"SwitchTransformersOnnxConfig\"),\n",
    "        (\"t5\", \"T5OnnxConfig\"),\n",
    "        (\"table-transformer\", \"TableTransformerOnnxConfig\"),\n",
    "        (\"tapas\", \"TapasOnnxConfig\"),\n",
    "        (\"time_series_transformer\", \"TimeSeriesTransformerOnnxConfig\"),\n",
    "        (\"trajectory_transformer\", \"TrajectoryTransformerOnnxConfig\"),\n",
    "        (\"transfo-xl\", \"TransfoXLOnnxConfig\"),\n",
    "        (\"trocr\", \"TrOCROnnxConfig\"),\n",
    "        (\"unispeech\", \"UniSpeechOnnxConfig\"),\n",
    "        (\"unispeech-sat\", \"UniSpeechSatOnnxConfig\"),\n",
    "        (\"van\", \"VanOnnxConfig\"),\n",
    "        (\"videomae\", \"VideoMAEOnnxConfig\"),\n",
    "        (\"vilt\", \"ViltOnnxConfig\"),\n",
    "        (\"vision-encoder-decoder\", \"VisionEncoderDecoderOnnxConfig\"),\n",
    "        (\"vision-text-dual-encoder\", \"VisionTextDualEncoderOnnxConfig\"),\n",
    "        (\"visual_bert\", \"VisualBertOnnxConfig\"),\n",
    "        (\"vit\", \"ViTOnnxConfig\"),\n",
    "        (\"vit_mae\", \"ViTMAEOnnxConfig\"),\n",
    "        (\"vit_msn\", \"ViTMSNOnnxConfig\"),\n",
    "        (\"wav2vec2\", \"Wav2Vec2OnnxConfig\"),\n",
    "        (\"wav2vec2-conformer\", \"Wav2Vec2ConformerOnnxConfig\"),\n",
    "        (\"wavlm\", \"WavLMOnnxConfig\"),\n",
    "        (\"whisper\", \"WhisperOnnxConfig\"),\n",
    "        (\"xclip\", \"XCLIPOnnxConfig\"),\n",
    "        (\"xglm\", \"XGLMOnnxConfig\"),\n",
    "        (\"xlm\", \"XLMOnnxConfig\"),\n",
    "        (\"xlm-prophetnet\", \"XLMProphetNetOnnxConfig\"),\n",
    "        (\"xlm-roberta\", \"XLMRobertaOnnxConfig\"),\n",
    "        (\"xlm-roberta-xl\", \"XLMRobertaXLOnnxConfig\"),\n",
    "        (\"xlnet\", \"XLNetOnnxConfig\"),\n",
    "        (\"yolos\", \"YolosOnnxConfig\"),\n",
    "        (\"yoso\", \"YosoOnnxConfig\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def auto_onnx_config(model_name: str, task: str) -> OnnxConfig:\n",
    "    \"\"\"\n",
    "    Returns a HF onnx config for the given model name if it exists\n",
    "    @ model_name: the name of the model (e.g. facebook/bart-base)\n",
    "    @ task: the task for which the model is used (e.g. question-answering)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "        # Assumes that the identifier of model_name \"facebook/bart-base\" is \"bart\"\n",
    "        identifier = model_name.split(\"/\")[-1].split(\"-\")[0]\n",
    "\n",
    "        config_name = CONFIG_MAPPING_NAMES[identifier]\n",
    "        config_class = import_module(f\"transformers.models.{identifier}\")\n",
    "        auto_onnx_config = getattr(config_class, config_name)\n",
    "        return auto_onnx_config.from_model_config(config, task=task)\n",
    "    except:\n",
    "        raise ValueError(f\"Could not find an AutoOnnxConfig for model {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readme(directory_path: str, base_model: str, skill: str, model_id: str, adapter: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Generates a README.md file for the exported model\n",
    "    @ directory_path: the path to the directory where the README.md file will be generated\n",
    "    @ base_model: the name of the base model (e.g. facebook/bart-base)\n",
    "    @ skill: the skill for which the model is exported (e.g. question-answering)\n",
    "    @ model_id: the id of the model (e.g. bart-base-pf-narrativeqa-onnx)\n",
    "    @ adapter: the adapter used for the export (if model is an adapter model)\n",
    "    \"\"\"\n",
    "\n",
    "    onnx_readme = \"{}/README.md\".format(directory_path)\n",
    "\n",
    "    if adapter is None:\n",
    "        readme_path = hf_hub_download(repo_id=base_model, filename=\"README.md\")\n",
    "\n",
    "        inserted_headline = False\n",
    "        with open(readme_path, 'r') as src, open(onnx_readme, 'w') as dst:\n",
    "            for line in src:\n",
    "                # Insert onnx tag\n",
    "                if line == 'tags:\\n':\n",
    "                    dst.write(\"inference: false\\n\")\n",
    "                    dst.write(line)\n",
    "                    dst.write('- onnx\\n')\n",
    "                    continue\n",
    "\n",
    "                if line.startswith(\"# \") and not inserted_headline:\n",
    "                    inserted_headline = True\n",
    "                    dst.write(\"# ONNX export of \" + base_model + \"\\n\")\n",
    "                    continue\n",
    "\n",
    "                dst.write(line)\n",
    "    else:\n",
    "        readme_path = hf_hub_download(repo_id=adapter, filename=\"README.md\")\n",
    "\n",
    "        skip = False\n",
    "        with open(readme_path, 'r') as src, open(onnx_readme, 'w') as dst:\n",
    "            for line in src:\n",
    "                # Insert onnx tag\n",
    "                if line == 'tags:\\n':\n",
    "                    dst.write(\"inference: false\\n\")\n",
    "                    dst.write(line)\n",
    "                    dst.write('- onnx\\n')\n",
    "                    continue\n",
    "\n",
    "                if line.startswith(\"# Adapter\"):\n",
    "                    skip = True\n",
    "\n",
    "                    # Insert custom README\n",
    "                    dst.write(\"# ONNX export of \" + line[2:])\n",
    "                    dst.write(f\"## Conversion of [{adapter}](https://huggingface.co/{adapter}) for UKP SQuARE\\n\\n\\n\")\n",
    "                    dst.write(\"## Usage\\n\")\n",
    "                    dst.write(\"```python\\n\")\n",
    "                    dst.write(f\"onnx_path = hf_hub_download(repo_id='UKP-SQuARE/{model_id}', filename='model.onnx') # or model_quant.onnx for quantization\\n\")\n",
    "                    dst.write(\"onnx_model = InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\\n\\n\")\n",
    "\n",
    "                    if (skill == \"span-extraction\"):\n",
    "                        dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                        dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                        dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                        dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                        dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "                        dst.write(\"```\\n\\n\")\n",
    "\n",
    "                    elif (skill == \"categorical\"):\n",
    "                        dst.write(\"context = 'English orthography typically represents vowel sounds with the five conventional vowel letters ⟨a, e, i, o, u⟩, as well as ⟨y⟩, which may also be a consonant depending on context. However, outside of abbreviations, there are a handful of words in English that do not have vowels, either because the vowel sounds are not written with vowel letters or because the words themselves are pronounced without vowel sounds'.\\n\")\n",
    "                        dst.write(\"question = 'can there be a word without a vowel'\\n\")\n",
    "                        dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                        dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                        dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "                        dst.write(\"```\\n\\n\")\n",
    "\n",
    "                    elif skill == \"multiple-choice\":\n",
    "                        dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                        dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                        dst.write('choices = [\"Cat\", \"Horse\", \"Tiger\", \"Fish\"]')\n",
    "\n",
    "                        dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "\n",
    "                        dst.write(\"raw_input = [[context, question + \" \" + choice] for choice in choices]\\n\")\n",
    "                        dst.write('inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"np\")\\n')\n",
    "\n",
    "                        dst.write(\"inputs['token_type_ids'] = np.expand_dims(inputs['token_type_ids'], axis=0)\\n\")\n",
    "                        dst.write(\"inputs['input_ids'] =  np.expand_dims(inputs['input_ids'], axis=0)\\n\")\n",
    "                        dst.write(\"inputs['attention_mask'] =  np.expand_dims(inputs['attention_mask'], axis=0)\\n\")\n",
    "                        dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "\n",
    "                        dst.write(\"```\\n\\n\")\n",
    "\n",
    "                    elif skill == \"abstractive\":\n",
    "                        dst.write(\"context = 'ONNX is an open format to represent models. The benefits of using ONNX include interoperability of frameworks and hardware optimization.'\\n\")\n",
    "                        dst.write(\"question = 'What are advantages of ONNX?'\\n\")\n",
    "                        dst.write(f\"tokenizer = AutoTokenizer.from_pretrained('UKP-SQuARE/{model_id}')\\n\\n\")\n",
    "                        dst.write(\"inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors='np')\\n\")\n",
    "                        dst.write(\"outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\\n\")\n",
    "                        dst.write(\"```\\n\\n\")\n",
    "\n",
    "\n",
    "                # Continue with normal model card\n",
    "                if line.startswith(\"## Architecture & Training\"): \n",
    "                    skip = False\n",
    "\n",
    "                if not skip: \n",
    "                    dst.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hub(save_dir:str , repository_id: str):\n",
    "    \"\"\"\n",
    "    Pushes the model to the HuggingFace Hub\n",
    "    @param save_dir: The directory where the model is saved\n",
    "    @param repository_id: The name of the repository\n",
    "    \"\"\"\n",
    "\n",
    "    huggingface_token = HfFolder.get_token()\n",
    "    api = HfApi()\n",
    "\n",
    "    api.create_repo(\n",
    "        token=huggingface_token,\n",
    "        repo_id=f'UKP-SQuARE/{repository_id}',\n",
    "        exist_ok=True,\n",
    "        private=False\n",
    "    )\n",
    "\n",
    "    for path, subdirs, files in os.walk(save_dir):\n",
    "        for name in files:\n",
    "            local_file_path = os.path.join(path, name)\n",
    "            _, hub_file_path = os.path.split(local_file_path)\n",
    "            try:\n",
    "                api.upload_file(\n",
    "                    token=huggingface_token,\n",
    "                    repo_id=f\"UKP-SQuARE/{repository_id}\",\n",
    "                    path_or_fileobj=os.path.join(os.getcwd(), local_file_path),\n",
    "                    path_in_repo=hub_file_path,\n",
    "                )\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except NameError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnx_export(model_name: str, skill: str, quantize_model: bool=True, adapter_id: Optional[str]=None, custom_onnx_config: Optional[str]=None):\n",
    "    \"\"\"\n",
    "    Exports a model to ONNX format.\n",
    "    @param model_name: The name of the model to be exported.\n",
    "    @param skill: The skill of the model to be exported.\n",
    "    @param quantize_model: Whether to quantize the model.\n",
    "    @param adapter_id: The id of the adapter to be used (if the model is from the AdapterHub).\n",
    "    @param custom_onnx_config: The path to a custom config file (if not specified, we try to infer the config from the model)\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelWithHeads.from_pretrained(model_name)\n",
    "\n",
    "    model_slur = model_name.split(\"/\")[-1]\n",
    "\n",
    "    if adapter_id is not None:\n",
    "        adapter = f\"AdapterHub/{model_slur}-pf-{adapter_id}\"\n",
    "        adapter_name = model.load_adapter(adapter, source=\"hf\")\n",
    "        model.active_adapters = adapter_name\n",
    "        model_id = adapter.split(\"/\")[-1]+\"-onnx\"\n",
    "    else:\n",
    "        model_id = model_slur+\"-onnx\"\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "    if custom_onnx_config is not None:\n",
    "        class CustomOnnxConfig(OnnxConfig):\n",
    "            @property\n",
    "            def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "                return OrderedDict({k: {int(k2): v2 for k2, v2 in v.items()} for k, v in json.loads(custom_onnx_config).items()})\n",
    "\n",
    "        onnx_config = CustomOnnxConfig(model_name, skill)\n",
    "\n",
    "    else:\n",
    "        onnx_config = auto_onnx_config(model_name, skill)\n",
    "    \n",
    "    # Generate the local directory in onnx/\n",
    "    directory_path = Path(\"onnx/{}\".format(model_id))\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_model_path = Path(\"{}/model.onnx\".format(directory_path))\n",
    "\n",
    "    # Export ONNX model\n",
    "    export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_model_path)\n",
    "\n",
    "    # Create config.json of vanilla model\n",
    "    model.save_pretrained(directory_path)\n",
    "    os.remove(directory_path / \"pytorch_model.bin\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(directory_path)\n",
    "\n",
    "    # Create README.md\n",
    "    generate_readme(directory_path, model_name, skill, model_id, adapter if adapter_id is not None else None)\n",
    "\n",
    "    if quantize_model:\n",
    "        quantized_model_path = \"{}/model_quant.onnx\".format(directory_path)\n",
    "        quantize_dynamic(onnx_model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "\n",
    "    print(\"Uploading model to hub... (may take a few minutes)\")\n",
    "    push_to_hub(\n",
    "        save_dir = directory_path,\n",
    "        repository_id = model_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/adapters/models/distilbert/adapter_model.py:277: FutureWarning: This class has been renamed to `DistilBertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/adapters/models/distilbert/adapter_model.py:255: FutureWarning: This class has been renamed to `DistilBertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModelWithHeads: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:227: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n",
      "/home/daedalus/anaconda3/envs/adapter/lib/python3.9/site-packages/transformers/adapters/composition.py:203: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor is not None and hidden_states.shape[0] != tensor.shape[0]:\n",
      "Downloading: 100%|██████████| 8.56k/8.56k [00:00<00:00, 4.73MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.0/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.0/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.1/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.1/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.2/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.2/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.3/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.3/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.4/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.4/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.5/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.5/attention/MatMul_1]\n",
      "Uploading model to hub... (may take a few minutes)\n"
     ]
    }
   ],
   "source": [
    "x = OrderedDict(\n",
    "            [\n",
    "                (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n",
    "                (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n",
    "            ])\n",
    "\n",
    "custom_onnx_config = json.dumps(x)\n",
    "onnx_export('distilbert-base-uncased', 'default', custom_onnx_config=custom_onnx_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapterhub_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, May 24 2022, 21:28:31) \n[Clang 13.1.6 (clang-1316.0.21.2)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dff476688330005cfc33f1ee0f15c13ae533c265ccd041ab146cdb98ecc6219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
