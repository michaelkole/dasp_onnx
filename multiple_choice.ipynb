{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import QuestionAnsweringPipeline, AutoAdapterModel, AutoModelWithHeads, AutoTokenizer, AutoConfig\n",
    "from transformers.onnx import OnnxConfig, validate_model_outputs, export\n",
    "from transformers.models.bert import BertOnnxConfig\n",
    "from transformers.onnx.features import FeaturesManager\n",
    "\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from onnxruntime import InferenceSession\n",
    "import onnxruntime\n",
    "\n",
    "from onnx_opcounter import calculate_params\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_metric, load_dataset\n",
    "\n",
    "from typing import Mapping, OrderedDict\n",
    "from pathlib import Path\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working and semi working \n",
    "# cosmos_qa, multirc, quartz, race und quail  \n",
    "\n",
    "# input diff\n",
    "# commonsense_qa und social_i_qa \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnx_inference(tokenizer, question, context, choices, onnx_model):\n",
    "    # onnx_model = InferenceSession(\n",
    "    #     str(onnx_path), providers=[\"CPUExecutionProvider\"]\n",
    "    # )\n",
    "\n",
    "    \n",
    "\n",
    "    raw_input = [[context, question + \" \" + choice] for choice in choices]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"np\")\n",
    "\n",
    "    inputs['token_type_ids'] = np.expand_dims(inputs['token_type_ids'], axis=0)\n",
    "    inputs['input_ids'] =  np.expand_dims(inputs['input_ids'], axis=0)\n",
    "    inputs['attention_mask'] =  np.expand_dims(inputs['attention_mask'], axis=0)\n",
    "\n",
    "    outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\n",
    "\n",
    "    answer_idx = np.argmax(outputs[0])\n",
    "    return choices[answer_idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working adapter models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cosmos_qa\n"
     ]
    }
   ],
   "source": [
    "# adapters based on sequence options \n",
    "adapter_list = [\"cosmos_qa\", \"multirc\", \"quartz\", \"race\", \"quail\"]\n",
    "adapter = adapter_list[0]\n",
    "print(f\"Using {adapter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 2811.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horse\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "adapter_name = model.load_adapter(f\"AdapterHub/bert-base-uncased-pf-{adapter}\", source=\"hf\")\n",
    "\n",
    "model.active_adapters = adapter_name\n",
    "\n",
    "def mc_model_inference(question, context, choices):\n",
    "    outputs = []\n",
    "\n",
    "    raw_input = [[context, question + \" \" + choice] for choice in choices]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_idx = torch.argmax(outputs.logits)\n",
    "    return choices[answer_idx]\n",
    "\n",
    "question = \"What animal has the most hair?\"\n",
    "context = \"Fish are typically not hairy. Cats have 10g of hair. Tigers have 12g of hair. Horses have 100g of hair.\"\n",
    "\n",
    "choices = [\"Cat\", \"Horse\", \"Tiger\", \"Fish\"]\n",
    "answer = mc_model_inference(question, context, choices)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_onnx, model_onnx_quant, as_list=False):\n",
    "    local_onnx_model = onnxruntime.InferenceSession(model_onnx, providers=[\"CPUExecutionProvider\"])\n",
    "    local_onnx_model_quant = onnxruntime.InferenceSession(model_onnx_quant, providers=[\"CPUExecutionProvider\"])\n",
    "    \n",
    "    so = onnxruntime.SessionOptions()\n",
    "    so.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    \n",
    "    local_onnx_model_opt = onnxruntime.InferenceSession(model_onnx, so)\n",
    "    local_onnx_model_quant_opt = onnxruntime.InferenceSession(model_onnx_quant, so)\n",
    "    \n",
    "    if as_list:\n",
    "        return [local_onnx_model, local_onnx_model_opt, local_onnx_model_quant, local_onnx_model_quant_opt]\n",
    "    return local_onnx_model, local_onnx_model_opt, local_onnx_model_quant, local_onnx_model_quant_opt\n",
    "\n",
    "def repo_builder(reader, adapter):\n",
    "    repo_id = f\"UKP-SQuARE/{reader}-pf-{adapter}-onnx\"\n",
    "    filename_onnx = \"model.onnx\"\n",
    "    filename_onnx_quant = \"model_quant.onnx\"\n",
    "\n",
    "    model_onnx = hf_hub_download(repo_id=repo_id, filename=filename_onnx)\n",
    "    model_onnx_quant = hf_hub_download(repo_id=repo_id, filename=filename_onnx_quant)\n",
    "\n",
    "    return model_onnx, model_onnx_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cosmos_qa (/Users/michaelhermann/.cache/huggingface/datasets/cosmos_qa/default/0.1.0/3e18538cbfdb2c04189b16642715f0f6da3e97ed5df0aadcec3641245b2cf157)\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"cosmos_qa\", split=f\"validation[:{10}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '3BFF0DJK8XA7YNK4QYIGCOG1A95STE##3180JW2OT5AF02OISBX66RFOCTG5J7##A2LTOS0AZ3B28A##Blog_56156##q1_a1##378G7J1SJNCDAAIN46FM2P7T6KZEW2',\n",
       " 'context': 'Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ?',\n",
       " 'question': 'Why is this person asking about divorce ?',\n",
       " 'answer0': 'If he gets married in the church he wo nt have to get a divorce .',\n",
       " 'answer1': 'He wants to get married to a different person .',\n",
       " 'answer2': 'He wants to know if he does nt like this girl can he divorce her ?',\n",
       " 'answer3': 'None of the above choices .',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX: He wants to know if he does nt like this girl can he divorce her ? True\n",
      "ONNX-OPT: He wants to know if he does nt like this girl can he divorce her ? True\n",
      "ONNX Quantized: He wants to know if he does nt like this girl can he divorce her ? True\n",
      "ONNX-OPT Quantized: He wants to know if he does nt like this girl can he divorce her ? True\n",
      "ONNX: He knows that he will be committing polygamy . True\n",
      "ONNX-OPT: He knows that he will be committing polygamy . True\n",
      "ONNX Quantized: He knows that he will be committing polygamy . True\n",
      "ONNX-OPT Quantized: He knows that he will be committing polygamy . True\n",
      "ONNX: The bus would arrive at the stop soon True\n",
      "ONNX-OPT: The bus would arrive at the stop soon True\n",
      "ONNX Quantized: An ambulance would likely come to the scene False\n",
      "ONNX-OPT Quantized: An ambulance would likely come to the scene False\n",
      "ONNX: Medical personnel would come to help the old man True\n",
      "ONNX-OPT: Medical personnel would come to help the old man True\n",
      "ONNX Quantized: The bus would arrive at the stop soon False\n",
      "ONNX-OPT Quantized: The bus would arrive at the stop soon False\n",
      "ONNX: He likely fell on the sidewalk and hit his head while intoxicated True\n",
      "ONNX-OPT: He likely fell on the sidewalk and hit his head while intoxicated True\n",
      "ONNX Quantized: He likely fell on the sidewalk and hit his head while intoxicated True\n",
      "ONNX-OPT Quantized: He likely fell on the sidewalk and hit his head while intoxicated True\n",
      "ONNX: He was waiting on a ride True\n",
      "ONNX-OPT: He was waiting on a ride True\n",
      "ONNX Quantized: He was waiting on a ride True\n",
      "ONNX-OPT Quantized: He was waiting on a ride True\n",
      "ONNX: Because it was 10 minutes before the start of school . True\n",
      "ONNX-OPT: Because it was 10 minutes before the start of school . True\n",
      "ONNX Quantized: Because it was late . False\n",
      "ONNX-OPT Quantized: Because it was late . False\n",
      "ONNX: I was able to separate myself from the wold . True\n",
      "ONNX-OPT: I was able to separate myself from the wold . True\n",
      "ONNX Quantized: I was able to separate myself from the wold . True\n",
      "ONNX-OPT Quantized: I was able to separate myself from the wold . True\n",
      "ONNX: It is realistically not important at all but I like it . True\n",
      "ONNX-OPT: It is realistically not important at all but I like it . True\n",
      "ONNX Quantized: It is realistically not important at all but I like it . True\n",
      "ONNX-OPT Quantized: It is realistically not important at all but I like it . True\n",
      "ONNX: None of the above choices . True\n",
      "ONNX-OPT: None of the above choices . True\n",
      "ONNX Quantized: None of the above choices . True\n",
      "ONNX-OPT Quantized: None of the above choices . True\n"
     ]
    }
   ],
   "source": [
    "#load onnx models\n",
    "model_onnx, model_onnx_quant = repo_builder(\"bert-base-uncased\", adapter)\n",
    "onnx_models_list = load_model(model_onnx, model_onnx_quant, as_list=True)\n",
    "onnx_models_name_helper_list = [\"ONNX\", \"ONNX-OPT\", \"ONNX Quantized\", \"ONNX-OPT Quantized\"]\n",
    "\n",
    "\n",
    "# data = load_dataset(\"cosmos_qa\", split=f\"validation[:{10}]\")\n",
    "for example in data:\n",
    "    question = example[\"question\"]\n",
    "    choices = [example[\"answer0\"], example[\"answer1\"], example[\"answer2\"], example[\"answer3\"]]\n",
    "\n",
    "    base_answer = mc_model_inference(question, context, choices)\n",
    "\n",
    "    # eval onnx models\n",
    "    for onnx_model, onnx_model_name in zip(onnx_models_list, onnx_models_name_helper_list):\n",
    "        # performance_log(reader , adapter, \"seq_length\", onnx_model_name, onnx_model, tokenizer, data)\n",
    "        # answer = mc_model_inference(question, context, choices)\n",
    "        answer = onnx_inference(tokenizer, question, context, choices, onnx_model)\n",
    "\n",
    "        print(f\"{onnx_model_name}: {answer} {base_answer == answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/layer.py:49: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if context.output_adapter_gating_scores:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/composition.py:202: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor is not None and hidden_states.shape[0] != tensor.shape[0]:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/context.py:117: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if getattr(ctx, \"output_\" + attr, False):\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'onnx/cosmos_qa/model.onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m onnx_config \u001b[39m=\u001b[39m BertOnnxConfig(config, task \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmultiple-choice\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m onnx_path \u001b[39m=\u001b[39m Path(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39monnx/\u001b[39m\u001b[39m{\u001b[39;00madapter\u001b[39m}\u001b[39;00m\u001b[39m/model.onnx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m onnx_inputs, onnx_outputs \u001b[39m=\u001b[39m export(tokenizer, model, onnx_config, onnx_config\u001b[39m.\u001b[39;49mdefault_onnx_opset, onnx_path)\n\u001b[1;32m      8\u001b[0m answer \u001b[39m=\u001b[39m onnx_inference(tokenizer, onnx_path, question, context, choices)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(answer)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/onnx/convert.py:336\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(preprocessor, model, config, opset, output, tokenizer, device)\u001b[0m\n\u001b[1;32m    330\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    331\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported PyTorch version for this model. Minimum required is \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39mtorch_onnx_minimum_version\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m got: \u001b[39m\u001b[39m{\u001b[39;00mtorch_version\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    333\u001b[0m         )\n\u001b[1;32m    335\u001b[0m \u001b[39mif\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(model), PreTrainedModel):\n\u001b[0;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m export_pytorch(preprocessor, model, config, opset, output, tokenizer\u001b[39m=\u001b[39;49mtokenizer, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    337\u001b[0m \u001b[39melif\u001b[39;00m is_tf_available() \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(model), TFPreTrainedModel):\n\u001b[1;32m    338\u001b[0m     \u001b[39mreturn\u001b[39;00m export_tensorflow(preprocessor, model, config, opset, output, tokenizer\u001b[39m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/onnx/convert.py:191\u001b[0m, in \u001b[0;36mexport_pytorch\u001b[0;34m(preprocessor, model, config, opset, output, tokenizer, device)\u001b[0m\n\u001b[1;32m    189\u001b[0m                     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    190\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m             onnx_export(\n\u001b[1;32m    192\u001b[0m                 model,\n\u001b[1;32m    193\u001b[0m                 (model_inputs,),\n\u001b[1;32m    194\u001b[0m                 f\u001b[39m=\u001b[39;49moutput\u001b[39m.\u001b[39;49mas_posix(),\n\u001b[1;32m    195\u001b[0m                 input_names\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(config\u001b[39m.\u001b[39;49minputs\u001b[39m.\u001b[39;49mkeys()),\n\u001b[1;32m    196\u001b[0m                 output_names\u001b[39m=\u001b[39;49monnx_outputs,\n\u001b[1;32m    197\u001b[0m                 dynamic_axes\u001b[39m=\u001b[39;49m{name: axes \u001b[39mfor\u001b[39;49;00m name, axes \u001b[39min\u001b[39;49;00m chain(config\u001b[39m.\u001b[39;49minputs\u001b[39m.\u001b[39;49mitems(), config\u001b[39m.\u001b[39;49moutputs\u001b[39m.\u001b[39;49mitems())},\n\u001b[1;32m    198\u001b[0m                 do_constant_folding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    199\u001b[0m                 opset_version\u001b[39m=\u001b[39;49mopset,\n\u001b[1;32m    200\u001b[0m             )\n\u001b[1;32m    202\u001b[0m         config\u001b[39m.\u001b[39mrestore_ops()\n\u001b[1;32m    204\u001b[0m \u001b[39mreturn\u001b[39;00m matched_inputs, onnx_outputs\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/onnx/utils.py:504\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    188\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     export_modules_as_functions: Union[\u001b[39mbool\u001b[39m, Collection[Type[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule]]] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    205\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m     _export(\n\u001b[1;32m    505\u001b[0m         model,\n\u001b[1;32m    506\u001b[0m         args,\n\u001b[1;32m    507\u001b[0m         f,\n\u001b[1;32m    508\u001b[0m         export_params,\n\u001b[1;32m    509\u001b[0m         verbose,\n\u001b[1;32m    510\u001b[0m         training,\n\u001b[1;32m    511\u001b[0m         input_names,\n\u001b[1;32m    512\u001b[0m         output_names,\n\u001b[1;32m    513\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    514\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    515\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    516\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    517\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    518\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    519\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    520\u001b[0m     )\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/onnx/utils.py:1604\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[39mif\u001b[39;00m export_type \u001b[39m==\u001b[39m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE:\n\u001b[1;32m   1603\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(export_map) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1604\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39;49mserialization\u001b[39m.\u001b[39;49m_open_file_like(f, \u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m   1605\u001b[0m         opened_file\u001b[39m.\u001b[39mwrite(proto)\n\u001b[1;32m   1606\u001b[0m \u001b[39melif\u001b[39;00m export_type \u001b[39min\u001b[39;00m [\n\u001b[1;32m   1607\u001b[0m     _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mZIP_ARCHIVE,\n\u001b[1;32m   1608\u001b[0m     _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mCOMPRESSED_ZIP_ARCHIVE,\n\u001b[1;32m   1609\u001b[0m ]:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'onnx/cosmos_qa/model.onnx'"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "onnx_config = BertOnnxConfig(config, task = \"multiple-choice\")\n",
    "\n",
    "onnx_path = Path(f\"onnx/{adapter}/model.onnx\")\n",
    "\n",
    "onnx_inputs, onnx_outputs = export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_path)\n",
    "\n",
    "answer = onnx_inference(tokenizer, onnx_path, question, context, choices)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non working Adapter models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"commonsense_qa\", \"social_i_qa\" \n",
    "# ony diff -> need diff amount of choices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### commonsense_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4133.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "adapter = \"commonsense_qa\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "adapter_name = model.load_adapter(f\"AdapterHub/bert-base-uncased-pf-{adapter}\", source=\"hf\")\n",
    "model.active_adapters = adapter_name\n",
    "\n",
    "def mc_model_inference(question, question_concept, choices):\n",
    "    outputs = []\n",
    "    \n",
    "    raw_input = [[question_concept, question + \" \" + choice] for choice in choices]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_idx = torch.argmax(outputs.logits)\n",
    "    return choices[answer_idx]\n",
    "\n",
    "question = \"The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\"\n",
    "question_concept = \"punishing\"\n",
    "choices = [\"ignore\", \"enforce\", \"authoritarian\", \"yell at\", \"avoid\"] #len 5\n",
    "\n",
    "answer = mc_model_inference(question, question_concept, choices)\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social_i_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4140.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# social_i_qa\n",
    "adapter = \"social_i_qa\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "adapter_name = model.load_adapter(f\"AdapterHub/bert-base-uncased-pf-{adapter}\", source=\"hf\")\n",
    "\n",
    "model.active_adapters = adapter_name\n",
    "\n",
    "def mc_model_inference(question, context, choices):\n",
    "    outputs = []\n",
    "\n",
    "    raw_input = [[context, question + \" \" + choice] for choice in choices]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_idx = torch.argmax(outputs.logits)\n",
    "    return choices[answer_idx]\n",
    "\n",
    "question = \"What animal has the most hair?\"\n",
    "context = \"Fish are typically not hairy. Cats have 10g of hair. Tigers have 12g of hair. Horses have 100g of hair.\"\n",
    "\n",
    "choices = [\"Cat\", \"Horse\", \"Tiger\"] ## only diff: 3 instead of 5 or 4 choices\n",
    "answer = mc_model_inference(question, context, choices)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOnnxConfig(OnnxConfig):\n",
    "    @property\n",
    "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "        \n",
    "        return OrderedDict(\n",
    "                [\n",
    "                    (\"input_ids\", {0: \"batch\", 1: \"choice\"}),\n",
    "                    (\"attention_mask\", {0: \"batch\", 1: \"choice\"}),\n",
    "                    # (\"token_type_ids\", {0: \"batch\", 1: \"choice\", 2: \"sequence\"}), # Roberta doesn't use this\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx/social_i_qa/model.onnx\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "# onnx_config = BertOnnxConfig(config, task = \"multiple-choice\")\n",
    "\n",
    "onnx_config = CustomOnnxConfig(config)\n",
    "\n",
    "onnx_path = Path(f\"onnx/{adapter}/model.onnx\")\n",
    "print(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('last_hidden_state', {0: 'batch', 1: 'sequence'})])\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(onnx_config.outputs)\n",
    "print(onnx_config.default_onnx_opset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/layer.py:49: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if context.output_adapter_gating_scores:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/composition.py:202: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor is not None and hidden_states.shape[0] != tensor.shape[0]:\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/context.py:117: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if getattr(ctx, \"output_\" + attr, False):\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 3]' is invalid for input of size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m export(tokenizer, model, onnx_config, onnx_config\u001b[39m.\u001b[39;49mdefault_onnx_opset, onnx_path)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/onnx/convert.py:336\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(preprocessor, model, config, opset, output, tokenizer, device)\u001b[0m\n\u001b[1;32m    330\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    331\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported PyTorch version for this model. Minimum required is \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39mtorch_onnx_minimum_version\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m got: \u001b[39m\u001b[39m{\u001b[39;00mtorch_version\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    333\u001b[0m         )\n\u001b[1;32m    335\u001b[0m \u001b[39mif\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(model), PreTrainedModel):\n\u001b[0;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m export_pytorch(preprocessor, model, config, opset, output, tokenizer\u001b[39m=\u001b[39;49mtokenizer, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    337\u001b[0m \u001b[39melif\u001b[39;00m is_tf_available() \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(model), TFPreTrainedModel):\n\u001b[1;32m    338\u001b[0m     \u001b[39mreturn\u001b[39;00m export_tensorflow(preprocessor, model, config, opset, output, tokenizer\u001b[39m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/onnx/convert.py:191\u001b[0m, in \u001b[0;36mexport_pytorch\u001b[0;34m(preprocessor, model, config, opset, output, tokenizer, device)\u001b[0m\n\u001b[1;32m    189\u001b[0m                     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    190\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m             onnx_export(\n\u001b[1;32m    192\u001b[0m                 model,\n\u001b[1;32m    193\u001b[0m                 (model_inputs,),\n\u001b[1;32m    194\u001b[0m                 f\u001b[39m=\u001b[39;49moutput\u001b[39m.\u001b[39;49mas_posix(),\n\u001b[1;32m    195\u001b[0m                 input_names\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(config\u001b[39m.\u001b[39;49minputs\u001b[39m.\u001b[39;49mkeys()),\n\u001b[1;32m    196\u001b[0m                 output_names\u001b[39m=\u001b[39;49monnx_outputs,\n\u001b[1;32m    197\u001b[0m                 dynamic_axes\u001b[39m=\u001b[39;49m{name: axes \u001b[39mfor\u001b[39;49;00m name, axes \u001b[39min\u001b[39;49;00m chain(config\u001b[39m.\u001b[39;49minputs\u001b[39m.\u001b[39;49mitems(), config\u001b[39m.\u001b[39;49moutputs\u001b[39m.\u001b[39;49mitems())},\n\u001b[1;32m    198\u001b[0m                 do_constant_folding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    199\u001b[0m                 opset_version\u001b[39m=\u001b[39;49mopset,\n\u001b[1;32m    200\u001b[0m             )\n\u001b[1;32m    202\u001b[0m         config\u001b[39m.\u001b[39mrestore_ops()\n\u001b[1;32m    204\u001b[0m \u001b[39mreturn\u001b[39;00m matched_inputs, onnx_outputs\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/onnx/utils.py:504\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    188\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     export_modules_as_functions: Union[\u001b[39mbool\u001b[39m, Collection[Type[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule]]] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    205\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m     _export(\n\u001b[1;32m    505\u001b[0m         model,\n\u001b[1;32m    506\u001b[0m         args,\n\u001b[1;32m    507\u001b[0m         f,\n\u001b[1;32m    508\u001b[0m         export_params,\n\u001b[1;32m    509\u001b[0m         verbose,\n\u001b[1;32m    510\u001b[0m         training,\n\u001b[1;32m    511\u001b[0m         input_names,\n\u001b[1;32m    512\u001b[0m         output_names,\n\u001b[1;32m    513\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    514\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    515\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    516\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    517\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    518\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    519\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    520\u001b[0m     )\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/onnx/utils.py:1529\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1527\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1529\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1530\u001b[0m     model,\n\u001b[1;32m   1531\u001b[0m     args,\n\u001b[1;32m   1532\u001b[0m     verbose,\n\u001b[1;32m   1533\u001b[0m     input_names,\n\u001b[1;32m   1534\u001b[0m     output_names,\n\u001b[1;32m   1535\u001b[0m     operator_export_type,\n\u001b[1;32m   1536\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1537\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1538\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1539\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[1;32m   1542\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1544\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1545\u001b[0m )\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/onnx/utils.py:1111\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m   1110\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1111\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m   1112\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1114\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/onnx/utils.py:987\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    982\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m    983\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     )\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m    988\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    989\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/onnx/utils.py:891\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    889\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    890\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 891\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[1;32m    892\u001b[0m     model,\n\u001b[1;32m    893\u001b[0m     args,\n\u001b[1;32m    894\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    895\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    896\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    898\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    900\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/jit/_trace.py:1184\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1183\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1184\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1185\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/jit/_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 127\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[1;32m    128\u001b[0m     wrapper,\n\u001b[1;32m    129\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[1;32m    130\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[1;32m    131\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[1;32m    133\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/jit/_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    117\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 118\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    120\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:84\u001b[0m, in \u001b[0;36mBertAdapterModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, head, output_adapter_gating_scores, output_adapter_fusion_attentions, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m head \u001b[39mor\u001b[39;00m AdapterSetup\u001b[39m.\u001b[39mget_context_head_setup() \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_head:\n\u001b[0;32m---> 84\u001b[0m     head_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_head(\n\u001b[1;32m     85\u001b[0m         head_inputs,\n\u001b[1;32m     86\u001b[0m         head_name\u001b[39m=\u001b[39;49mhead,\n\u001b[1;32m     87\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     88\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m     89\u001b[0m         pooled_output\u001b[39m=\u001b[39;49mpooled_output,\n\u001b[1;32m     90\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m head_outputs\n\u001b[1;32m     93\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[39m# in case no head is used just return the output of the base model (including pooler output)\u001b[39;00m\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/heads/base.py:819\u001b[0m, in \u001b[0;36mModelWithFlexibleHeadsAdaptersMixin.forward_head\u001b[0;34m(self, all_outputs, head_name, cls_output, attention_mask, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     head_module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads[used_heads[\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 819\u001b[0m     return_output \u001b[39m=\u001b[39m head_module(all_outputs, cls_output, attention_mask, return_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    821\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(return_output, ModelOutput):\n\u001b[1;32m    822\u001b[0m     \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m ForwardContext\u001b[39m.\u001b[39mcontext_attributes:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/heads/base.py:263\u001b[0m, in \u001b[0;36mMultipleChoiceHead.forward\u001b[0;34m(self, outputs, cls_output, attention_mask, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m         cls_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m][:, \u001b[39m0\u001b[39m]\n\u001b[1;32m    262\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mforward(cls_output)\n\u001b[0;32m--> 263\u001b[0m logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_choices\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    264\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    265\u001b[0m labels \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 3]' is invalid for input of size 2"
     ]
    }
   ],
   "source": [
    "export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = onnx_inference(tokenizer, onnx_path, question, context, choices)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapterhub_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, May 24 2022, 21:28:31) \n[Clang 13.1.6 (clang-1316.0.21.2)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dff476688330005cfc33f1ee0f15c13ae533c265ccd041ab146cdb98ecc6219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
