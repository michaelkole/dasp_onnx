{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import QuestionAnsweringPipeline, AutoAdapterModel, AutoModelWithHeads, AutoTokenizer, AutoConfig\n",
    "from transformers.onnx import OnnxConfig, validate_model_outputs, export\n",
    "from transformers.models.bert import BertOnnxConfig\n",
    "\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from onnxruntime import InferenceSession\n",
    "import onnxruntime\n",
    "\n",
    "from onnx_opcounter import calculate_params\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_metric, load_dataset\n",
    "\n",
    "from typing import Mapping, OrderedDict\n",
    "from pathlib import Path\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working\n",
    "# cosmos_qa\n",
    "\n",
    "# semi working\n",
    "# multirc, quartz, race und quail  \n",
    "\n",
    "# repo error\n",
    "# narrativeqa\n",
    "\n",
    "# input diff\n",
    "# commonsense_qa und social_i_qa \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapters based on sequence options \n",
    "adapter_list = [\"cosmos_qa\", \"multi#rc\", \"quartz\", \"race\", \"quail\"]\n",
    "adapter = adapter_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4245.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "adapter_name = model.load_adapter(f\"AdapterHub/bert-base-uncased-pf-{adapter}\", source=\"hf\")\n",
    "\n",
    "model.active_adapters = adapter_name\n",
    "\n",
    "def mc_model_inference(question, context, choices):\n",
    "    outputs = []\n",
    "\n",
    "    raw_input = [[context, question + \" \" + choice] for choice in choices]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_idx = torch.argmax(outputs.logits)\n",
    "    return choices[answer_idx]\n",
    "\n",
    "question = \"What animal has the most hair?\"\n",
    "context = \"Fish are typically not hairy. Cats have 10g of hair. Tigers have 12g of hair. Horses have 100g of hair.\"\n",
    "\n",
    "choices = [\"Cat\", \"Horse\", \"Tiger\", \"Fish\"]\n",
    "answer = mc_model_inference(question, context, choices)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"narrativeqa\", \"commonsense_qa\", \"social_i_qa\"\n",
    "# adapter_name = model.load_adapter(f\"AdapterHub/narrativeqa\", source=\"hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3681.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignore\n"
     ]
    }
   ],
   "source": [
    "# commonsense_qa\n",
    "adapter = \"commonsense_qa\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "adapter_name = model.load_adapter(f\"AdapterHub/bert-base-uncased-pf-{adapter}\", source=\"hf\")\n",
    "model.active_adapters = adapter_name\n",
    "\n",
    "def mc_model_inference(question, question_concept, choices):\n",
    "    outputs = []\n",
    "\n",
    "    raw_input = [[question_concept, question + \" \" + choice] for choice in choices[\"text\"]]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_idx = torch.argmax(outputs.logits)\n",
    "    return choices[\"text\"][answer_idx]\n",
    "\n",
    "question = \"The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\"\n",
    "question_concept = \"punishing\"\n",
    "choices = { \"label\": [ \"A\", \"B\", \"C\", \"D\", \"E\" ], \"text\": [ \"ignore\", \"enforce\", \"authoritarian\", \"yell at\", \"avoid\" ] }\n",
    "\n",
    "# question = \"Google Maps and other highway and street GPS services have replaced what?\"\n",
    "# question_concept = \"highway\"\n",
    "# choices = { \"label\": [ \"A\", \"B\", \"C\", \"D\", \"E\" ], \"text\": [ \"united states\", \"mexico\", \"countryside\", \"atlas\", \"oceans\" ] }\n",
    "\n",
    "answer = mc_model_inference(question, question_concept, choices)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOnnxConfig(OnnxConfig):\n",
    "    # Inspired by BertONNXConfig, can be extended to support other QA tasks\n",
    "    @property\n",
    "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
    "        if self.task == \"multiple-choice\":\n",
    "            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n",
    "        elif self.task == \"mcq_commonsense_qa\":\n",
    "            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n",
    "        else:\n",
    "            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n",
    "\n",
    "        return OrderedDict(\n",
    "                [\n",
    "                    (\"input_ids\", dynamic_axis),\n",
    "                    (\"attention_mask\", dynamic_axis),\n",
    "                    (\"token_type_ids\", dynamic_axis), # Roberta doesn't use this\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 5]' is invalid for input of size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [105], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m onnx_config \u001b[39m=\u001b[39m BertOnnxConfig(config, task\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmultiple-choice\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m onnx_path \u001b[39m=\u001b[39m Path(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39monnx/\u001b[39m\u001b[39m{\u001b[39;00madapter\u001b[39m}\u001b[39;00m\u001b[39m/model.onnx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m export(tokenizer, model, onnx_config, onnx_config\u001b[39m.\u001b[39;49mdefault_onnx_opset, onnx_path)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/onnx/convert.py:336\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(preprocessor, model, config, opset, output, tokenizer, device)\u001b[0m\n\u001b[1;32m    330\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    331\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported PyTorch version for this model. Minimum required is \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39mtorch_onnx_minimum_version\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m got: \u001b[39m\u001b[39m{\u001b[39;00mtorch_version\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    333\u001b[0m         )\n\u001b[1;32m    335\u001b[0m \u001b[39mif\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(model), PreTrainedModel):\n\u001b[0;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m export_pytorch(preprocessor, model, config, opset, output, tokenizer\u001b[39m=\u001b[39;49mtokenizer, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    337\u001b[0m \u001b[39melif\u001b[39;00m is_tf_available() \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(model), TFPreTrainedModel):\n\u001b[1;32m    338\u001b[0m     \u001b[39mreturn\u001b[39;00m export_tensorflow(preprocessor, model, config, opset, output, tokenizer\u001b[39m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/onnx/convert.py:191\u001b[0m, in \u001b[0;36mexport_pytorch\u001b[0;34m(preprocessor, model, config, opset, output, tokenizer, device)\u001b[0m\n\u001b[1;32m    189\u001b[0m                     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    190\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m             onnx_export(\n\u001b[1;32m    192\u001b[0m                 model,\n\u001b[1;32m    193\u001b[0m                 (model_inputs,),\n\u001b[1;32m    194\u001b[0m                 f\u001b[39m=\u001b[39;49moutput\u001b[39m.\u001b[39;49mas_posix(),\n\u001b[1;32m    195\u001b[0m                 input_names\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(config\u001b[39m.\u001b[39;49minputs\u001b[39m.\u001b[39;49mkeys()),\n\u001b[1;32m    196\u001b[0m                 output_names\u001b[39m=\u001b[39;49monnx_outputs,\n\u001b[1;32m    197\u001b[0m                 dynamic_axes\u001b[39m=\u001b[39;49m{name: axes \u001b[39mfor\u001b[39;49;00m name, axes \u001b[39min\u001b[39;49;00m chain(config\u001b[39m.\u001b[39;49minputs\u001b[39m.\u001b[39;49mitems(), config\u001b[39m.\u001b[39;49moutputs\u001b[39m.\u001b[39;49mitems())},\n\u001b[1;32m    198\u001b[0m                 do_constant_folding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    199\u001b[0m                 opset_version\u001b[39m=\u001b[39;49mopset,\n\u001b[1;32m    200\u001b[0m             )\n\u001b[1;32m    202\u001b[0m         config\u001b[39m.\u001b[39mrestore_ops()\n\u001b[1;32m    204\u001b[0m \u001b[39mreturn\u001b[39;00m matched_inputs, onnx_outputs\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/onnx/utils.py:504\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    188\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     export_modules_as_functions: Union[\u001b[39mbool\u001b[39m, Collection[Type[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule]]] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    205\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m     _export(\n\u001b[1;32m    505\u001b[0m         model,\n\u001b[1;32m    506\u001b[0m         args,\n\u001b[1;32m    507\u001b[0m         f,\n\u001b[1;32m    508\u001b[0m         export_params,\n\u001b[1;32m    509\u001b[0m         verbose,\n\u001b[1;32m    510\u001b[0m         training,\n\u001b[1;32m    511\u001b[0m         input_names,\n\u001b[1;32m    512\u001b[0m         output_names,\n\u001b[1;32m    513\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    514\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    515\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    516\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    517\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    518\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    519\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    520\u001b[0m     )\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/onnx/utils.py:1529\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1527\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1529\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1530\u001b[0m     model,\n\u001b[1;32m   1531\u001b[0m     args,\n\u001b[1;32m   1532\u001b[0m     verbose,\n\u001b[1;32m   1533\u001b[0m     input_names,\n\u001b[1;32m   1534\u001b[0m     output_names,\n\u001b[1;32m   1535\u001b[0m     operator_export_type,\n\u001b[1;32m   1536\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1537\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1538\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1539\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[1;32m   1542\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1544\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1545\u001b[0m )\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/onnx/utils.py:1111\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m   1110\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1111\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m   1112\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1114\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/onnx/utils.py:987\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    982\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m    983\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     )\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m    988\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    989\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/onnx/utils.py:891\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    889\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    890\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 891\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[1;32m    892\u001b[0m     model,\n\u001b[1;32m    893\u001b[0m     args,\n\u001b[1;32m    894\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    895\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    896\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    898\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    900\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/jit/_trace.py:1184\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1183\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1184\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1185\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/jit/_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 127\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[1;32m    128\u001b[0m     wrapper,\n\u001b[1;32m    129\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[1;32m    130\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[1;32m    131\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[1;32m    133\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/jit/_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    117\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 118\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    120\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:84\u001b[0m, in \u001b[0;36mBertAdapterModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, head, output_adapter_gating_scores, output_adapter_fusion_attentions, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m head \u001b[39mor\u001b[39;00m AdapterSetup\u001b[39m.\u001b[39mget_context_head_setup() \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_head:\n\u001b[0;32m---> 84\u001b[0m     head_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_head(\n\u001b[1;32m     85\u001b[0m         head_inputs,\n\u001b[1;32m     86\u001b[0m         head_name\u001b[39m=\u001b[39;49mhead,\n\u001b[1;32m     87\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     88\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m     89\u001b[0m         pooled_output\u001b[39m=\u001b[39;49mpooled_output,\n\u001b[1;32m     90\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m head_outputs\n\u001b[1;32m     93\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[39m# in case no head is used just return the output of the base model (including pooler output)\u001b[39;00m\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/heads/base.py:819\u001b[0m, in \u001b[0;36mModelWithFlexibleHeadsAdaptersMixin.forward_head\u001b[0;34m(self, all_outputs, head_name, cls_output, attention_mask, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     head_module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads[used_heads[\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 819\u001b[0m     return_output \u001b[39m=\u001b[39m head_module(all_outputs, cls_output, attention_mask, return_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    821\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(return_output, ModelOutput):\n\u001b[1;32m    822\u001b[0m     \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m ForwardContext\u001b[39m.\u001b[39mcontext_attributes:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/heads/base.py:263\u001b[0m, in \u001b[0;36mMultipleChoiceHead.forward\u001b[0;34m(self, outputs, cls_output, attention_mask, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m         cls_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m][:, \u001b[39m0\u001b[39m]\n\u001b[1;32m    262\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mforward(cls_output)\n\u001b[0;32m--> 263\u001b[0m logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_choices\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    264\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    265\u001b[0m labels \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 5]' is invalid for input of size 8"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "onnx_config = BertOnnxConfig(config, task=\"multiple-choice\")\n",
    "\n",
    "onnx_path = Path(f\"onnx/{adapter}/model.onnx\")\n",
    "\n",
    "export(tokenizer, model, onnx_config, onnx_config.default_onnx_opset, onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horse\n"
     ]
    }
   ],
   "source": [
    "def onnx_inference(onnx_path, question, context, choices):\n",
    "    onnx_model = InferenceSession(\n",
    "        str(onnx_path), providers=[\"CPUExecutionProvider\"]\n",
    "    )\n",
    "\n",
    "    raw_input = [[context, question + \" \" + choice] for choice in choices]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"np\")\n",
    "\n",
    "    inputs['token_type_ids'] = np.expand_dims(inputs['token_type_ids'], axis=0)\n",
    "    inputs['input_ids'] =  np.expand_dims(inputs['input_ids'], axis=0)\n",
    "    inputs['attention_mask'] =  np.expand_dims(inputs['attention_mask'], axis=0)\n",
    "\n",
    "    outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\n",
    "\n",
    "    answer_idx = np.argmax(outputs[0])\n",
    "    return choices[answer_idx]\n",
    "    \n",
    "answer = onnx_inference(onnx_path, question, context, choices)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a7f5a2c07603db35bc4e52cfd5b475adbf202ae824ea4c5e531d495460257f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
