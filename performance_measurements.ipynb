{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import evaluate\n",
    "from transformers import AutoModelWithHeads, AutoTokenizer\n",
    "from transformers.models.bert import BertOnnxConfig\n",
    "from transformers.onnx import OnnxConfig, validate_model_outputs, export\n",
    "\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from onnxruntime import InferenceSession\n",
    "import onnxruntime\n",
    "\n",
    "\n",
    "import time\n",
    "from typing import Tuple, Union\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Mapping, OrderedDict\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_skills(skill_type, path=\"square_skills/impl_skills.csv\"):\n",
    "    all_skills = pd.read_csv(path)\n",
    "    skills = all_skills[all_skills[\"Type\"] == skill_type]\n",
    "    return skills"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Inference Time "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extractive qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = pd.read_csv(\"square_skills/impl_skills.csv\")\n",
    "skill = \"span-extraction\"\n",
    "skills = load_skills(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(\n",
    "            start_: np.ndarray,\n",
    "            end_: np.ndarray,\n",
    "            topk: int,\n",
    "            max_answer_len: int,\n",
    "            undesired_tokens_: np.ndarray,\n",
    "    ) -> Tuple:\n",
    "    \"\"\"\n",
    "    Take the output of any :obj:`ModelForQuestionAnswering` and\n",
    "        will generate probabilities for each span to be the\n",
    "        actual answer.\n",
    "    In addition, it filters out some unwanted/impossible cases\n",
    "    like answer len being greater than max_answer_len or\n",
    "    answer end position being before the starting position.\n",
    "    The method supports output the k-best answer through\n",
    "    the topk argument.\n",
    "    Args:\n",
    "        start_ (:obj:`np.ndarray`): Individual start\n",
    "            probabilities for each token.\n",
    "        end (:obj:`np.ndarray`): Individual end_ probabilities\n",
    "            for each token.\n",
    "        topk (:obj:`int`): Indicates how many possible answer\n",
    "            span(s) to extract from the model output.\n",
    "        max_answer_len (:obj:`int`): Maximum size of the answer\n",
    "            to extract from the model\"s output.\n",
    "        undesired_tokens_ (:obj:`np.ndarray`): Mask determining\n",
    "            tokens that can be part of the answer\n",
    "    \"\"\"\n",
    "    # Ensure we have batch axis\n",
    "    if start_.ndim == 1:\n",
    "        start_ = start_[None]\n",
    "\n",
    "    if end_.ndim == 1:\n",
    "        end_ = end_[None]\n",
    "\n",
    "    # Compute the score of each tuple(start_, end_) to be the real answer\n",
    "    outer = np.matmul(np.expand_dims(start_, -1), np.expand_dims(end_, 1))\n",
    "\n",
    "    # Remove candidate with end_ < start_ and end_ - start_ > max_answer_len\n",
    "    candidates = np.tril(np.triu(outer), max_answer_len - 1)\n",
    "\n",
    "    #  Inspired by Chen & al. (https://github.com/facebookresearch/DrQA)\n",
    "    scores_flat = candidates.flatten()\n",
    "    if topk == 1:\n",
    "        idx_sort = [np.argmax(scores_flat)]\n",
    "    elif len(scores_flat) < topk:\n",
    "        idx_sort = np.argsort(-scores_flat)\n",
    "    else:\n",
    "        idx = np.argpartition(-scores_flat, topk)[0:topk]\n",
    "        idx_sort = idx[np.argsort(-scores_flat[idx])]\n",
    "\n",
    "    starts_, ends_ = np.unravel_index(idx_sort, candidates.shape)[1:]\n",
    "    desired_spans = np.isin(starts_, undesired_tokens_.nonzero()) & np.isin(\n",
    "        ends_, undesired_tokens_.nonzero()\n",
    "    )\n",
    "    starts_ = starts_[desired_spans]\n",
    "    ends_ = ends_[desired_spans]\n",
    "    scores_ = candidates[0, starts_, ends_]\n",
    "\n",
    "    return starts_, ends_, scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_predict(\n",
    "            model, input, tokenizer, preprocessing_kwargs, model_kwargs, batch_size=1, disable_gpu=True, output_features=False\n",
    "    ) -> Union[dict, Tuple[dict, dict]]:\n",
    "        \"\"\"\n",
    "        Inference on the input.\n",
    "        Args:\n",
    "         request: the request with the input and optional kwargs\n",
    "         output_features: return the features of the input.\n",
    "            Necessary if, e.g., attention mask is needed for post-processing.\n",
    "        Returns:\n",
    "             The model outputs and optionally the input features\n",
    "        \"\"\"\n",
    "\n",
    "        all_predictions = []\n",
    "        preprocessing_kwargs[\"padding\"] = preprocessing_kwargs.get(\n",
    "            \"padding\", True\n",
    "        )\n",
    "        preprocessing_kwargs[\"truncation\"] = preprocessing_kwargs.get(\n",
    "            \"truncation\", True\n",
    "        )\n",
    "        model.to(\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available() and not disable_gpu\n",
    "            else \"cpu\"\n",
    "        )\n",
    "\n",
    "        features = tokenizer(\n",
    "            input, return_tensors=\"pt\", **preprocessing_kwargs\n",
    "        )\n",
    "\n",
    "        for start_idx in range(0, len(input), batch_size):\n",
    "            with torch.no_grad():\n",
    "                input_features = {\n",
    "                    k: features[k][start_idx: start_idx + batch_size]\n",
    "                    for k in features.keys()\n",
    "                }\n",
    "                predictions = model(**input_features, **model_kwargs)\n",
    "                all_predictions.append(predictions)\n",
    "\n",
    "        keys = all_predictions[0].keys()\n",
    "        final_prediction = {}\n",
    "        for key in keys:\n",
    "            # HuggingFace outputs for \"attentions\" and more is\n",
    "            # returned as tuple of tensors\n",
    "            # Tuple of tuples only exists for \"past_key_values\"\n",
    "            # which is only relevant for generation.\n",
    "            # Generation should NOT use this function\n",
    "            if isinstance(all_predictions[0][key], tuple):\n",
    "                tuple_of_lists = list(\n",
    "                    zip(\n",
    "                        *[\n",
    "                            [\n",
    "                                torch.stack(p).cpu()\n",
    "                                if isinstance(p, tuple)\n",
    "                                else p.cpu()\n",
    "                                for p in tpl[key]\n",
    "                            ]\n",
    "                            for tpl in all_predictions\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "                final_prediction[key] = tuple(torch.cat(l) for l in tuple_of_lists)\n",
    "            else:\n",
    "                final_prediction[key] = torch.cat(\n",
    "                    [p[key].cpu() for p in all_predictions]\n",
    "                )\n",
    "        if output_features:\n",
    "            return final_prediction, features\n",
    "\n",
    "        return final_prediction\n",
    "\n",
    "def base_qa(model, tokenizer, input, preprocessing_kwargs, task_kwargs, model_kwargs):\n",
    "    \"\"\"\n",
    "    Span-based question answering for a given question and context.\n",
    "    We expect the input to use the (question, context) format for the text pairs.\n",
    "    Args:\n",
    "        request: the prediction request\n",
    "    \"\"\"    \n",
    "    preprocessing_kwargs[\"truncation\"] = \"only_second\"\n",
    "    features = tokenizer(\n",
    "        input, return_tensors=\"pt\", **preprocessing_kwargs\n",
    "    )\n",
    "    predictions, features = base_predict(model, input, tokenizer, preprocessing_kwargs, model_kwargs, output_features=True)\n",
    "\n",
    "    task_outputs = {\n",
    "        \"answers\": [],\n",
    "        \"attributions\": [],\n",
    "        \"adversarial\": {\n",
    "            \"indices\": [],\n",
    "        },  # for hotflip, input_reduction and topk\n",
    "    }\n",
    "\n",
    "    for idx, (start, end, (_, context)) in enumerate(\n",
    "            zip(predictions[\"start_logits\"], predictions[\"end_logits\"], input)\n",
    "    ):\n",
    "        # Ensure padded tokens & question tokens cannot\n",
    "        # belong to the set of candidate answers.\n",
    "        question_tokens = np.abs(np.array([s != 1 for s in features.sequence_ids(idx)]) - 1)\n",
    "        # Unmask CLS token for \"no answer\"\n",
    "        question_tokens[0] = 1\n",
    "        undesired_tokens = question_tokens & features[\"attention_mask\"][idx].numpy()\n",
    "\n",
    "        # Generate mask\n",
    "        undesired_tokens_mask = undesired_tokens == 0.0\n",
    "\n",
    "        # Make sure non-context indexes in the tensor cannot\n",
    "        # contribute to the softmax\n",
    "        start = np.where(undesired_tokens_mask, -10000.0, start)\n",
    "        end = np.where(undesired_tokens_mask, -10000.0, end)\n",
    "\n",
    "        start = np.exp(start - np.log(np.sum(np.exp(start), axis=-1, keepdims=True)))\n",
    "        end = np.exp(end - np.log(np.sum(np.exp(end), axis=-1, keepdims=True)))\n",
    "\n",
    "        # Get score for \"no answer\" then mask for decoding step (CLS token\n",
    "        no_answer_score = (start[0] * end[0]).item()\n",
    "        start[0] = end[0] = 0.0\n",
    "\n",
    "        starts, ends, scores = decode(\n",
    "            start,\n",
    "            end,\n",
    "            task_kwargs.get(\"topk\", 1),\n",
    "            task_kwargs.get(\"max_answer_len\", 128),\n",
    "            undesired_tokens,\n",
    "        )\n",
    "\n",
    "        enc = features[idx]\n",
    "        original_ans_start = enc.token_to_word(starts[0])\n",
    "        original_ans_end = enc.token_to_word(ends[0])\n",
    "        answers = [\n",
    "            {\n",
    "                \"score\": score.item(),\n",
    "                \"start\": enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0],\n",
    "                \"end\": enc.word_to_chars(enc.token_to_word(e), sequence_index=1)[1],\n",
    "                \"answer\": context[\n",
    "                            enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0]: enc.word_to_chars(\n",
    "                                enc.token_to_word(e), sequence_index=1\n",
    "                            )[1]\n",
    "                            ],\n",
    "            }\n",
    "            for s, e, score in zip(starts, ends, scores)\n",
    "        ]\n",
    "        if task_kwargs.get(\"show_null_answers\", True):\n",
    "            answers.append({\"score\": no_answer_score, \"start\": 0, \"end\": 0, \"answer\": \"\"})\n",
    "        answers = sorted(answers, key=lambda x: x[\"score\"], reverse=True)[: task_kwargs.get(\"topk\", 1)]\n",
    "        task_outputs[\"answers\"].append(answers)\n",
    "\n",
    "    return predictions, task_outputs, original_ans_start, original_ans_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from SQuARE ONNX QA Pipeline (note: some features like explainability and attack mode have been removed)\n",
    "def question_answering(model_qa, tokenizer, input, preprocessing_kwargs, task_kwargs, model_kwargs):\n",
    "    \"\"\"\n",
    "    Span-based question answering for a given question and context.\n",
    "    We expect the input to use the (question, context) format for the text pairs.\n",
    "    Args:\n",
    "        request: the prediction request\n",
    "    \"\"\"    \n",
    "    preprocessing_kwargs[\"truncation\"] = \"only_second\"\n",
    "\n",
    "    features = tokenizer(\n",
    "        input, return_tensors=\"np\", **preprocessing_kwargs\n",
    "    )\n",
    "    onnx_inputs = {key: np.array(features[key], dtype=np.int64) for key in features}\n",
    "    \n",
    "    predictions_onnx = model_qa.run(input_feed=onnx_inputs, output_names=None)\n",
    "    predictions = {\n",
    "        \"start_logits\": predictions_onnx[0],\n",
    "        \"end_logits\": predictions_onnx[1]\n",
    "    }\n",
    "\n",
    "    task_outputs = {\n",
    "        \"answers\": [],\n",
    "        \"attributions\": [],\n",
    "        \"adversarial\": {\n",
    "            \"indices\": [],\n",
    "        },  # for hotflip, input_reduction and topk\n",
    "    }\n",
    "\n",
    "    for idx, (start, end, (_, context)) in enumerate(\n",
    "            zip(predictions[\"start_logits\"], predictions[\"end_logits\"], input)\n",
    "    ):\n",
    "        # Ensure padded tokens & question tokens cannot\n",
    "        # belong to the set of candidate answers.\n",
    "        question_tokens = np.abs(np.array([s != 1 for s in features.sequence_ids(idx)]) - 1)\n",
    "        # Unmask CLS token for \"no answer\"\n",
    "        question_tokens[0] = 1\n",
    "        undesired_tokens = question_tokens & features[\"attention_mask\"][idx]\n",
    "\n",
    "        # Generate mask\n",
    "        undesired_tokens_mask = undesired_tokens == 0.0\n",
    "\n",
    "        # Make sure non-context indexes in the tensor cannot\n",
    "        # contribute to the softmax\n",
    "        start = np.where(undesired_tokens_mask, -10000.0, start)\n",
    "        end = np.where(undesired_tokens_mask, -10000.0, end)\n",
    "\n",
    "        start = np.exp(start - np.log(np.sum(np.exp(start), axis=-1, keepdims=True)))\n",
    "        end = np.exp(end - np.log(np.sum(np.exp(end), axis=-1, keepdims=True)))\n",
    "\n",
    "        # Get score for \"no answer\" then mask for decoding step (CLS token\n",
    "        no_answer_score = (start[0] * end[0]).item()\n",
    "        start[0] = end[0] = 0.0\n",
    "\n",
    "        starts, ends, scores = decode(\n",
    "            start,\n",
    "            end,\n",
    "            task_kwargs.get(\"topk\", 1),\n",
    "            task_kwargs.get(\"max_answer_len\", 128),\n",
    "            undesired_tokens,\n",
    "        )\n",
    "\n",
    "        enc = features[idx]\n",
    "        original_ans_start = enc.token_to_word(starts[0])\n",
    "        original_ans_end = enc.token_to_word(ends[0])\n",
    "        answers = [\n",
    "            {\n",
    "                \"score\": score.item(),\n",
    "                \"start\": enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0],\n",
    "                \"end\": enc.word_to_chars(enc.token_to_word(e), sequence_index=1)[1],\n",
    "                \"answer\": context[\n",
    "                            enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0]: enc.word_to_chars(\n",
    "                                enc.token_to_word(e), sequence_index=1\n",
    "                            )[1]\n",
    "                            ],\n",
    "            }\n",
    "            for s, e, score in zip(starts, ends, scores)\n",
    "        ]\n",
    "        if task_kwargs.get(\"show_null_answers\", True):\n",
    "            answers.append({\"score\": no_answer_score, \"start\": 0, \"end\": 0, \"answer\": \"\"})\n",
    "        answers = sorted(answers, key=lambda x: x[\"score\"], reverse=True)[: task_kwargs.get(\"topk\", 1)]\n",
    "        task_outputs[\"answers\"].append(answers)\n",
    "\n",
    "    return predictions, task_outputs, original_ans_start, original_ans_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_kwargs = {\"padding\": True, \"truncation\": True}\n",
    "\n",
    "task_kwargs = {\"show_null_answers\": False, \"topk\": 1, \"max_answer_len\": 128}\n",
    "\n",
    "model_kwargs = {\"\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_onnx, model_onnx_quant, as_list=False):\n",
    "    local_onnx_model = onnxruntime.InferenceSession(model_onnx, providers=[\"CPUExecutionProvider\"])\n",
    "    local_onnx_model_quant = onnxruntime.InferenceSession(model_onnx_quant, providers=[\"CPUExecutionProvider\"])\n",
    "    \n",
    "    so = onnxruntime.SessionOptions()\n",
    "    so.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    \n",
    "    local_onnx_model_opt = onnxruntime.InferenceSession(model_onnx, so)\n",
    "    local_onnx_model_quant_opt = onnxruntime.InferenceSession(model_onnx_quant, so)\n",
    "    \n",
    "    if as_list:\n",
    "        return [local_onnx_model, local_onnx_model_opt, local_onnx_model_quant, local_onnx_model_quant_opt]\n",
    "    return local_onnx_model, local_onnx_model_opt, local_onnx_model_quant, local_onnx_model_quant_opt\n",
    "\n",
    "def repo_builder(reader, adapter):\n",
    "    repo_id = f\"UKP-SQuARE/{reader}-pf-{adapter}-onnx\"\n",
    "    filename_onnx = \"model.onnx\"\n",
    "    filename_onnx_quant = \"model_quant.onnx\"\n",
    "\n",
    "    model_onnx = hf_hub_download(repo_id=repo_id, filename=filename_onnx)\n",
    "    model_onnx_quant = hf_hub_download(repo_id=repo_id, filename=filename_onnx_quant)\n",
    "\n",
    "    return model_onnx, model_onnx_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_torch(model, inputs):\n",
    "#     with torch.no_grad():\n",
    "#         model(**inputs)\n",
    "\n",
    "# def run_onnx(qa_model, onnx_inputs):\n",
    "#     qa_model.run(output_names=[\"start_logits\", \"end_logits\"], input_feed=dict(onnx_inputs))   \n",
    "\n",
    "# def get_time_duration(func, model, inputs): \n",
    "#     st= time.time()\n",
    "#     func(model, inputs)\n",
    "#     et = time.time()\n",
    "#     return 1000 * (et - st)\n",
    "\n",
    "def save_df(df_new, path_to_logger_file = \"logger_all.csv\"):\n",
    "\n",
    "    if os.path.exists(path_to_logger_file):\n",
    "        df_fin = pd.concat([pd.read_csv(path_to_logger_file), df_new])\n",
    "        df_fin.to_csv(path_to_logger_file,index=False)\n",
    "    else: \n",
    "        df_new.to_csv(path_to_logger_file,index=False)\n",
    "\n",
    "# def measure_time(perf_type, tokenizer, question, context, model):\n",
    "#     if perf_type == \"base\":\n",
    "#         inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True)\n",
    "#         mode = run_torch\n",
    "#         # time_once = get_time_duration(run_torch, model, inputs)\n",
    "    \n",
    "#     elif perf_type == \"seq_length\":\n",
    "#         inputs = tokenizer(question, context, return_tensors=\"np\", truncation=True)\n",
    "#         inputs = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\n",
    "#         mode = run_onnx\n",
    "#         # time_once = get_time_duration(run_onnx, model, inputs) \n",
    "    \n",
    "#     time_once = get_time_duration(mode, model, inputs) \n",
    "\n",
    "#     return time_once\n",
    "\n",
    "# def performance_log(perf_type, name, model, tokenizer, data, data_intervall = 0): \n",
    "#     df = pd.DataFrame(columns=[\"model_name\", \"time once (ms)\", \"average_time 50 times (ms)\", \"seq_length\", \"context\", \"question\", \"data_id\"])\n",
    "    \n",
    "#     for i in range(0, len(data[\"context\"]), data_intervall):\n",
    "#         context = data[\"context\"][i]\n",
    "#         question = data[\"question\"][i]\n",
    "#         time_duration = measure_time(perf_type, tokenizer, question, context, model)\n",
    "        \n",
    "#         seq_length = len(context.split()) # TODO -> reduce stopwords? Real Tokenization?\n",
    "        \n",
    "#         df.loc[len(df)] = [name, time_duration, \"\", seq_length, context, question, data[\"id\"][i]]\n",
    "        \n",
    "#         print(\"Model: {}, Input Length {}: {:.3f} ms\".format(name, seq_length, time_duration))\n",
    "#     save_df(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = pd.read_csv(\"square_skills/impl_skills.csv\")\n",
    "skill = \"categorical\"\n",
    "skills = load_skills(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_base_inference(model, tokenizer, question, context):\n",
    "    \n",
    "    raw_input = [[context, question]]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    answer_idx = torch.argmax(outputs.logits)\n",
    "    \n",
    "    return bool(answer_idx)\n",
    "\n",
    "def categorical_onnx_inference(onnx_model, tokenizer, question, context):\n",
    "\n",
    "    inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors=\"np\")\n",
    "    inputs = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\n",
    "\n",
    "    outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\n",
    "\n",
    "    return bool(np.argmax(outputs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_duration(func, model, tokenizer, question, context): \n",
    "    st= time.time()\n",
    "    func(model, tokenizer, question, context)\n",
    "    et = time.time()\n",
    "    return 1000 * (et - st)\n",
    "\n",
    "def save_df(df_new, path_to_logger_file = \"logger_all.csv\"):\n",
    "    if os.path.exists(path_to_logger_file):\n",
    "        df_fin = pd.concat([pd.read_csv(path_to_logger_file), df_new])\n",
    "        df_fin.to_csv(path_to_logger_file,index=False)\n",
    "    else: \n",
    "        df_new.to_csv(path_to_logger_file,index=False)\n",
    "\n",
    "def performance_log(func, name, model, tokenizer, data, data_set_name, data_intervall = 0): \n",
    "    df = pd.DataFrame(columns=[\"model_name\", \"time once (ms)\", \"average_time 50 times (ms)\", \"seq_length\", \"context\", \"question\", \"data_id\", \"data_set_name\"])\n",
    "    \n",
    "    for i in range(0, len(data[\"passage\"]), data_intervall):\n",
    "        context = data[\"passage\"][i]\n",
    "        question = data[\"question\"][i]\n",
    "        time_duration = get_time_duration(func, model, tokenizer, question, context)\n",
    "        \n",
    "        seq_length = len(context.split()) # TODO -> reduce stopwords\n",
    "        \n",
    "        df.loc[len(df)] = [name, time_duration, \"\", seq_length, context, question, i, data_set_name]\n",
    "        \n",
    "        print(\"Model: {}, Input Length {}: {:.3f} ms\".format(name, seq_length, time_duration))\n",
    "    save_df(df, path_to_logger_file=\"inference_time.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 250\n",
    "data_set_name = \"boolq\"\n",
    "data = load_dataset(data_set_name, split=f\"validation[:{runs}]\")\n",
    "\n",
    "for i in range(5):\n",
    "    for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "        print(\"Loading: {} {}\".format(reader, adapter))\n",
    "        \n",
    "        #load base model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(reader)\n",
    "        default_model = AutoModelWithHeads.from_pretrained(reader)\n",
    "        adapter_name = default_model.load_adapter(f\"AdapterHub/{reader}-pf-{adapter}\", source=\"hf\")\n",
    "        default_model.active_adapters = adapter_name\n",
    "\n",
    "        performance_log(categorical_base_inference, \"Base\", default_model, tokenizer, data, data_set_name, 1) \n",
    "\n",
    "        #load quant model\n",
    "        quantized_base_model = torch.quantization.quantize_dynamic(default_model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "        performance_log(categorical_base_inference, \"Base Quantized\", quantized_base_model, tokenizer, data, data_set_name, 1) \n",
    "        \n",
    "        #load onnx models\n",
    "        model_onnx, model_onnx_quant = repo_builder(reader, adapter)\n",
    "        onnx_models_list = load_model(model_onnx, model_onnx_quant, as_list=True)\n",
    "        onnx_models_name_helper_list = [\"ONNX\", \"ONNX-OPT\", \"ONNX Quantized\", \"ONNX-OPT Quantized\"]\n",
    "\n",
    "        # eval onnx models\n",
    "        for onnx_model, onnx_model_name in zip(onnx_models_list, onnx_models_name_helper_list):\n",
    "            performance_log(categorical_onnx_inference, onnx_model_name, onnx_model, tokenizer, data, data_set_name, 1) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = pd.read_csv(\"square_skills/impl_skills.csv\")\n",
    "skill = \"multiple-choice\"\n",
    "skills = load_skills(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_model_inference(model, tokenizer, question, context, choices):\n",
    "    outputs = []\n",
    "    raw_input = [[context, question + \" \" + choice] for choice in choices]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_idx = torch.argmax(outputs.logits)\n",
    "\n",
    "    return choices[answer_idx]\n",
    "\n",
    "def mc_onnx_inference(onnx_model, tokenizer, question, context, choices):\n",
    "\n",
    "    raw_input = [[context, question + \" \" + choice] for choice in choices]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"np\")\n",
    "\n",
    "    inputs['input_ids'] =  np.expand_dims(inputs['input_ids'], axis=0)\n",
    "    inputs['attention_mask'] =  np.expand_dims(inputs['attention_mask'], axis=0)\n",
    "\n",
    "    if \"token_type_ids\" in inputs: #roberta does not use this\n",
    "        inputs['token_type_ids'] = np.expand_dims(inputs['token_type_ids'], axis=0)\n",
    "\n",
    "    outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\n",
    "    answer_idx = np.argmax(outputs[0])\n",
    "    return choices[answer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_duration(func, model, tokenizer, question, context, choices): \n",
    "    st= time.time()\n",
    "    func(model, tokenizer, question, context, choices)\n",
    "    et = time.time()\n",
    "    return 1000 * (et - st)\n",
    "\n",
    "def save_df(df_new, path_to_logger_file = \"logger_all.csv\"):\n",
    "    if os.path.exists(path_to_logger_file):\n",
    "        df_fin = pd.concat([pd.read_csv(path_to_logger_file), df_new])\n",
    "        df_fin.to_csv(path_to_logger_file,index=False)\n",
    "    else: \n",
    "        df_new.to_csv(path_to_logger_file,index=False)\n",
    "\n",
    "def performance_log(func, name, model, tokenizer, preped_data_set, data_set_name, data_intervall=1, run_amount=10): \n",
    "\n",
    "    for i in range(run_amount):\n",
    "        df = pd.DataFrame(columns=[\"model_name\", \"time once (ms)\", \"average_time 50 times (ms)\", \"seq_length\", \"context\", \"question\", \"choices\", \"data_id\", \"data_set_name\"])\n",
    "        \n",
    "        for i in range(0, len(preped_data_set), data_intervall):\n",
    "            question, context, choices = preped_data_set[i][0], preped_data_set[i][1], preped_data_set[i][2]\n",
    "            time_duration = get_time_duration(func, model, tokenizer, question, context, choices)\n",
    "            \n",
    "            seq_length = len(context.split()) # TODO -> reduce stopwords\n",
    "            \n",
    "            df.loc[len(df)] = [name, time_duration, \"\", seq_length, context, question, choices, i, data_set_name]\n",
    "            \n",
    "            print(\"Model: {}, Input Length {}: {:.3f} ms\".format(name, seq_length, time_duration))\n",
    "        save_df(df, path_to_logger_file=\"inference_time_mcq_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: bert-base-uncased cosmos_qa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cosmos_qa (/Users/michaelhermann/.cache/huggingface/datasets/cosmos_qa/default/0.1.0/3e18538cbfdb2c04189b16642715f0f6da3e97ed5df0aadcec3641245b2cf157)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: cosmos_qa\n",
      "Preped data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3577.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Base, Input Length 92: 364.141 ms\n",
      "Model: Base, Input Length 67: 240.724 ms\n",
      "Model: Base, Input Length 117: 454.405 ms\n",
      "Model: Base, Input Length 58: 268.950 ms\n",
      "Model: Base, Input Length 62: 261.901 ms\n",
      "Model: Base, Input Length 92: 326.817 ms\n",
      "Model: Base, Input Length 112: 340.621 ms\n",
      "Model: Base, Input Length 69: 195.496 ms\n",
      "Model: Base, Input Length 107: 335.328 ms\n",
      "Model: Base, Input Length 44: 186.007 ms\n",
      "Model: Base, Input Length 92: 307.447 ms\n",
      "Model: Base, Input Length 67: 189.204 ms\n",
      "Model: Base, Input Length 117: 380.620 ms\n",
      "Model: Base, Input Length 58: 257.042 ms\n",
      "Model: Base, Input Length 62: 212.051 ms\n",
      "Model: Base, Input Length 92: 270.642 ms\n",
      "Model: Base, Input Length 112: 323.247 ms\n",
      "Model: Base, Input Length 69: 195.751 ms\n",
      "Model: Base, Input Length 107: 407.122 ms\n",
      "Model: Base, Input Length 44: 217.303 ms\n",
      "Model: Base, Input Length 92: 334.316 ms\n",
      "Model: Base, Input Length 67: 222.130 ms\n",
      "Model: Base, Input Length 117: 397.649 ms\n",
      "Model: Base, Input Length 58: 224.278 ms\n",
      "Model: Base, Input Length 62: 225.970 ms\n",
      "Model: Base, Input Length 92: 334.356 ms\n",
      "Model: Base, Input Length 112: 475.465 ms\n",
      "Model: Base, Input Length 69: 243.985 ms\n",
      "Model: Base, Input Length 107: 448.709 ms\n",
      "Model: Base, Input Length 44: 272.532 ms\n",
      "Model: Base, Input Length 92: 338.996 ms\n",
      "Model: Base, Input Length 67: 251.869 ms\n",
      "Model: Base, Input Length 117: 417.311 ms\n",
      "Model: Base, Input Length 58: 236.954 ms\n",
      "Model: Base, Input Length 62: 250.935 ms\n",
      "Model: Base, Input Length 92: 375.694 ms\n",
      "Model: Base, Input Length 112: 540.757 ms\n",
      "Model: Base, Input Length 69: 409.428 ms\n",
      "Model: Base, Input Length 107: 612.325 ms\n",
      "Model: Base, Input Length 44: 239.650 ms\n",
      "Model: Base, Input Length 92: 366.500 ms\n",
      "Model: Base, Input Length 67: 266.516 ms\n",
      "Model: Base, Input Length 117: 471.047 ms\n",
      "Model: Base, Input Length 58: 303.550 ms\n",
      "Model: Base, Input Length 62: 285.592 ms\n",
      "Model: Base, Input Length 92: 340.205 ms\n",
      "Model: Base, Input Length 112: 376.842 ms\n",
      "Model: Base, Input Length 69: 249.121 ms\n",
      "Model: Base, Input Length 107: 414.912 ms\n",
      "Model: Base, Input Length 44: 202.126 ms\n",
      "Model: Base Quantized, Input Length 92: 304.446 ms\n",
      "Model: Base Quantized, Input Length 67: 211.321 ms\n",
      "Model: Base Quantized, Input Length 117: 429.886 ms\n",
      "Model: Base Quantized, Input Length 58: 277.185 ms\n",
      "Model: Base Quantized, Input Length 62: 277.438 ms\n",
      "Model: Base Quantized, Input Length 92: 286.454 ms\n",
      "Model: Base Quantized, Input Length 112: 345.600 ms\n",
      "Model: Base Quantized, Input Length 69: 223.840 ms\n",
      "Model: Base Quantized, Input Length 107: 356.799 ms\n",
      "Model: Base Quantized, Input Length 44: 229.888 ms\n",
      "Model: Base Quantized, Input Length 92: 307.350 ms\n",
      "Model: Base Quantized, Input Length 67: 253.687 ms\n",
      "Model: Base Quantized, Input Length 117: 366.512 ms\n",
      "Model: Base Quantized, Input Length 58: 223.167 ms\n",
      "Model: Base Quantized, Input Length 62: 219.351 ms\n",
      "Model: Base Quantized, Input Length 92: 293.689 ms\n",
      "Model: Base Quantized, Input Length 112: 347.130 ms\n",
      "Model: Base Quantized, Input Length 69: 219.944 ms\n",
      "Model: Base Quantized, Input Length 107: 374.956 ms\n",
      "Model: Base Quantized, Input Length 44: 217.993 ms\n",
      "Model: Base Quantized, Input Length 92: 296.677 ms\n",
      "Model: Base Quantized, Input Length 67: 262.514 ms\n",
      "Model: Base Quantized, Input Length 117: 408.531 ms\n",
      "Model: Base Quantized, Input Length 58: 284.501 ms\n",
      "Model: Base Quantized, Input Length 62: 317.265 ms\n",
      "Model: Base Quantized, Input Length 92: 364.091 ms\n",
      "Model: Base Quantized, Input Length 112: 437.941 ms\n",
      "Model: Base Quantized, Input Length 69: 230.285 ms\n",
      "Model: Base Quantized, Input Length 107: 397.940 ms\n",
      "Model: Base Quantized, Input Length 44: 213.290 ms\n",
      "Model: Base Quantized, Input Length 92: 389.603 ms\n",
      "Model: Base Quantized, Input Length 67: 313.571 ms\n",
      "Model: Base Quantized, Input Length 117: 568.156 ms\n",
      "Model: Base Quantized, Input Length 58: 294.829 ms\n",
      "Model: Base Quantized, Input Length 62: 283.710 ms\n",
      "Model: Base Quantized, Input Length 92: 391.744 ms\n",
      "Model: Base Quantized, Input Length 112: 418.331 ms\n",
      "Model: Base Quantized, Input Length 69: 274.076 ms\n",
      "Model: Base Quantized, Input Length 107: 518.619 ms\n",
      "Model: Base Quantized, Input Length 44: 319.851 ms\n",
      "Model: Base Quantized, Input Length 92: 418.043 ms\n",
      "Model: Base Quantized, Input Length 67: 221.303 ms\n",
      "Model: Base Quantized, Input Length 117: 403.958 ms\n",
      "Model: Base Quantized, Input Length 58: 345.309 ms\n",
      "Model: Base Quantized, Input Length 62: 285.595 ms\n",
      "Model: Base Quantized, Input Length 92: 349.241 ms\n",
      "Model: Base Quantized, Input Length 112: 496.520 ms\n",
      "Model: Base Quantized, Input Length 69: 325.272 ms\n",
      "Model: Base Quantized, Input Length 107: 522.399 ms\n",
      "Model: Base Quantized, Input Length 44: 306.189 ms\n",
      "Model: ONNX, Input Length 92: 207.608 ms\n",
      "Model: ONNX, Input Length 67: 176.738 ms\n",
      "Model: ONNX, Input Length 117: 272.107 ms\n",
      "Model: ONNX, Input Length 58: 161.407 ms\n",
      "Model: ONNX, Input Length 62: 170.263 ms\n",
      "Model: ONNX, Input Length 92: 206.090 ms\n",
      "Model: ONNX, Input Length 112: 252.026 ms\n",
      "Model: ONNX, Input Length 69: 152.692 ms\n",
      "Model: ONNX, Input Length 107: 264.422 ms\n",
      "Model: ONNX, Input Length 44: 152.032 ms\n",
      "Model: ONNX, Input Length 92: 235.769 ms\n",
      "Model: ONNX, Input Length 67: 147.820 ms\n",
      "Model: ONNX, Input Length 117: 328.971 ms\n",
      "Model: ONNX, Input Length 58: 163.514 ms\n",
      "Model: ONNX, Input Length 62: 163.854 ms\n",
      "Model: ONNX, Input Length 92: 205.883 ms\n",
      "Model: ONNX, Input Length 112: 254.637 ms\n",
      "Model: ONNX, Input Length 69: 154.712 ms\n",
      "Model: ONNX, Input Length 107: 292.744 ms\n",
      "Model: ONNX, Input Length 44: 150.456 ms\n",
      "Model: ONNX, Input Length 92: 262.043 ms\n",
      "Model: ONNX, Input Length 67: 187.508 ms\n",
      "Model: ONNX, Input Length 117: 323.966 ms\n",
      "Model: ONNX, Input Length 58: 170.070 ms\n",
      "Model: ONNX, Input Length 62: 160.587 ms\n",
      "Model: ONNX, Input Length 92: 210.814 ms\n",
      "Model: ONNX, Input Length 112: 257.007 ms\n",
      "Model: ONNX, Input Length 69: 159.912 ms\n",
      "Model: ONNX, Input Length 107: 270.813 ms\n",
      "Model: ONNX, Input Length 44: 150.588 ms\n",
      "Model: ONNX, Input Length 92: 211.964 ms\n",
      "Model: ONNX, Input Length 67: 149.595 ms\n",
      "Model: ONNX, Input Length 117: 367.397 ms\n",
      "Model: ONNX, Input Length 58: 181.993 ms\n",
      "Model: ONNX, Input Length 62: 181.408 ms\n",
      "Model: ONNX, Input Length 92: 235.194 ms\n",
      "Model: ONNX, Input Length 112: 263.892 ms\n",
      "Model: ONNX, Input Length 69: 160.570 ms\n",
      "Model: ONNX, Input Length 107: 315.189 ms\n",
      "Model: ONNX, Input Length 44: 198.093 ms\n",
      "Model: ONNX, Input Length 92: 272.765 ms\n",
      "Model: ONNX, Input Length 67: 189.090 ms\n",
      "Model: ONNX, Input Length 117: 345.298 ms\n",
      "Model: ONNX, Input Length 58: 225.006 ms\n",
      "Model: ONNX, Input Length 62: 231.880 ms\n",
      "Model: ONNX, Input Length 92: 315.010 ms\n",
      "Model: ONNX, Input Length 112: 313.677 ms\n",
      "Model: ONNX, Input Length 69: 238.568 ms\n",
      "Model: ONNX, Input Length 107: 371.671 ms\n",
      "Model: ONNX, Input Length 44: 192.164 ms\n",
      "Model: ONNX-OPT, Input Length 92: 269.275 ms\n",
      "Model: ONNX-OPT, Input Length 67: 229.399 ms\n",
      "Model: ONNX-OPT, Input Length 117: 385.034 ms\n",
      "Model: ONNX-OPT, Input Length 58: 193.419 ms\n",
      "Model: ONNX-OPT, Input Length 62: 197.083 ms\n",
      "Model: ONNX-OPT, Input Length 92: 250.120 ms\n",
      "Model: ONNX-OPT, Input Length 112: 308.908 ms\n",
      "Model: ONNX-OPT, Input Length 69: 199.111 ms\n",
      "Model: ONNX-OPT, Input Length 107: 328.649 ms\n",
      "Model: ONNX-OPT, Input Length 44: 201.111 ms\n",
      "Model: ONNX-OPT, Input Length 92: 316.775 ms\n",
      "Model: ONNX-OPT, Input Length 67: 188.904 ms\n",
      "Model: ONNX-OPT, Input Length 117: 325.466 ms\n",
      "Model: ONNX-OPT, Input Length 58: 207.617 ms\n",
      "Model: ONNX-OPT, Input Length 62: 191.500 ms\n",
      "Model: ONNX-OPT, Input Length 92: 266.457 ms\n",
      "Model: ONNX-OPT, Input Length 112: 290.986 ms\n",
      "Model: ONNX-OPT, Input Length 69: 183.374 ms\n",
      "Model: ONNX-OPT, Input Length 107: 299.928 ms\n",
      "Model: ONNX-OPT, Input Length 44: 168.964 ms\n",
      "Model: ONNX-OPT, Input Length 92: 234.311 ms\n",
      "Model: ONNX-OPT, Input Length 67: 158.892 ms\n",
      "Model: ONNX-OPT, Input Length 117: 313.954 ms\n",
      "Model: ONNX-OPT, Input Length 58: 236.948 ms\n",
      "Model: ONNX-OPT, Input Length 62: 193.711 ms\n",
      "Model: ONNX-OPT, Input Length 92: 234.595 ms\n",
      "Model: ONNX-OPT, Input Length 112: 286.149 ms\n",
      "Model: ONNX-OPT, Input Length 69: 168.749 ms\n",
      "Model: ONNX-OPT, Input Length 107: 284.691 ms\n",
      "Model: ONNX-OPT, Input Length 44: 170.769 ms\n",
      "Model: ONNX-OPT, Input Length 92: 235.234 ms\n",
      "Model: ONNX-OPT, Input Length 67: 160.723 ms\n",
      "Model: ONNX-OPT, Input Length 117: 305.426 ms\n",
      "Model: ONNX-OPT, Input Length 58: 189.624 ms\n",
      "Model: ONNX-OPT, Input Length 62: 235.954 ms\n",
      "Model: ONNX-OPT, Input Length 92: 244.142 ms\n",
      "Model: ONNX-OPT, Input Length 112: 278.487 ms\n",
      "Model: ONNX-OPT, Input Length 69: 174.374 ms\n",
      "Model: ONNX-OPT, Input Length 107: 292.829 ms\n",
      "Model: ONNX-OPT, Input Length 44: 178.457 ms\n",
      "Model: ONNX-OPT, Input Length 92: 245.716 ms\n",
      "Model: ONNX-OPT, Input Length 67: 186.084 ms\n",
      "Model: ONNX-OPT, Input Length 117: 409.440 ms\n",
      "Model: ONNX-OPT, Input Length 58: 197.667 ms\n",
      "Model: ONNX-OPT, Input Length 62: 220.384 ms\n",
      "Model: ONNX-OPT, Input Length 92: 272.673 ms\n",
      "Model: ONNX-OPT, Input Length 112: 323.241 ms\n",
      "Model: ONNX-OPT, Input Length 69: 163.790 ms\n",
      "Model: ONNX-OPT, Input Length 107: 298.968 ms\n",
      "Model: ONNX-OPT, Input Length 44: 157.547 ms\n",
      "Model: ONNX Quantized, Input Length 92: 215.991 ms\n",
      "Model: ONNX Quantized, Input Length 67: 150.613 ms\n",
      "Model: ONNX Quantized, Input Length 117: 253.543 ms\n",
      "Model: ONNX Quantized, Input Length 58: 140.324 ms\n",
      "Model: ONNX Quantized, Input Length 62: 137.305 ms\n",
      "Model: ONNX Quantized, Input Length 92: 194.598 ms\n",
      "Model: ONNX Quantized, Input Length 112: 253.409 ms\n",
      "Model: ONNX Quantized, Input Length 69: 129.965 ms\n",
      "Model: ONNX Quantized, Input Length 107: 217.004 ms\n",
      "Model: ONNX Quantized, Input Length 44: 149.339 ms\n",
      "Model: ONNX Quantized, Input Length 92: 217.731 ms\n",
      "Model: ONNX Quantized, Input Length 67: 160.277 ms\n",
      "Model: ONNX Quantized, Input Length 117: 313.538 ms\n",
      "Model: ONNX Quantized, Input Length 58: 165.979 ms\n",
      "Model: ONNX Quantized, Input Length 62: 173.855 ms\n",
      "Model: ONNX Quantized, Input Length 92: 221.019 ms\n",
      "Model: ONNX Quantized, Input Length 112: 253.047 ms\n",
      "Model: ONNX Quantized, Input Length 69: 136.074 ms\n",
      "Model: ONNX Quantized, Input Length 107: 233.710 ms\n",
      "Model: ONNX Quantized, Input Length 44: 116.682 ms\n",
      "Model: ONNX Quantized, Input Length 92: 166.557 ms\n",
      "Model: ONNX Quantized, Input Length 67: 136.425 ms\n",
      "Model: ONNX Quantized, Input Length 117: 254.083 ms\n",
      "Model: ONNX Quantized, Input Length 58: 159.102 ms\n",
      "Model: ONNX Quantized, Input Length 62: 128.667 ms\n",
      "Model: ONNX Quantized, Input Length 92: 156.761 ms\n",
      "Model: ONNX Quantized, Input Length 112: 198.815 ms\n",
      "Model: ONNX Quantized, Input Length 69: 121.185 ms\n",
      "Model: ONNX Quantized, Input Length 107: 243.188 ms\n",
      "Model: ONNX Quantized, Input Length 44: 135.012 ms\n",
      "Model: ONNX Quantized, Input Length 92: 170.298 ms\n",
      "Model: ONNX Quantized, Input Length 67: 111.552 ms\n",
      "Model: ONNX Quantized, Input Length 117: 268.980 ms\n",
      "Model: ONNX Quantized, Input Length 58: 143.389 ms\n",
      "Model: ONNX Quantized, Input Length 62: 148.806 ms\n",
      "Model: ONNX Quantized, Input Length 92: 217.436 ms\n",
      "Model: ONNX Quantized, Input Length 112: 252.580 ms\n",
      "Model: ONNX Quantized, Input Length 69: 142.894 ms\n",
      "Model: ONNX Quantized, Input Length 107: 242.073 ms\n",
      "Model: ONNX Quantized, Input Length 44: 150.386 ms\n",
      "Model: ONNX Quantized, Input Length 92: 208.216 ms\n",
      "Model: ONNX Quantized, Input Length 67: 148.615 ms\n",
      "Model: ONNX Quantized, Input Length 117: 262.523 ms\n",
      "Model: ONNX Quantized, Input Length 58: 137.526 ms\n",
      "Model: ONNX Quantized, Input Length 62: 130.361 ms\n",
      "Model: ONNX Quantized, Input Length 92: 190.848 ms\n",
      "Model: ONNX Quantized, Input Length 112: 257.761 ms\n",
      "Model: ONNX Quantized, Input Length 69: 148.806 ms\n",
      "Model: ONNX Quantized, Input Length 107: 252.890 ms\n",
      "Model: ONNX Quantized, Input Length 44: 127.433 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 207.952 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 67: 135.534 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 117: 288.676 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 58: 158.568 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 62: 155.082 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 208.676 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 112: 260.694 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 69: 172.746 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 107: 266.272 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 44: 150.676 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 206.692 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 67: 146.092 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 117: 302.186 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 58: 162.404 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 62: 159.486 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 207.779 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 112: 255.270 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 69: 147.645 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 107: 256.519 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 44: 174.311 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 229.056 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 67: 145.326 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 117: 232.479 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 58: 142.951 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 62: 129.731 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 173.554 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 112: 216.056 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 69: 125.623 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 107: 225.976 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 44: 135.326 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 167.793 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 67: 118.628 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 117: 228.889 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 58: 132.576 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 62: 132.723 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 190.753 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 112: 264.397 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 69: 154.926 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 107: 257.438 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 44: 136.194 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 177.756 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 67: 122.254 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 117: 230.634 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 58: 119.091 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 62: 205.848 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 226.384 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 112: 213.002 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 69: 150.567 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 107: 250.214 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 44: 124.390 ms\n",
      "Loading: roberta-base cosmos_qa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cosmos_qa (/Users/michaelhermann/.cache/huggingface/datasets/cosmos_qa/default/0.1.0/3e18538cbfdb2c04189b16642715f0f6da3e97ed5df0aadcec3641245b2cf157)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: cosmos_qa\n",
      "Preped data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 2406.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Base, Input Length 92: 259.745 ms\n",
      "Model: Base, Input Length 67: 174.830 ms\n",
      "Model: Base, Input Length 117: 311.686 ms\n",
      "Model: Base, Input Length 58: 196.325 ms\n",
      "Model: Base, Input Length 62: 201.475 ms\n",
      "Model: Base, Input Length 92: 269.657 ms\n",
      "Model: Base, Input Length 112: 295.798 ms\n",
      "Model: Base, Input Length 69: 200.789 ms\n",
      "Model: Base, Input Length 107: 349.566 ms\n",
      "Model: Base, Input Length 44: 187.335 ms\n",
      "Model: Base, Input Length 92: 281.679 ms\n",
      "Model: Base, Input Length 67: 193.471 ms\n",
      "Model: Base, Input Length 117: 317.115 ms\n",
      "Model: Base, Input Length 58: 167.446 ms\n",
      "Model: Base, Input Length 62: 235.951 ms\n",
      "Model: Base, Input Length 92: 275.280 ms\n",
      "Model: Base, Input Length 112: 298.218 ms\n",
      "Model: Base, Input Length 69: 189.559 ms\n",
      "Model: Base, Input Length 107: 353.011 ms\n",
      "Model: Base, Input Length 44: 184.889 ms\n",
      "Model: Base, Input Length 92: 303.838 ms\n",
      "Model: Base, Input Length 67: 201.477 ms\n",
      "Model: Base, Input Length 117: 335.673 ms\n",
      "Model: Base, Input Length 58: 186.741 ms\n",
      "Model: Base, Input Length 62: 197.916 ms\n",
      "Model: Base, Input Length 92: 241.759 ms\n",
      "Model: Base, Input Length 112: 356.375 ms\n",
      "Model: Base, Input Length 69: 220.709 ms\n",
      "Model: Base, Input Length 107: 354.216 ms\n",
      "Model: Base, Input Length 44: 208.103 ms\n",
      "Model: Base, Input Length 92: 289.541 ms\n",
      "Model: Base, Input Length 67: 225.443 ms\n",
      "Model: Base, Input Length 117: 414.248 ms\n",
      "Model: Base, Input Length 58: 210.400 ms\n",
      "Model: Base, Input Length 62: 217.701 ms\n",
      "Model: Base, Input Length 92: 269.921 ms\n",
      "Model: Base, Input Length 112: 351.635 ms\n",
      "Model: Base, Input Length 69: 223.412 ms\n",
      "Model: Base, Input Length 107: 357.568 ms\n",
      "Model: Base, Input Length 44: 201.702 ms\n",
      "Model: Base, Input Length 92: 311.371 ms\n",
      "Model: Base, Input Length 67: 251.269 ms\n",
      "Model: Base, Input Length 117: 373.576 ms\n",
      "Model: Base, Input Length 58: 208.183 ms\n",
      "Model: Base, Input Length 62: 209.542 ms\n",
      "Model: Base, Input Length 92: 251.082 ms\n",
      "Model: Base, Input Length 112: 317.974 ms\n",
      "Model: Base, Input Length 69: 200.275 ms\n",
      "Model: Base, Input Length 107: 312.668 ms\n",
      "Model: Base, Input Length 44: 206.832 ms\n",
      "Model: Base Quantized, Input Length 92: 300.952 ms\n",
      "Model: Base Quantized, Input Length 67: 226.195 ms\n",
      "Model: Base Quantized, Input Length 117: 360.140 ms\n",
      "Model: Base Quantized, Input Length 58: 202.800 ms\n",
      "Model: Base Quantized, Input Length 62: 203.076 ms\n",
      "Model: Base Quantized, Input Length 92: 256.740 ms\n",
      "Model: Base Quantized, Input Length 112: 325.026 ms\n",
      "Model: Base Quantized, Input Length 69: 230.184 ms\n",
      "Model: Base Quantized, Input Length 107: 343.450 ms\n",
      "Model: Base Quantized, Input Length 44: 189.426 ms\n",
      "Model: Base Quantized, Input Length 92: 259.585 ms\n",
      "Model: Base Quantized, Input Length 67: 185.482 ms\n",
      "Model: Base Quantized, Input Length 117: 362.511 ms\n",
      "Model: Base Quantized, Input Length 58: 209.498 ms\n",
      "Model: Base Quantized, Input Length 62: 245.999 ms\n",
      "Model: Base Quantized, Input Length 92: 275.139 ms\n",
      "Model: Base Quantized, Input Length 112: 313.639 ms\n",
      "Model: Base Quantized, Input Length 69: 211.311 ms\n",
      "Model: Base Quantized, Input Length 107: 311.598 ms\n",
      "Model: Base Quantized, Input Length 44: 182.641 ms\n",
      "Model: Base Quantized, Input Length 92: 339.834 ms\n",
      "Model: Base Quantized, Input Length 67: 177.399 ms\n",
      "Model: Base Quantized, Input Length 117: 347.923 ms\n",
      "Model: Base Quantized, Input Length 58: 196.258 ms\n",
      "Model: Base Quantized, Input Length 62: 199.009 ms\n",
      "Model: Base Quantized, Input Length 92: 262.087 ms\n",
      "Model: Base Quantized, Input Length 112: 313.483 ms\n",
      "Model: Base Quantized, Input Length 69: 205.545 ms\n",
      "Model: Base Quantized, Input Length 107: 336.568 ms\n",
      "Model: Base Quantized, Input Length 44: 192.256 ms\n",
      "Model: Base Quantized, Input Length 92: 331.949 ms\n",
      "Model: Base Quantized, Input Length 67: 188.725 ms\n",
      "Model: Base Quantized, Input Length 117: 359.323 ms\n",
      "Model: Base Quantized, Input Length 58: 196.970 ms\n",
      "Model: Base Quantized, Input Length 62: 206.048 ms\n",
      "Model: Base Quantized, Input Length 92: 255.012 ms\n",
      "Model: Base Quantized, Input Length 112: 317.782 ms\n",
      "Model: Base Quantized, Input Length 69: 210.498 ms\n",
      "Model: Base Quantized, Input Length 107: 330.510 ms\n",
      "Model: Base Quantized, Input Length 44: 188.298 ms\n",
      "Model: Base Quantized, Input Length 92: 332.589 ms\n",
      "Model: Base Quantized, Input Length 67: 182.599 ms\n",
      "Model: Base Quantized, Input Length 117: 362.858 ms\n",
      "Model: Base Quantized, Input Length 58: 203.466 ms\n",
      "Model: Base Quantized, Input Length 62: 198.399 ms\n",
      "Model: Base Quantized, Input Length 92: 248.953 ms\n",
      "Model: Base Quantized, Input Length 112: 306.230 ms\n",
      "Model: Base Quantized, Input Length 69: 208.321 ms\n",
      "Model: Base Quantized, Input Length 107: 315.519 ms\n",
      "Model: Base Quantized, Input Length 44: 182.865 ms\n",
      "Model: ONNX, Input Length 92: 159.742 ms\n",
      "Model: ONNX, Input Length 67: 128.161 ms\n",
      "Model: ONNX, Input Length 117: 278.143 ms\n",
      "Model: ONNX, Input Length 58: 173.794 ms\n",
      "Model: ONNX, Input Length 62: 163.379 ms\n",
      "Model: ONNX, Input Length 92: 207.622 ms\n",
      "Model: ONNX, Input Length 112: 257.837 ms\n",
      "Model: ONNX, Input Length 69: 141.862 ms\n",
      "Model: ONNX, Input Length 107: 244.924 ms\n",
      "Model: ONNX, Input Length 44: 154.970 ms\n",
      "Model: ONNX, Input Length 92: 191.516 ms\n",
      "Model: ONNX, Input Length 67: 154.994 ms\n",
      "Model: ONNX, Input Length 117: 275.158 ms\n",
      "Model: ONNX, Input Length 58: 150.784 ms\n",
      "Model: ONNX, Input Length 62: 166.881 ms\n",
      "Model: ONNX, Input Length 92: 212.791 ms\n",
      "Model: ONNX, Input Length 112: 229.814 ms\n",
      "Model: ONNX, Input Length 69: 143.255 ms\n",
      "Model: ONNX, Input Length 107: 228.507 ms\n",
      "Model: ONNX, Input Length 44: 134.577 ms\n",
      "Model: ONNX, Input Length 92: 189.499 ms\n",
      "Model: ONNX, Input Length 67: 134.893 ms\n",
      "Model: ONNX, Input Length 117: 354.483 ms\n",
      "Model: ONNX, Input Length 58: 196.133 ms\n",
      "Model: ONNX, Input Length 62: 169.014 ms\n",
      "Model: ONNX, Input Length 92: 193.815 ms\n",
      "Model: ONNX, Input Length 112: 249.581 ms\n",
      "Model: ONNX, Input Length 69: 162.391 ms\n",
      "Model: ONNX, Input Length 107: 256.865 ms\n",
      "Model: ONNX, Input Length 44: 158.427 ms\n",
      "Model: ONNX, Input Length 92: 202.278 ms\n",
      "Model: ONNX, Input Length 67: 145.406 ms\n",
      "Model: ONNX, Input Length 117: 305.258 ms\n",
      "Model: ONNX, Input Length 58: 187.240 ms\n",
      "Model: ONNX, Input Length 62: 154.799 ms\n",
      "Model: ONNX, Input Length 92: 200.032 ms\n",
      "Model: ONNX, Input Length 112: 255.176 ms\n",
      "Model: ONNX, Input Length 69: 160.616 ms\n",
      "Model: ONNX, Input Length 107: 271.346 ms\n",
      "Model: ONNX, Input Length 44: 194.952 ms\n",
      "Model: ONNX, Input Length 92: 234.155 ms\n",
      "Model: ONNX, Input Length 67: 157.380 ms\n",
      "Model: ONNX, Input Length 117: 311.781 ms\n",
      "Model: ONNX, Input Length 58: 176.728 ms\n",
      "Model: ONNX, Input Length 62: 181.665 ms\n",
      "Model: ONNX, Input Length 92: 277.852 ms\n",
      "Model: ONNX, Input Length 112: 312.337 ms\n",
      "Model: ONNX, Input Length 69: 219.441 ms\n",
      "Model: ONNX, Input Length 107: 289.034 ms\n",
      "Model: ONNX, Input Length 44: 173.095 ms\n",
      "Model: ONNX-OPT, Input Length 92: 305.782 ms\n",
      "Model: ONNX-OPT, Input Length 67: 195.420 ms\n",
      "Model: ONNX-OPT, Input Length 117: 322.193 ms\n",
      "Model: ONNX-OPT, Input Length 58: 181.404 ms\n",
      "Model: ONNX-OPT, Input Length 62: 201.222 ms\n",
      "Model: ONNX-OPT, Input Length 92: 246.726 ms\n",
      "Model: ONNX-OPT, Input Length 112: 361.257 ms\n",
      "Model: ONNX-OPT, Input Length 69: 305.680 ms\n",
      "Model: ONNX-OPT, Input Length 107: 312.967 ms\n",
      "Model: ONNX-OPT, Input Length 44: 155.129 ms\n",
      "Model: ONNX-OPT, Input Length 92: 215.407 ms\n",
      "Model: ONNX-OPT, Input Length 67: 153.196 ms\n",
      "Model: ONNX-OPT, Input Length 117: 288.312 ms\n",
      "Model: ONNX-OPT, Input Length 58: 171.361 ms\n",
      "Model: ONNX-OPT, Input Length 62: 201.454 ms\n",
      "Model: ONNX-OPT, Input Length 92: 266.398 ms\n",
      "Model: ONNX-OPT, Input Length 112: 276.522 ms\n",
      "Model: ONNX-OPT, Input Length 69: 176.369 ms\n",
      "Model: ONNX-OPT, Input Length 107: 252.837 ms\n",
      "Model: ONNX-OPT, Input Length 44: 148.142 ms\n",
      "Model: ONNX-OPT, Input Length 92: 274.817 ms\n",
      "Model: ONNX-OPT, Input Length 67: 184.523 ms\n",
      "Model: ONNX-OPT, Input Length 117: 313.175 ms\n",
      "Model: ONNX-OPT, Input Length 58: 176.092 ms\n",
      "Model: ONNX-OPT, Input Length 62: 204.278 ms\n",
      "Model: ONNX-OPT, Input Length 92: 242.306 ms\n",
      "Model: ONNX-OPT, Input Length 112: 262.121 ms\n",
      "Model: ONNX-OPT, Input Length 69: 169.370 ms\n",
      "Model: ONNX-OPT, Input Length 107: 260.903 ms\n",
      "Model: ONNX-OPT, Input Length 44: 150.164 ms\n",
      "Model: ONNX-OPT, Input Length 92: 266.136 ms\n",
      "Model: ONNX-OPT, Input Length 67: 160.089 ms\n",
      "Model: ONNX-OPT, Input Length 117: 329.283 ms\n",
      "Model: ONNX-OPT, Input Length 58: 174.689 ms\n",
      "Model: ONNX-OPT, Input Length 62: 162.410 ms\n",
      "Model: ONNX-OPT, Input Length 92: 216.661 ms\n",
      "Model: ONNX-OPT, Input Length 112: 276.415 ms\n",
      "Model: ONNX-OPT, Input Length 69: 166.082 ms\n",
      "Model: ONNX-OPT, Input Length 107: 297.836 ms\n",
      "Model: ONNX-OPT, Input Length 44: 183.164 ms\n",
      "Model: ONNX-OPT, Input Length 92: 274.518 ms\n",
      "Model: ONNX-OPT, Input Length 67: 175.981 ms\n",
      "Model: ONNX-OPT, Input Length 117: 308.508 ms\n",
      "Model: ONNX-OPT, Input Length 58: 164.618 ms\n",
      "Model: ONNX-OPT, Input Length 62: 162.515 ms\n",
      "Model: ONNX-OPT, Input Length 92: 213.319 ms\n",
      "Model: ONNX-OPT, Input Length 112: 303.915 ms\n",
      "Model: ONNX-OPT, Input Length 69: 181.723 ms\n",
      "Model: ONNX-OPT, Input Length 107: 320.512 ms\n",
      "Model: ONNX-OPT, Input Length 44: 205.724 ms\n",
      "Model: ONNX Quantized, Input Length 92: 221.850 ms\n",
      "Model: ONNX Quantized, Input Length 67: 160.337 ms\n",
      "Model: ONNX Quantized, Input Length 117: 314.328 ms\n",
      "Model: ONNX Quantized, Input Length 58: 163.729 ms\n",
      "Model: ONNX Quantized, Input Length 62: 171.563 ms\n",
      "Model: ONNX Quantized, Input Length 92: 213.158 ms\n",
      "Model: ONNX Quantized, Input Length 112: 270.211 ms\n",
      "Model: ONNX Quantized, Input Length 69: 175.845 ms\n",
      "Model: ONNX Quantized, Input Length 107: 232.553 ms\n",
      "Model: ONNX Quantized, Input Length 44: 130.095 ms\n",
      "Model: ONNX Quantized, Input Length 92: 201.443 ms\n",
      "Model: ONNX Quantized, Input Length 67: 154.279 ms\n",
      "Model: ONNX Quantized, Input Length 117: 305.302 ms\n",
      "Model: ONNX Quantized, Input Length 58: 151.275 ms\n",
      "Model: ONNX Quantized, Input Length 62: 140.640 ms\n",
      "Model: ONNX Quantized, Input Length 92: 181.719 ms\n",
      "Model: ONNX Quantized, Input Length 112: 236.113 ms\n",
      "Model: ONNX Quantized, Input Length 69: 139.640 ms\n",
      "Model: ONNX Quantized, Input Length 107: 242.542 ms\n",
      "Model: ONNX Quantized, Input Length 44: 121.313 ms\n",
      "Model: ONNX Quantized, Input Length 92: 197.911 ms\n",
      "Model: ONNX Quantized, Input Length 67: 149.011 ms\n",
      "Model: ONNX Quantized, Input Length 117: 284.374 ms\n",
      "Model: ONNX Quantized, Input Length 58: 151.348 ms\n",
      "Model: ONNX Quantized, Input Length 62: 151.122 ms\n",
      "Model: ONNX Quantized, Input Length 92: 181.488 ms\n",
      "Model: ONNX Quantized, Input Length 112: 239.593 ms\n",
      "Model: ONNX Quantized, Input Length 69: 126.497 ms\n",
      "Model: ONNX Quantized, Input Length 107: 206.159 ms\n",
      "Model: ONNX Quantized, Input Length 44: 138.122 ms\n",
      "Model: ONNX Quantized, Input Length 92: 195.191 ms\n",
      "Model: ONNX Quantized, Input Length 67: 140.547 ms\n",
      "Model: ONNX Quantized, Input Length 117: 300.622 ms\n",
      "Model: ONNX Quantized, Input Length 58: 149.697 ms\n",
      "Model: ONNX Quantized, Input Length 62: 121.210 ms\n",
      "Model: ONNX Quantized, Input Length 92: 155.378 ms\n",
      "Model: ONNX Quantized, Input Length 112: 196.146 ms\n",
      "Model: ONNX Quantized, Input Length 69: 124.483 ms\n",
      "Model: ONNX Quantized, Input Length 107: 201.509 ms\n",
      "Model: ONNX Quantized, Input Length 44: 114.966 ms\n",
      "Model: ONNX Quantized, Input Length 92: 159.035 ms\n",
      "Model: ONNX Quantized, Input Length 67: 112.372 ms\n",
      "Model: ONNX Quantized, Input Length 117: 273.971 ms\n",
      "Model: ONNX Quantized, Input Length 58: 167.022 ms\n",
      "Model: ONNX Quantized, Input Length 62: 165.730 ms\n",
      "Model: ONNX Quantized, Input Length 92: 210.773 ms\n",
      "Model: ONNX Quantized, Input Length 112: 236.762 ms\n",
      "Model: ONNX Quantized, Input Length 69: 143.372 ms\n",
      "Model: ONNX Quantized, Input Length 107: 205.905 ms\n",
      "Model: ONNX Quantized, Input Length 44: 122.747 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 219.457 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 67: 160.169 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 117: 272.842 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 58: 129.432 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 62: 136.718 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 174.557 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 112: 207.217 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 69: 133.399 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 107: 206.958 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 44: 117.555 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 162.730 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 67: 110.640 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 117: 258.653 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 58: 163.889 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 62: 136.574 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 156.979 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 112: 216.381 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 69: 133.896 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 107: 243.202 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 44: 143.604 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 210.655 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 67: 154.664 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 117: 256.049 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 58: 141.809 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 62: 162.959 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 198.645 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 112: 230.049 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 69: 160.108 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 107: 225.128 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 44: 148.797 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 224.679 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 67: 155.669 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 117: 291.571 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 58: 133.615 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 62: 148.728 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 174.159 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 112: 249.572 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 69: 139.563 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 107: 257.900 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 44: 145.074 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 222.682 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 67: 153.110 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 117: 286.765 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 58: 137.641 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 62: 151.974 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 92: 172.759 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 112: 200.654 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 69: 140.506 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 107: 218.109 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 44: 131.198 ms\n",
      "Loading: bert-base-uncased multirc\n",
      "Loading: roberta-base multirc\n",
      "Loading: bert-base-uncased quail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset quail (/Users/michaelhermann/.cache/huggingface/datasets/quail/quail/1.3.0/3cabab19c99e571b528209e14313cfff1debf772db9e24e19b4fcbeb8399336c)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: quail\n",
      "Preped data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3302.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Base, Input Length 310: 1163.226 ms\n",
      "Model: Base, Input Length 310: 1059.952 ms\n",
      "Model: Base, Input Length 320: 1136.427 ms\n",
      "Model: Base, Input Length 320: 1162.821 ms\n",
      "Model: Base, Input Length 325: 1289.864 ms\n",
      "Model: Base, Input Length 325: 1371.122 ms\n",
      "Model: Base, Input Length 278: 1188.074 ms\n",
      "Model: Base, Input Length 278: 1155.267 ms\n",
      "Model: Base, Input Length 313: 1341.083 ms\n",
      "Model: Base, Input Length 313: 1275.993 ms\n",
      "Model: Base, Input Length 310: 1314.396 ms\n",
      "Model: Base, Input Length 310: 1381.996 ms\n",
      "Model: Base, Input Length 320: 1361.550 ms\n",
      "Model: Base, Input Length 320: 1375.574 ms\n",
      "Model: Base, Input Length 325: 1523.039 ms\n",
      "Model: Base, Input Length 325: 1471.356 ms\n",
      "Model: Base, Input Length 278: 1292.772 ms\n",
      "Model: Base, Input Length 278: 1272.892 ms\n",
      "Model: Base, Input Length 313: 1324.723 ms\n",
      "Model: Base, Input Length 313: 1429.339 ms\n",
      "Model: Base, Input Length 310: 1394.206 ms\n",
      "Model: Base, Input Length 310: 1396.219 ms\n",
      "Model: Base, Input Length 320: 1474.157 ms\n",
      "Model: Base, Input Length 320: 1520.774 ms\n",
      "Model: Base, Input Length 325: 1727.926 ms\n",
      "Model: Base, Input Length 325: 1701.977 ms\n",
      "Model: Base, Input Length 278: 1390.304 ms\n",
      "Model: Base, Input Length 278: 1498.513 ms\n",
      "Model: Base, Input Length 313: 1431.831 ms\n",
      "Model: Base, Input Length 313: 1435.196 ms\n",
      "Model: Base, Input Length 310: 1391.684 ms\n",
      "Model: Base, Input Length 310: 1399.562 ms\n",
      "Model: Base, Input Length 320: 1446.269 ms\n",
      "Model: Base, Input Length 320: 1395.573 ms\n",
      "Model: Base, Input Length 325: 1621.451 ms\n",
      "Model: Base, Input Length 325: 1598.908 ms\n",
      "Model: Base, Input Length 278: 1322.310 ms\n",
      "Model: Base, Input Length 278: 1338.185 ms\n",
      "Model: Base, Input Length 313: 1428.076 ms\n",
      "Model: Base, Input Length 313: 1528.177 ms\n",
      "Model: Base, Input Length 310: 1564.222 ms\n",
      "Model: Base, Input Length 310: 1500.615 ms\n",
      "Model: Base, Input Length 320: 1571.262 ms\n",
      "Model: Base, Input Length 320: 1634.822 ms\n",
      "Model: Base, Input Length 325: 1913.224 ms\n",
      "Model: Base, Input Length 325: 1644.678 ms\n",
      "Model: Base, Input Length 278: 1507.374 ms\n",
      "Model: Base, Input Length 278: 1387.468 ms\n",
      "Model: Base, Input Length 313: 1379.025 ms\n",
      "Model: Base, Input Length 313: 1310.118 ms\n",
      "Model: Base Quantized, Input Length 310: 1355.027 ms\n",
      "Model: Base Quantized, Input Length 310: 1306.726 ms\n",
      "Model: Base Quantized, Input Length 320: 1338.289 ms\n",
      "Model: Base Quantized, Input Length 320: 1373.488 ms\n",
      "Model: Base Quantized, Input Length 325: 1478.201 ms\n",
      "Model: Base Quantized, Input Length 325: 1491.121 ms\n",
      "Model: Base Quantized, Input Length 278: 1308.091 ms\n",
      "Model: Base Quantized, Input Length 278: 1298.283 ms\n",
      "Model: Base Quantized, Input Length 313: 1291.586 ms\n",
      "Model: Base Quantized, Input Length 313: 1289.558 ms\n",
      "Model: Base Quantized, Input Length 310: 1339.219 ms\n",
      "Model: Base Quantized, Input Length 310: 1256.090 ms\n",
      "Model: Base Quantized, Input Length 320: 1277.479 ms\n",
      "Model: Base Quantized, Input Length 320: 1352.673 ms\n",
      "Model: Base Quantized, Input Length 325: 1398.509 ms\n",
      "Model: Base Quantized, Input Length 325: 1426.268 ms\n",
      "Model: Base Quantized, Input Length 278: 1233.626 ms\n",
      "Model: Base Quantized, Input Length 278: 1241.949 ms\n",
      "Model: Base Quantized, Input Length 313: 1271.621 ms\n",
      "Model: Base Quantized, Input Length 313: 1314.017 ms\n",
      "Model: Base Quantized, Input Length 310: 1527.115 ms\n",
      "Model: Base Quantized, Input Length 310: 1225.319 ms\n",
      "Model: Base Quantized, Input Length 320: 1291.638 ms\n",
      "Model: Base Quantized, Input Length 320: 1246.402 ms\n",
      "Model: Base Quantized, Input Length 325: 1375.040 ms\n",
      "Model: Base Quantized, Input Length 325: 1382.181 ms\n",
      "Model: Base Quantized, Input Length 278: 1229.377 ms\n",
      "Model: Base Quantized, Input Length 278: 1253.664 ms\n",
      "Model: Base Quantized, Input Length 313: 1218.424 ms\n",
      "Model: Base Quantized, Input Length 313: 1235.685 ms\n",
      "Model: Base Quantized, Input Length 310: 1279.878 ms\n",
      "Model: Base Quantized, Input Length 310: 1274.270 ms\n",
      "Model: Base Quantized, Input Length 320: 1233.304 ms\n",
      "Model: Base Quantized, Input Length 320: 1326.620 ms\n",
      "Model: Base Quantized, Input Length 325: 1392.389 ms\n",
      "Model: Base Quantized, Input Length 325: 1379.271 ms\n",
      "Model: Base Quantized, Input Length 278: 1214.134 ms\n",
      "Model: Base Quantized, Input Length 278: 1214.887 ms\n",
      "Model: Base Quantized, Input Length 313: 1210.466 ms\n",
      "Model: Base Quantized, Input Length 313: 1201.785 ms\n",
      "Model: Base Quantized, Input Length 310: 1265.874 ms\n",
      "Model: Base Quantized, Input Length 310: 1223.055 ms\n",
      "Model: Base Quantized, Input Length 320: 1253.090 ms\n",
      "Model: Base Quantized, Input Length 320: 1254.686 ms\n",
      "Model: Base Quantized, Input Length 325: 1398.168 ms\n",
      "Model: Base Quantized, Input Length 325: 1423.776 ms\n",
      "Model: Base Quantized, Input Length 278: 1224.135 ms\n",
      "Model: Base Quantized, Input Length 278: 1228.433 ms\n",
      "Model: Base Quantized, Input Length 313: 1256.844 ms\n",
      "Model: Base Quantized, Input Length 313: 1282.169 ms\n",
      "Model: ONNX, Input Length 310: 753.306 ms\n",
      "Model: ONNX, Input Length 310: 774.194 ms\n",
      "Model: ONNX, Input Length 320: 820.317 ms\n",
      "Model: ONNX, Input Length 320: 805.477 ms\n",
      "Model: ONNX, Input Length 325: 834.279 ms\n",
      "Model: ONNX, Input Length 325: 869.624 ms\n",
      "Model: ONNX, Input Length 278: 797.772 ms\n",
      "Model: ONNX, Input Length 278: 797.033 ms\n",
      "Model: ONNX, Input Length 313: 800.585 ms\n",
      "Model: ONNX, Input Length 313: 806.053 ms\n",
      "Model: ONNX, Input Length 310: 884.818 ms\n",
      "Model: ONNX, Input Length 310: 808.349 ms\n",
      "Model: ONNX, Input Length 320: 861.471 ms\n",
      "Model: ONNX, Input Length 320: 876.880 ms\n",
      "Model: ONNX, Input Length 325: 1001.313 ms\n",
      "Model: ONNX, Input Length 325: 961.687 ms\n",
      "Model: ONNX, Input Length 278: 823.768 ms\n",
      "Model: ONNX, Input Length 278: 841.328 ms\n",
      "Model: ONNX, Input Length 313: 856.573 ms\n",
      "Model: ONNX, Input Length 313: 852.389 ms\n",
      "Model: ONNX, Input Length 310: 942.980 ms\n",
      "Model: ONNX, Input Length 310: 860.917 ms\n",
      "Model: ONNX, Input Length 320: 884.996 ms\n",
      "Model: ONNX, Input Length 320: 891.270 ms\n",
      "Model: ONNX, Input Length 325: 954.230 ms\n",
      "Model: ONNX, Input Length 325: 967.691 ms\n",
      "Model: ONNX, Input Length 278: 850.755 ms\n",
      "Model: ONNX, Input Length 278: 850.867 ms\n",
      "Model: ONNX, Input Length 313: 931.165 ms\n",
      "Model: ONNX, Input Length 313: 960.661 ms\n",
      "Model: ONNX, Input Length 310: 1053.658 ms\n",
      "Model: ONNX, Input Length 310: 934.900 ms\n",
      "Model: ONNX, Input Length 320: 1087.578 ms\n",
      "Model: ONNX, Input Length 320: 1170.494 ms\n",
      "Model: ONNX, Input Length 325: 1145.628 ms\n",
      "Model: ONNX, Input Length 325: 1091.331 ms\n",
      "Model: ONNX, Input Length 278: 937.632 ms\n",
      "Model: ONNX, Input Length 278: 913.067 ms\n",
      "Model: ONNX, Input Length 313: 1011.093 ms\n",
      "Model: ONNX, Input Length 313: 1150.478 ms\n",
      "Model: ONNX, Input Length 310: 1039.366 ms\n",
      "Model: ONNX, Input Length 310: 990.313 ms\n",
      "Model: ONNX, Input Length 320: 1102.361 ms\n",
      "Model: ONNX, Input Length 320: 976.247 ms\n",
      "Model: ONNX, Input Length 325: 1061.262 ms\n",
      "Model: ONNX, Input Length 325: 1205.415 ms\n",
      "Model: ONNX, Input Length 278: 1030.515 ms\n",
      "Model: ONNX, Input Length 278: 1007.964 ms\n",
      "Model: ONNX, Input Length 313: 1026.361 ms\n",
      "Model: ONNX, Input Length 313: 1193.754 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1266.415 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1188.964 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1286.621 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1280.598 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1437.118 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1340.135 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1201.278 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1299.510 ms\n",
      "Model: ONNX-OPT, Input Length 313: 1325.361 ms\n",
      "Model: ONNX-OPT, Input Length 313: 1305.582 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1342.014 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1148.824 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1123.203 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1171.844 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1442.097 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1461.872 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1304.656 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1270.465 ms\n",
      "Model: ONNX-OPT, Input Length 313: 1169.582 ms\n",
      "Model: ONNX-OPT, Input Length 313: 1196.445 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1089.104 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1063.388 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1071.257 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1086.505 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1047.775 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1142.136 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1019.036 ms\n",
      "Model: ONNX-OPT, Input Length 278: 909.565 ms\n",
      "Model: ONNX-OPT, Input Length 313: 929.404 ms\n",
      "Model: ONNX-OPT, Input Length 313: 976.742 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1046.367 ms\n",
      "Model: ONNX-OPT, Input Length 310: 986.729 ms\n",
      "Model: ONNX-OPT, Input Length 320: 996.157 ms\n",
      "Model: ONNX-OPT, Input Length 320: 978.960 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1046.514 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1079.595 ms\n",
      "Model: ONNX-OPT, Input Length 278: 992.776 ms\n",
      "Model: ONNX-OPT, Input Length 278: 958.585 ms\n",
      "Model: ONNX-OPT, Input Length 313: 950.490 ms\n",
      "Model: ONNX-OPT, Input Length 313: 1016.280 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1012.468 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1023.530 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1224.537 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1118.255 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1217.304 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1266.179 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1160.942 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1187.578 ms\n",
      "Model: ONNX-OPT, Input Length 313: 1257.774 ms\n",
      "Model: ONNX-OPT, Input Length 313: 1274.130 ms\n",
      "Model: ONNX Quantized, Input Length 310: 1045.404 ms\n",
      "Model: ONNX Quantized, Input Length 310: 894.198 ms\n",
      "Model: ONNX Quantized, Input Length 320: 911.219 ms\n",
      "Model: ONNX Quantized, Input Length 320: 965.963 ms\n",
      "Model: ONNX Quantized, Input Length 325: 1000.955 ms\n",
      "Model: ONNX Quantized, Input Length 325: 1103.735 ms\n",
      "Model: ONNX Quantized, Input Length 278: 984.110 ms\n",
      "Model: ONNX Quantized, Input Length 278: 893.618 ms\n",
      "Model: ONNX Quantized, Input Length 313: 828.329 ms\n",
      "Model: ONNX Quantized, Input Length 313: 869.329 ms\n",
      "Model: ONNX Quantized, Input Length 310: 870.971 ms\n",
      "Model: ONNX Quantized, Input Length 310: 883.456 ms\n",
      "Model: ONNX Quantized, Input Length 320: 975.560 ms\n",
      "Model: ONNX Quantized, Input Length 320: 946.156 ms\n",
      "Model: ONNX Quantized, Input Length 325: 1069.769 ms\n",
      "Model: ONNX Quantized, Input Length 325: 1031.286 ms\n",
      "Model: ONNX Quantized, Input Length 278: 893.211 ms\n",
      "Model: ONNX Quantized, Input Length 278: 892.156 ms\n",
      "Model: ONNX Quantized, Input Length 313: 900.101 ms\n",
      "Model: ONNX Quantized, Input Length 313: 937.316 ms\n",
      "Model: ONNX Quantized, Input Length 310: 982.781 ms\n",
      "Model: ONNX Quantized, Input Length 310: 944.863 ms\n",
      "Model: ONNX Quantized, Input Length 320: 848.929 ms\n",
      "Model: ONNX Quantized, Input Length 320: 922.213 ms\n",
      "Model: ONNX Quantized, Input Length 325: 1022.387 ms\n",
      "Model: ONNX Quantized, Input Length 325: 959.624 ms\n",
      "Model: ONNX Quantized, Input Length 278: 960.707 ms\n",
      "Model: ONNX Quantized, Input Length 278: 958.887 ms\n",
      "Model: ONNX Quantized, Input Length 313: 960.075 ms\n",
      "Model: ONNX Quantized, Input Length 313: 921.492 ms\n",
      "Model: ONNX Quantized, Input Length 310: 866.156 ms\n",
      "Model: ONNX Quantized, Input Length 310: 797.626 ms\n",
      "Model: ONNX Quantized, Input Length 320: 817.919 ms\n",
      "Model: ONNX Quantized, Input Length 320: 828.740 ms\n",
      "Model: ONNX Quantized, Input Length 325: 890.103 ms\n",
      "Model: ONNX Quantized, Input Length 325: 802.149 ms\n",
      "Model: ONNX Quantized, Input Length 278: 754.539 ms\n",
      "Model: ONNX Quantized, Input Length 278: 696.485 ms\n",
      "Model: ONNX Quantized, Input Length 313: 808.011 ms\n",
      "Model: ONNX Quantized, Input Length 313: 946.284 ms\n",
      "Model: ONNX Quantized, Input Length 310: 763.045 ms\n",
      "Model: ONNX Quantized, Input Length 310: 892.913 ms\n",
      "Model: ONNX Quantized, Input Length 320: 750.907 ms\n",
      "Model: ONNX Quantized, Input Length 320: 925.608 ms\n",
      "Model: ONNX Quantized, Input Length 325: 986.172 ms\n",
      "Model: ONNX Quantized, Input Length 325: 994.241 ms\n",
      "Model: ONNX Quantized, Input Length 278: 826.728 ms\n",
      "Model: ONNX Quantized, Input Length 278: 760.683 ms\n",
      "Model: ONNX Quantized, Input Length 313: 821.157 ms\n",
      "Model: ONNX Quantized, Input Length 313: 805.072 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 873.138 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 808.208 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 845.335 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 760.428 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 769.298 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 786.743 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 718.244 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 756.396 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 820.382 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 823.851 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 823.588 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 801.950 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 805.402 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 872.450 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 945.260 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 945.849 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 791.864 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 827.361 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 919.138 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 895.408 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 882.742 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 859.652 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 921.488 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 778.074 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 907.162 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 895.850 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 934.523 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 904.882 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 847.311 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 759.095 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 865.266 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 821.586 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 967.538 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 901.634 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 947.244 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 904.232 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 840.408 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 935.003 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 898.861 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 773.959 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 820.355 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 792.213 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 801.777 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 822.549 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 853.199 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 824.253 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 708.682 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 723.282 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 869.651 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 812.704 ms\n",
      "Loading: roberta-base quail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset quail (/Users/michaelhermann/.cache/huggingface/datasets/quail/quail/1.3.0/3cabab19c99e571b528209e14313cfff1debf772db9e24e19b4fcbeb8399336c)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: quail\n",
      "Preped data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4187.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Base, Input Length 310: 1227.966 ms\n",
      "Model: Base, Input Length 310: 1319.022 ms\n",
      "Model: Base, Input Length 320: 1443.268 ms\n",
      "Model: Base, Input Length 320: 1188.583 ms\n",
      "Model: Base, Input Length 325: 1245.538 ms\n",
      "Model: Base, Input Length 325: 1323.734 ms\n",
      "Model: Base, Input Length 278: 1169.779 ms\n",
      "Model: Base, Input Length 278: 1432.532 ms\n",
      "Model: Base, Input Length 313: 1338.215 ms\n",
      "Model: Base, Input Length 313: 1386.475 ms\n",
      "Model: Base, Input Length 310: 1785.359 ms\n",
      "Model: Base, Input Length 310: 1737.205 ms\n",
      "Model: Base, Input Length 320: 1553.691 ms\n",
      "Model: Base, Input Length 320: 1644.744 ms\n",
      "Model: Base, Input Length 325: 1806.277 ms\n",
      "Model: Base, Input Length 325: 1815.897 ms\n",
      "Model: Base, Input Length 278: 1469.884 ms\n",
      "Model: Base, Input Length 278: 1761.851 ms\n",
      "Model: Base, Input Length 313: 1850.211 ms\n",
      "Model: Base, Input Length 313: 1590.867 ms\n",
      "Model: Base, Input Length 310: 1814.666 ms\n",
      "Model: Base, Input Length 310: 1956.559 ms\n",
      "Model: Base, Input Length 320: 1670.192 ms\n",
      "Model: Base, Input Length 320: 1590.382 ms\n",
      "Model: Base, Input Length 325: 1742.045 ms\n",
      "Model: Base, Input Length 325: 1895.024 ms\n",
      "Model: Base, Input Length 278: 1796.578 ms\n",
      "Model: Base, Input Length 278: 1736.356 ms\n",
      "Model: Base, Input Length 313: 1777.131 ms\n",
      "Model: Base, Input Length 313: 1578.247 ms\n",
      "Model: Base, Input Length 310: 1599.292 ms\n",
      "Model: Base, Input Length 310: 1563.690 ms\n",
      "Model: Base, Input Length 320: 1523.640 ms\n",
      "Model: Base, Input Length 320: 1637.301 ms\n",
      "Model: Base, Input Length 325: 1869.322 ms\n",
      "Model: Base, Input Length 325: 1813.851 ms\n",
      "Model: Base, Input Length 278: 1484.369 ms\n",
      "Model: Base, Input Length 278: 1382.678 ms\n",
      "Model: Base, Input Length 313: 1442.227 ms\n",
      "Model: Base, Input Length 313: 1334.573 ms\n",
      "Model: Base, Input Length 310: 1462.411 ms\n",
      "Model: Base, Input Length 310: 1496.639 ms\n",
      "Model: Base, Input Length 320: 1684.220 ms\n",
      "Model: Base, Input Length 320: 1460.787 ms\n",
      "Model: Base, Input Length 325: 1779.761 ms\n",
      "Model: Base, Input Length 325: 1636.893 ms\n",
      "Model: Base, Input Length 278: 1401.142 ms\n",
      "Model: Base, Input Length 278: 1350.375 ms\n",
      "Model: Base, Input Length 313: 1570.587 ms\n",
      "Model: Base, Input Length 313: 1604.456 ms\n",
      "Model: Base Quantized, Input Length 310: 1742.021 ms\n",
      "Model: Base Quantized, Input Length 310: 1830.832 ms\n",
      "Model: Base Quantized, Input Length 320: 1676.858 ms\n",
      "Model: Base Quantized, Input Length 320: 1581.567 ms\n",
      "Model: Base Quantized, Input Length 325: 1690.760 ms\n",
      "Model: Base Quantized, Input Length 325: 1597.323 ms\n",
      "Model: Base Quantized, Input Length 278: 1380.382 ms\n",
      "Model: Base Quantized, Input Length 278: 1383.783 ms\n",
      "Model: Base Quantized, Input Length 313: 1535.523 ms\n",
      "Model: Base Quantized, Input Length 313: 1417.210 ms\n",
      "Model: Base Quantized, Input Length 310: 1727.517 ms\n",
      "Model: Base Quantized, Input Length 310: 1598.769 ms\n",
      "Model: Base Quantized, Input Length 320: 1459.716 ms\n",
      "Model: Base Quantized, Input Length 320: 1572.725 ms\n",
      "Model: Base Quantized, Input Length 325: 1726.563 ms\n",
      "Model: Base Quantized, Input Length 325: 1721.614 ms\n",
      "Model: Base Quantized, Input Length 278: 1479.269 ms\n",
      "Model: Base Quantized, Input Length 278: 1460.178 ms\n",
      "Model: Base Quantized, Input Length 313: 1478.752 ms\n",
      "Model: Base Quantized, Input Length 313: 1394.462 ms\n",
      "Model: Base Quantized, Input Length 310: 1609.880 ms\n",
      "Model: Base Quantized, Input Length 310: 1736.943 ms\n",
      "Model: Base Quantized, Input Length 320: 1703.683 ms\n",
      "Model: Base Quantized, Input Length 320: 1582.346 ms\n",
      "Model: Base Quantized, Input Length 325: 1732.439 ms\n",
      "Model: Base Quantized, Input Length 325: 1670.407 ms\n",
      "Model: Base Quantized, Input Length 278: 1624.672 ms\n",
      "Model: Base Quantized, Input Length 278: 1467.038 ms\n",
      "Model: Base Quantized, Input Length 313: 1583.537 ms\n",
      "Model: Base Quantized, Input Length 313: 1515.678 ms\n",
      "Model: Base Quantized, Input Length 310: 1716.616 ms\n",
      "Model: Base Quantized, Input Length 310: 1590.242 ms\n",
      "Model: Base Quantized, Input Length 320: 1594.212 ms\n",
      "Model: Base Quantized, Input Length 320: 1592.368 ms\n",
      "Model: Base Quantized, Input Length 325: 1610.447 ms\n",
      "Model: Base Quantized, Input Length 325: 1682.653 ms\n",
      "Model: Base Quantized, Input Length 278: 1534.409 ms\n",
      "Model: Base Quantized, Input Length 278: 1477.596 ms\n",
      "Model: Base Quantized, Input Length 313: 1391.748 ms\n",
      "Model: Base Quantized, Input Length 313: 1436.701 ms\n",
      "Model: Base Quantized, Input Length 310: 1514.360 ms\n",
      "Model: Base Quantized, Input Length 310: 1434.177 ms\n",
      "Model: Base Quantized, Input Length 320: 1485.849 ms\n",
      "Model: Base Quantized, Input Length 320: 1663.336 ms\n",
      "Model: Base Quantized, Input Length 325: 1651.828 ms\n",
      "Model: Base Quantized, Input Length 325: 1625.144 ms\n",
      "Model: Base Quantized, Input Length 278: 1321.205 ms\n",
      "Model: Base Quantized, Input Length 278: 1505.185 ms\n",
      "Model: Base Quantized, Input Length 313: 1579.140 ms\n",
      "Model: Base Quantized, Input Length 313: 1510.738 ms\n",
      "Model: ONNX, Input Length 310: 828.309 ms\n",
      "Model: ONNX, Input Length 310: 815.813 ms\n",
      "Model: ONNX, Input Length 320: 797.435 ms\n",
      "Model: ONNX, Input Length 320: 812.427 ms\n",
      "Model: ONNX, Input Length 325: 873.034 ms\n",
      "Model: ONNX, Input Length 325: 910.030 ms\n",
      "Model: ONNX, Input Length 278: 989.788 ms\n",
      "Model: ONNX, Input Length 278: 938.860 ms\n",
      "Model: ONNX, Input Length 313: 944.588 ms\n",
      "Model: ONNX, Input Length 313: 955.897 ms\n",
      "Model: ONNX, Input Length 310: 1110.766 ms\n",
      "Model: ONNX, Input Length 310: 1041.453 ms\n",
      "Model: ONNX, Input Length 320: 1100.395 ms\n",
      "Model: ONNX, Input Length 320: 1042.738 ms\n",
      "Model: ONNX, Input Length 325: 997.887 ms\n",
      "Model: ONNX, Input Length 325: 1057.323 ms\n",
      "Model: ONNX, Input Length 278: 1032.030 ms\n",
      "Model: ONNX, Input Length 278: 1164.391 ms\n",
      "Model: ONNX, Input Length 313: 1069.506 ms\n",
      "Model: ONNX, Input Length 313: 990.941 ms\n",
      "Model: ONNX, Input Length 310: 1146.403 ms\n",
      "Model: ONNX, Input Length 310: 1188.558 ms\n",
      "Model: ONNX, Input Length 320: 1241.609 ms\n",
      "Model: ONNX, Input Length 320: 1229.687 ms\n",
      "Model: ONNX, Input Length 325: 1286.577 ms\n",
      "Model: ONNX, Input Length 325: 1172.122 ms\n",
      "Model: ONNX, Input Length 278: 965.614 ms\n",
      "Model: ONNX, Input Length 278: 1017.347 ms\n",
      "Model: ONNX, Input Length 313: 1218.874 ms\n",
      "Model: ONNX, Input Length 313: 1116.841 ms\n",
      "Model: ONNX, Input Length 310: 1232.920 ms\n",
      "Model: ONNX, Input Length 310: 998.989 ms\n",
      "Model: ONNX, Input Length 320: 994.410 ms\n",
      "Model: ONNX, Input Length 320: 1124.804 ms\n",
      "Model: ONNX, Input Length 325: 1250.894 ms\n",
      "Model: ONNX, Input Length 325: 1215.860 ms\n",
      "Model: ONNX, Input Length 278: 1050.982 ms\n",
      "Model: ONNX, Input Length 278: 1037.578 ms\n",
      "Model: ONNX, Input Length 313: 1096.347 ms\n",
      "Model: ONNX, Input Length 313: 1194.121 ms\n",
      "Model: ONNX, Input Length 310: 1193.346 ms\n",
      "Model: ONNX, Input Length 310: 1181.064 ms\n",
      "Model: ONNX, Input Length 320: 1122.087 ms\n",
      "Model: ONNX, Input Length 320: 1153.645 ms\n",
      "Model: ONNX, Input Length 325: 1122.575 ms\n",
      "Model: ONNX, Input Length 325: 1112.124 ms\n",
      "Model: ONNX, Input Length 278: 990.500 ms\n",
      "Model: ONNX, Input Length 278: 1071.627 ms\n",
      "Model: ONNX, Input Length 313: 1270.902 ms\n",
      "Model: ONNX, Input Length 313: 1164.094 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1274.238 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1061.589 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1046.501 ms\n",
      "Model: ONNX-OPT, Input Length 320: 938.997 ms\n",
      "Model: ONNX-OPT, Input Length 325: 967.623 ms\n",
      "Model: ONNX-OPT, Input Length 325: 991.384 ms\n",
      "Model: ONNX-OPT, Input Length 278: 854.075 ms\n",
      "Model: ONNX-OPT, Input Length 278: 856.921 ms\n",
      "Model: ONNX-OPT, Input Length 313: 888.991 ms\n",
      "Model: ONNX-OPT, Input Length 313: 894.683 ms\n",
      "Model: ONNX-OPT, Input Length 310: 999.150 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1014.574 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1082.620 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1010.805 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1099.020 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1115.075 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1098.255 ms\n",
      "Model: ONNX-OPT, Input Length 278: 914.738 ms\n",
      "Model: ONNX-OPT, Input Length 313: 934.721 ms\n",
      "Model: ONNX-OPT, Input Length 313: 921.619 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1101.268 ms\n",
      "Model: ONNX-OPT, Input Length 310: 936.922 ms\n",
      "Model: ONNX-OPT, Input Length 320: 922.954 ms\n",
      "Model: ONNX-OPT, Input Length 320: 932.513 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1038.704 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1205.021 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1085.369 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1233.201 ms\n",
      "Model: ONNX-OPT, Input Length 313: 1254.788 ms\n",
      "Model: ONNX-OPT, Input Length 313: 1290.162 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1164.388 ms\n",
      "Model: ONNX-OPT, Input Length 310: 982.283 ms\n",
      "Model: ONNX-OPT, Input Length 320: 993.398 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1146.728 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1143.098 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1126.017 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1078.851 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1205.607 ms\n",
      "Model: ONNX-OPT, Input Length 313: 1222.595 ms\n",
      "Model: ONNX-OPT, Input Length 313: 1073.292 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1187.016 ms\n",
      "Model: ONNX-OPT, Input Length 310: 1117.363 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1204.535 ms\n",
      "Model: ONNX-OPT, Input Length 320: 1311.937 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1395.598 ms\n",
      "Model: ONNX-OPT, Input Length 325: 1418.801 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1376.103 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1347.477 ms\n",
      "Model: ONNX-OPT, Input Length 313: 1331.275 ms\n",
      "Model: ONNX-OPT, Input Length 313: 1348.499 ms\n",
      "Model: ONNX Quantized, Input Length 310: 990.647 ms\n",
      "Model: ONNX Quantized, Input Length 310: 932.975 ms\n",
      "Model: ONNX Quantized, Input Length 320: 919.261 ms\n",
      "Model: ONNX Quantized, Input Length 320: 878.207 ms\n",
      "Model: ONNX Quantized, Input Length 325: 920.678 ms\n",
      "Model: ONNX Quantized, Input Length 325: 921.057 ms\n",
      "Model: ONNX Quantized, Input Length 278: 783.835 ms\n",
      "Model: ONNX Quantized, Input Length 278: 752.061 ms\n",
      "Model: ONNX Quantized, Input Length 313: 825.974 ms\n",
      "Model: ONNX Quantized, Input Length 313: 796.525 ms\n",
      "Model: ONNX Quantized, Input Length 310: 814.132 ms\n",
      "Model: ONNX Quantized, Input Length 310: 757.386 ms\n",
      "Model: ONNX Quantized, Input Length 320: 781.436 ms\n",
      "Model: ONNX Quantized, Input Length 320: 802.757 ms\n",
      "Model: ONNX Quantized, Input Length 325: 899.469 ms\n",
      "Model: ONNX Quantized, Input Length 325: 848.065 ms\n",
      "Model: ONNX Quantized, Input Length 278: 698.188 ms\n",
      "Model: ONNX Quantized, Input Length 278: 700.125 ms\n",
      "Model: ONNX Quantized, Input Length 313: 752.794 ms\n",
      "Model: ONNX Quantized, Input Length 313: 718.283 ms\n",
      "Model: ONNX Quantized, Input Length 310: 728.991 ms\n",
      "Model: ONNX Quantized, Input Length 310: 790.087 ms\n",
      "Model: ONNX Quantized, Input Length 320: 733.415 ms\n",
      "Model: ONNX Quantized, Input Length 320: 732.554 ms\n",
      "Model: ONNX Quantized, Input Length 325: 777.514 ms\n",
      "Model: ONNX Quantized, Input Length 325: 765.973 ms\n",
      "Model: ONNX Quantized, Input Length 278: 669.050 ms\n",
      "Model: ONNX Quantized, Input Length 278: 683.995 ms\n",
      "Model: ONNX Quantized, Input Length 313: 728.326 ms\n",
      "Model: ONNX Quantized, Input Length 313: 735.256 ms\n",
      "Model: ONNX Quantized, Input Length 310: 714.000 ms\n",
      "Model: ONNX Quantized, Input Length 310: 744.461 ms\n",
      "Model: ONNX Quantized, Input Length 320: 719.100 ms\n",
      "Model: ONNX Quantized, Input Length 320: 703.509 ms\n",
      "Model: ONNX Quantized, Input Length 325: 750.452 ms\n",
      "Model: ONNX Quantized, Input Length 325: 779.768 ms\n",
      "Model: ONNX Quantized, Input Length 278: 660.492 ms\n",
      "Model: ONNX Quantized, Input Length 278: 682.421 ms\n",
      "Model: ONNX Quantized, Input Length 313: 680.296 ms\n",
      "Model: ONNX Quantized, Input Length 313: 679.562 ms\n",
      "Model: ONNX Quantized, Input Length 310: 745.218 ms\n",
      "Model: ONNX Quantized, Input Length 310: 708.606 ms\n",
      "Model: ONNX Quantized, Input Length 320: 757.600 ms\n",
      "Model: ONNX Quantized, Input Length 320: 716.898 ms\n",
      "Model: ONNX Quantized, Input Length 325: 761.318 ms\n",
      "Model: ONNX Quantized, Input Length 325: 765.202 ms\n",
      "Model: ONNX Quantized, Input Length 278: 716.063 ms\n",
      "Model: ONNX Quantized, Input Length 278: 651.187 ms\n",
      "Model: ONNX Quantized, Input Length 313: 683.445 ms\n",
      "Model: ONNX Quantized, Input Length 313: 721.215 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 809.099 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 750.341 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 703.419 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 742.051 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 744.745 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 795.006 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 693.469 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 677.373 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 691.409 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 730.185 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 847.540 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 684.094 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 691.543 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 696.534 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 738.380 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 802.913 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 853.280 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 816.452 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 775.130 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 787.536 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 803.766 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 712.895 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 724.655 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 720.542 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 823.445 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 849.922 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 715.644 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 763.196 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 801.465 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 779.862 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 778.136 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 754.062 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 849.642 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 782.327 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 940.097 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 921.748 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 794.639 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 793.609 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 745.949 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 895.111 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 941.805 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 310: 978.452 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 891.939 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 320: 883.053 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 950.270 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 325: 1006.628 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 765.453 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 817.554 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 801.644 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 313: 761.997 ms\n",
      "Loading: roberta-base quartz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset quartz (/Users/michaelhermann/.cache/huggingface/datasets/quartz/default/0.1.0/6e5195fb88ecd7a75eda5d8f3549c262c8b15267366f38f9c153f40da92724a6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: quartz\n",
      "Preped data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3025.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Base, Input Length 19: 86.306 ms\n",
      "Model: Base, Input Length 20: 64.568 ms\n",
      "Model: Base, Input Length 20: 79.444 ms\n",
      "Model: Base, Input Length 18: 79.097 ms\n",
      "Model: Base, Input Length 18: 88.553 ms\n",
      "Model: Base, Input Length 10: 69.967 ms\n",
      "Model: Base, Input Length 16: 93.287 ms\n",
      "Model: Base, Input Length 10: 113.341 ms\n",
      "Model: Base, Input Length 22: 87.493 ms\n",
      "Model: Base, Input Length 14: 70.494 ms\n",
      "Model: Base, Input Length 19: 84.970 ms\n",
      "Model: Base, Input Length 20: 72.066 ms\n",
      "Model: Base, Input Length 20: 81.114 ms\n",
      "Model: Base, Input Length 18: 81.951 ms\n",
      "Model: Base, Input Length 18: 103.982 ms\n",
      "Model: Base, Input Length 10: 79.603 ms\n",
      "Model: Base, Input Length 16: 107.818 ms\n",
      "Model: Base, Input Length 10: 114.892 ms\n",
      "Model: Base, Input Length 22: 88.366 ms\n",
      "Model: Base, Input Length 14: 71.104 ms\n",
      "Model: Base, Input Length 19: 87.990 ms\n",
      "Model: Base, Input Length 20: 72.964 ms\n",
      "Model: Base, Input Length 20: 86.119 ms\n",
      "Model: Base, Input Length 18: 108.206 ms\n",
      "Model: Base, Input Length 18: 108.174 ms\n",
      "Model: Base, Input Length 10: 90.745 ms\n",
      "Model: Base, Input Length 16: 94.842 ms\n",
      "Model: Base, Input Length 10: 121.131 ms\n",
      "Model: Base, Input Length 22: 102.121 ms\n",
      "Model: Base, Input Length 14: 72.409 ms\n",
      "Model: Base, Input Length 19: 81.532 ms\n",
      "Model: Base, Input Length 20: 64.402 ms\n",
      "Model: Base, Input Length 20: 97.209 ms\n",
      "Model: Base, Input Length 18: 99.997 ms\n",
      "Model: Base, Input Length 18: 121.164 ms\n",
      "Model: Base, Input Length 10: 80.472 ms\n",
      "Model: Base, Input Length 16: 92.711 ms\n",
      "Model: Base, Input Length 10: 112.242 ms\n",
      "Model: Base, Input Length 22: 89.144 ms\n",
      "Model: Base, Input Length 14: 61.500 ms\n",
      "Model: Base, Input Length 19: 85.179 ms\n",
      "Model: Base, Input Length 20: 73.423 ms\n",
      "Model: Base, Input Length 20: 89.592 ms\n",
      "Model: Base, Input Length 18: 89.506 ms\n",
      "Model: Base, Input Length 18: 91.362 ms\n",
      "Model: Base, Input Length 10: 64.839 ms\n",
      "Model: Base, Input Length 16: 96.997 ms\n",
      "Model: Base, Input Length 10: 134.228 ms\n",
      "Model: Base, Input Length 22: 110.670 ms\n",
      "Model: Base, Input Length 14: 69.440 ms\n",
      "Model: Base Quantized, Input Length 19: 101.251 ms\n",
      "Model: Base Quantized, Input Length 20: 85.050 ms\n",
      "Model: Base Quantized, Input Length 20: 91.919 ms\n",
      "Model: Base Quantized, Input Length 18: 112.409 ms\n",
      "Model: Base Quantized, Input Length 18: 112.753 ms\n",
      "Model: Base Quantized, Input Length 10: 81.115 ms\n",
      "Model: Base Quantized, Input Length 16: 94.953 ms\n",
      "Model: Base Quantized, Input Length 10: 140.464 ms\n",
      "Model: Base Quantized, Input Length 22: 128.737 ms\n",
      "Model: Base Quantized, Input Length 14: 96.621 ms\n",
      "Model: Base Quantized, Input Length 19: 97.015 ms\n",
      "Model: Base Quantized, Input Length 20: 91.733 ms\n",
      "Model: Base Quantized, Input Length 20: 96.589 ms\n",
      "Model: Base Quantized, Input Length 18: 92.495 ms\n",
      "Model: Base Quantized, Input Length 18: 122.943 ms\n",
      "Model: Base Quantized, Input Length 10: 100.405 ms\n",
      "Model: Base Quantized, Input Length 16: 116.746 ms\n",
      "Model: Base Quantized, Input Length 10: 120.829 ms\n",
      "Model: Base Quantized, Input Length 22: 99.339 ms\n",
      "Model: Base Quantized, Input Length 14: 75.384 ms\n",
      "Model: Base Quantized, Input Length 19: 92.612 ms\n",
      "Model: Base Quantized, Input Length 20: 83.915 ms\n",
      "Model: Base Quantized, Input Length 20: 94.491 ms\n",
      "Model: Base Quantized, Input Length 18: 91.216 ms\n",
      "Model: Base Quantized, Input Length 18: 91.238 ms\n",
      "Model: Base Quantized, Input Length 10: 71.523 ms\n",
      "Model: Base Quantized, Input Length 16: 89.585 ms\n",
      "Model: Base Quantized, Input Length 10: 117.267 ms\n",
      "Model: Base Quantized, Input Length 22: 106.154 ms\n",
      "Model: Base Quantized, Input Length 14: 90.722 ms\n",
      "Model: Base Quantized, Input Length 19: 101.522 ms\n",
      "Model: Base Quantized, Input Length 20: 95.217 ms\n",
      "Model: Base Quantized, Input Length 20: 115.747 ms\n",
      "Model: Base Quantized, Input Length 18: 110.844 ms\n",
      "Model: Base Quantized, Input Length 18: 94.456 ms\n",
      "Model: Base Quantized, Input Length 10: 75.615 ms\n",
      "Model: Base Quantized, Input Length 16: 87.516 ms\n",
      "Model: Base Quantized, Input Length 10: 105.092 ms\n",
      "Model: Base Quantized, Input Length 22: 104.497 ms\n",
      "Model: Base Quantized, Input Length 14: 80.340 ms\n",
      "Model: Base Quantized, Input Length 19: 83.538 ms\n",
      "Model: Base Quantized, Input Length 20: 73.353 ms\n",
      "Model: Base Quantized, Input Length 20: 82.581 ms\n",
      "Model: Base Quantized, Input Length 18: 87.711 ms\n",
      "Model: Base Quantized, Input Length 18: 98.103 ms\n",
      "Model: Base Quantized, Input Length 10: 99.976 ms\n",
      "Model: Base Quantized, Input Length 16: 121.157 ms\n",
      "Model: Base Quantized, Input Length 10: 130.205 ms\n",
      "Model: Base Quantized, Input Length 22: 112.485 ms\n",
      "Model: Base Quantized, Input Length 14: 82.929 ms\n",
      "Model: ONNX, Input Length 19: 55.776 ms\n",
      "Model: ONNX, Input Length 20: 46.768 ms\n",
      "Model: ONNX, Input Length 20: 47.934 ms\n",
      "Model: ONNX, Input Length 18: 48.828 ms\n",
      "Model: ONNX, Input Length 18: 57.603 ms\n",
      "Model: ONNX, Input Length 10: 56.912 ms\n",
      "Model: ONNX, Input Length 16: 74.149 ms\n",
      "Model: ONNX, Input Length 10: 82.782 ms\n",
      "Model: ONNX, Input Length 22: 61.573 ms\n",
      "Model: ONNX, Input Length 14: 50.919 ms\n",
      "Model: ONNX, Input Length 19: 66.337 ms\n",
      "Model: ONNX, Input Length 20: 55.325 ms\n",
      "Model: ONNX, Input Length 20: 58.604 ms\n",
      "Model: ONNX, Input Length 18: 60.595 ms\n",
      "Model: ONNX, Input Length 18: 63.098 ms\n",
      "Model: ONNX, Input Length 10: 52.206 ms\n",
      "Model: ONNX, Input Length 16: 56.376 ms\n",
      "Model: ONNX, Input Length 10: 68.824 ms\n",
      "Model: ONNX, Input Length 22: 60.796 ms\n",
      "Model: ONNX, Input Length 14: 48.355 ms\n",
      "Model: ONNX, Input Length 19: 59.538 ms\n",
      "Model: ONNX, Input Length 20: 57.288 ms\n",
      "Model: ONNX, Input Length 20: 57.090 ms\n",
      "Model: ONNX, Input Length 18: 54.237 ms\n",
      "Model: ONNX, Input Length 18: 60.465 ms\n",
      "Model: ONNX, Input Length 10: 54.959 ms\n",
      "Model: ONNX, Input Length 16: 53.107 ms\n",
      "Model: ONNX, Input Length 10: 68.714 ms\n",
      "Model: ONNX, Input Length 22: 63.190 ms\n",
      "Model: ONNX, Input Length 14: 54.057 ms\n",
      "Model: ONNX, Input Length 19: 72.705 ms\n",
      "Model: ONNX, Input Length 20: 61.749 ms\n",
      "Model: ONNX, Input Length 20: 69.929 ms\n",
      "Model: ONNX, Input Length 18: 72.189 ms\n",
      "Model: ONNX, Input Length 18: 73.781 ms\n",
      "Model: ONNX, Input Length 10: 61.060 ms\n",
      "Model: ONNX, Input Length 16: 71.529 ms\n",
      "Model: ONNX, Input Length 10: 80.541 ms\n",
      "Model: ONNX, Input Length 22: 61.383 ms\n",
      "Model: ONNX, Input Length 14: 61.838 ms\n",
      "Model: ONNX, Input Length 19: 68.670 ms\n",
      "Model: ONNX, Input Length 20: 60.517 ms\n",
      "Model: ONNX, Input Length 20: 64.473 ms\n",
      "Model: ONNX, Input Length 18: 57.630 ms\n",
      "Model: ONNX, Input Length 18: 62.728 ms\n",
      "Model: ONNX, Input Length 10: 55.286 ms\n",
      "Model: ONNX, Input Length 16: 74.716 ms\n",
      "Model: ONNX, Input Length 10: 83.739 ms\n",
      "Model: ONNX, Input Length 22: 73.987 ms\n",
      "Model: ONNX, Input Length 14: 55.301 ms\n",
      "Model: ONNX-OPT, Input Length 19: 70.088 ms\n",
      "Model: ONNX-OPT, Input Length 20: 61.561 ms\n",
      "Model: ONNX-OPT, Input Length 20: 68.443 ms\n",
      "Model: ONNX-OPT, Input Length 18: 69.640 ms\n",
      "Model: ONNX-OPT, Input Length 18: 81.140 ms\n",
      "Model: ONNX-OPT, Input Length 10: 68.277 ms\n",
      "Model: ONNX-OPT, Input Length 16: 70.542 ms\n",
      "Model: ONNX-OPT, Input Length 10: 88.646 ms\n",
      "Model: ONNX-OPT, Input Length 22: 71.253 ms\n",
      "Model: ONNX-OPT, Input Length 14: 65.871 ms\n",
      "Model: ONNX-OPT, Input Length 19: 63.952 ms\n",
      "Model: ONNX-OPT, Input Length 20: 54.754 ms\n",
      "Model: ONNX-OPT, Input Length 20: 68.526 ms\n",
      "Model: ONNX-OPT, Input Length 18: 68.932 ms\n",
      "Model: ONNX-OPT, Input Length 18: 80.279 ms\n",
      "Model: ONNX-OPT, Input Length 10: 66.045 ms\n",
      "Model: ONNX-OPT, Input Length 16: 76.501 ms\n",
      "Model: ONNX-OPT, Input Length 10: 87.001 ms\n",
      "Model: ONNX-OPT, Input Length 22: 63.436 ms\n",
      "Model: ONNX-OPT, Input Length 14: 49.210 ms\n",
      "Model: ONNX-OPT, Input Length 19: 56.320 ms\n",
      "Model: ONNX-OPT, Input Length 20: 48.648 ms\n",
      "Model: ONNX-OPT, Input Length 20: 53.194 ms\n",
      "Model: ONNX-OPT, Input Length 18: 53.492 ms\n",
      "Model: ONNX-OPT, Input Length 18: 59.179 ms\n",
      "Model: ONNX-OPT, Input Length 10: 52.565 ms\n",
      "Model: ONNX-OPT, Input Length 16: 58.214 ms\n",
      "Model: ONNX-OPT, Input Length 10: 69.904 ms\n",
      "Model: ONNX-OPT, Input Length 22: 58.513 ms\n",
      "Model: ONNX-OPT, Input Length 14: 51.979 ms\n",
      "Model: ONNX-OPT, Input Length 19: 73.627 ms\n",
      "Model: ONNX-OPT, Input Length 20: 60.748 ms\n",
      "Model: ONNX-OPT, Input Length 20: 69.927 ms\n",
      "Model: ONNX-OPT, Input Length 18: 67.614 ms\n",
      "Model: ONNX-OPT, Input Length 18: 77.581 ms\n",
      "Model: ONNX-OPT, Input Length 10: 57.410 ms\n",
      "Model: ONNX-OPT, Input Length 16: 71.881 ms\n",
      "Model: ONNX-OPT, Input Length 10: 89.670 ms\n",
      "Model: ONNX-OPT, Input Length 22: 75.084 ms\n",
      "Model: ONNX-OPT, Input Length 14: 64.839 ms\n",
      "Model: ONNX-OPT, Input Length 19: 77.932 ms\n",
      "Model: ONNX-OPT, Input Length 20: 65.694 ms\n",
      "Model: ONNX-OPT, Input Length 20: 72.283 ms\n",
      "Model: ONNX-OPT, Input Length 18: 78.548 ms\n",
      "Model: ONNX-OPT, Input Length 18: 87.205 ms\n",
      "Model: ONNX-OPT, Input Length 10: 68.292 ms\n",
      "Model: ONNX-OPT, Input Length 16: 77.952 ms\n",
      "Model: ONNX-OPT, Input Length 10: 85.767 ms\n",
      "Model: ONNX-OPT, Input Length 22: 72.473 ms\n",
      "Model: ONNX-OPT, Input Length 14: 66.779 ms\n",
      "Model: ONNX Quantized, Input Length 19: 49.736 ms\n",
      "Model: ONNX Quantized, Input Length 20: 42.533 ms\n",
      "Model: ONNX Quantized, Input Length 20: 47.791 ms\n",
      "Model: ONNX Quantized, Input Length 18: 52.889 ms\n",
      "Model: ONNX Quantized, Input Length 18: 59.809 ms\n",
      "Model: ONNX Quantized, Input Length 10: 49.381 ms\n",
      "Model: ONNX Quantized, Input Length 16: 59.104 ms\n",
      "Model: ONNX Quantized, Input Length 10: 70.578 ms\n",
      "Model: ONNX Quantized, Input Length 22: 53.504 ms\n",
      "Model: ONNX Quantized, Input Length 14: 50.339 ms\n",
      "Model: ONNX Quantized, Input Length 19: 55.201 ms\n",
      "Model: ONNX Quantized, Input Length 20: 49.282 ms\n",
      "Model: ONNX Quantized, Input Length 20: 53.843 ms\n",
      "Model: ONNX Quantized, Input Length 18: 51.800 ms\n",
      "Model: ONNX Quantized, Input Length 18: 60.414 ms\n",
      "Model: ONNX Quantized, Input Length 10: 54.501 ms\n",
      "Model: ONNX Quantized, Input Length 16: 60.920 ms\n",
      "Model: ONNX Quantized, Input Length 10: 72.603 ms\n",
      "Model: ONNX Quantized, Input Length 22: 58.382 ms\n",
      "Model: ONNX Quantized, Input Length 14: 48.648 ms\n",
      "Model: ONNX Quantized, Input Length 19: 49.751 ms\n",
      "Model: ONNX Quantized, Input Length 20: 49.509 ms\n",
      "Model: ONNX Quantized, Input Length 20: 53.583 ms\n",
      "Model: ONNX Quantized, Input Length 18: 46.685 ms\n",
      "Model: ONNX Quantized, Input Length 18: 56.853 ms\n",
      "Model: ONNX Quantized, Input Length 10: 50.050 ms\n",
      "Model: ONNX Quantized, Input Length 16: 57.830 ms\n",
      "Model: ONNX Quantized, Input Length 10: 70.389 ms\n",
      "Model: ONNX Quantized, Input Length 22: 56.329 ms\n",
      "Model: ONNX Quantized, Input Length 14: 47.829 ms\n",
      "Model: ONNX Quantized, Input Length 19: 49.779 ms\n",
      "Model: ONNX Quantized, Input Length 20: 44.952 ms\n",
      "Model: ONNX Quantized, Input Length 20: 48.625 ms\n",
      "Model: ONNX Quantized, Input Length 18: 51.045 ms\n",
      "Model: ONNX Quantized, Input Length 18: 55.520 ms\n",
      "Model: ONNX Quantized, Input Length 10: 48.262 ms\n",
      "Model: ONNX Quantized, Input Length 16: 44.413 ms\n",
      "Model: ONNX Quantized, Input Length 10: 58.915 ms\n",
      "Model: ONNX Quantized, Input Length 22: 46.426 ms\n",
      "Model: ONNX Quantized, Input Length 14: 45.243 ms\n",
      "Model: ONNX Quantized, Input Length 19: 55.751 ms\n",
      "Model: ONNX Quantized, Input Length 20: 49.553 ms\n",
      "Model: ONNX Quantized, Input Length 20: 53.430 ms\n",
      "Model: ONNX Quantized, Input Length 18: 53.717 ms\n",
      "Model: ONNX Quantized, Input Length 18: 59.734 ms\n",
      "Model: ONNX Quantized, Input Length 10: 50.274 ms\n",
      "Model: ONNX Quantized, Input Length 16: 49.425 ms\n",
      "Model: ONNX Quantized, Input Length 10: 64.528 ms\n",
      "Model: ONNX Quantized, Input Length 22: 51.901 ms\n",
      "Model: ONNX Quantized, Input Length 14: 45.240 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 19: 45.055 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 20: 45.991 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 20: 45.698 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 18: 48.705 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 18: 56.897 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 10: 49.878 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 16: 60.140 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 10: 68.660 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 22: 58.212 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 14: 50.358 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 19: 50.965 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 20: 45.228 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 20: 49.207 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 18: 49.576 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 18: 58.835 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 10: 48.570 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 16: 49.677 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 10: 60.502 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 22: 46.377 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 14: 41.080 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 19: 50.429 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 20: 48.047 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 20: 48.385 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 18: 50.481 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 18: 64.393 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 10: 52.187 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 16: 59.637 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 10: 69.136 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 22: 59.267 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 14: 48.682 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 19: 56.362 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 20: 47.926 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 20: 55.863 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 18: 57.078 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 18: 61.771 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 10: 51.490 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 16: 49.022 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 10: 66.571 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 22: 51.417 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 14: 45.296 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 19: 52.072 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 20: 45.686 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 20: 52.097 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 18: 51.465 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 18: 61.359 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 10: 49.318 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 16: 58.322 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 10: 70.636 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 22: 50.721 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 14: 45.104 ms\n",
      "Loading: bert-base-uncased race\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset race (/Users/michaelhermann/.cache/huggingface/datasets/race/all/0.1.0/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: race\n",
      "Preped data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 2838.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Base, Input Length 339: 1192.867 ms\n",
      "Model: Base, Input Length 433: 1494.180 ms\n",
      "Model: Base, Input Length 385: 1505.298 ms\n",
      "Model: Base, Input Length 317: 1056.481 ms\n",
      "Model: Base, Input Length 327: 1145.822 ms\n",
      "Model: Base, Input Length 583: 1600.533 ms\n",
      "Model: Base, Input Length 254: 921.250 ms\n",
      "Model: Base, Input Length 347: 1616.987 ms\n",
      "Model: Base, Input Length 305: 1182.015 ms\n",
      "Model: Base, Input Length 278: 1176.651 ms\n",
      "Model: Base, Input Length 339: 1595.228 ms\n",
      "Model: Base, Input Length 433: 1808.810 ms\n",
      "Model: Base, Input Length 385: 1794.946 ms\n",
      "Model: Base, Input Length 317: 1259.386 ms\n",
      "Model: Base, Input Length 327: 1352.276 ms\n",
      "Model: Base, Input Length 583: 2050.027 ms\n",
      "Model: Base, Input Length 254: 1084.414 ms\n",
      "Model: Base, Input Length 347: 1815.329 ms\n",
      "Model: Base, Input Length 305: 1223.675 ms\n",
      "Model: Base, Input Length 278: 1237.223 ms\n",
      "Model: Base, Input Length 339: 1690.683 ms\n",
      "Model: Base, Input Length 433: 2092.458 ms\n",
      "Model: Base, Input Length 385: 2091.062 ms\n",
      "Model: Base, Input Length 317: 1847.992 ms\n",
      "Model: Base, Input Length 327: 1656.005 ms\n",
      "Model: Base, Input Length 583: 1865.074 ms\n",
      "Model: Base, Input Length 254: 1023.141 ms\n",
      "Model: Base, Input Length 347: 1993.591 ms\n",
      "Model: Base, Input Length 305: 1550.725 ms\n",
      "Model: Base, Input Length 278: 1592.798 ms\n",
      "Model: Base, Input Length 339: 1862.984 ms\n",
      "Model: Base, Input Length 433: 2307.450 ms\n",
      "Model: Base, Input Length 385: 2133.295 ms\n",
      "Model: Base, Input Length 317: 1609.594 ms\n",
      "Model: Base, Input Length 327: 1442.248 ms\n",
      "Model: Base, Input Length 583: 2026.501 ms\n",
      "Model: Base, Input Length 254: 1336.108 ms\n",
      "Model: Base, Input Length 347: 2031.389 ms\n",
      "Model: Base, Input Length 305: 1589.763 ms\n",
      "Model: Base, Input Length 278: 1541.186 ms\n",
      "Model: Base, Input Length 339: 1872.038 ms\n",
      "Model: Base, Input Length 433: 2339.223 ms\n",
      "Model: Base, Input Length 385: 2231.499 ms\n",
      "Model: Base, Input Length 317: 1667.549 ms\n",
      "Model: Base, Input Length 327: 1571.384 ms\n",
      "Model: Base, Input Length 583: 2004.007 ms\n",
      "Model: Base, Input Length 254: 990.622 ms\n",
      "Model: Base, Input Length 347: 1638.350 ms\n",
      "Model: Base, Input Length 305: 1144.690 ms\n",
      "Model: Base, Input Length 278: 1181.992 ms\n",
      "Model: Base Quantized, Input Length 339: 1516.780 ms\n",
      "Model: Base Quantized, Input Length 433: 1712.347 ms\n",
      "Model: Base Quantized, Input Length 385: 1692.499 ms\n",
      "Model: Base Quantized, Input Length 317: 1255.179 ms\n",
      "Model: Base Quantized, Input Length 327: 1281.207 ms\n",
      "Model: Base Quantized, Input Length 583: 1699.106 ms\n",
      "Model: Base Quantized, Input Length 254: 1020.962 ms\n",
      "Model: Base Quantized, Input Length 347: 1540.984 ms\n",
      "Model: Base Quantized, Input Length 305: 1049.864 ms\n",
      "Model: Base Quantized, Input Length 278: 1044.646 ms\n",
      "Model: Base Quantized, Input Length 339: 1520.720 ms\n",
      "Model: Base Quantized, Input Length 433: 1655.685 ms\n",
      "Model: Base Quantized, Input Length 385: 1575.682 ms\n",
      "Model: Base Quantized, Input Length 317: 1150.858 ms\n",
      "Model: Base Quantized, Input Length 327: 1279.609 ms\n",
      "Model: Base Quantized, Input Length 583: 1649.079 ms\n",
      "Model: Base Quantized, Input Length 254: 893.086 ms\n",
      "Model: Base Quantized, Input Length 347: 1523.739 ms\n",
      "Model: Base Quantized, Input Length 305: 1030.660 ms\n",
      "Model: Base Quantized, Input Length 278: 1066.044 ms\n",
      "Model: Base Quantized, Input Length 339: 1434.115 ms\n",
      "Model: Base Quantized, Input Length 433: 1666.587 ms\n",
      "Model: Base Quantized, Input Length 385: 1583.474 ms\n",
      "Model: Base Quantized, Input Length 317: 1224.656 ms\n",
      "Model: Base Quantized, Input Length 327: 1389.221 ms\n",
      "Model: Base Quantized, Input Length 583: 1858.057 ms\n",
      "Model: Base Quantized, Input Length 254: 1055.298 ms\n",
      "Model: Base Quantized, Input Length 347: 1825.716 ms\n",
      "Model: Base Quantized, Input Length 305: 1100.694 ms\n",
      "Model: Base Quantized, Input Length 278: 1159.393 ms\n",
      "Model: Base Quantized, Input Length 339: 1566.732 ms\n",
      "Model: Base Quantized, Input Length 433: 1818.403 ms\n",
      "Model: Base Quantized, Input Length 385: 2055.139 ms\n",
      "Model: Base Quantized, Input Length 317: 1424.375 ms\n",
      "Model: Base Quantized, Input Length 327: 1443.733 ms\n",
      "Model: Base Quantized, Input Length 583: 1876.992 ms\n",
      "Model: Base Quantized, Input Length 254: 973.024 ms\n",
      "Model: Base Quantized, Input Length 347: 1645.731 ms\n",
      "Model: Base Quantized, Input Length 305: 1164.190 ms\n",
      "Model: Base Quantized, Input Length 278: 1201.158 ms\n",
      "Model: Base Quantized, Input Length 339: 1575.011 ms\n",
      "Model: Base Quantized, Input Length 433: 1842.244 ms\n",
      "Model: Base Quantized, Input Length 385: 1770.044 ms\n",
      "Model: Base Quantized, Input Length 317: 1373.871 ms\n",
      "Model: Base Quantized, Input Length 327: 1354.650 ms\n",
      "Model: Base Quantized, Input Length 583: 1810.300 ms\n",
      "Model: Base Quantized, Input Length 254: 965.733 ms\n",
      "Model: Base Quantized, Input Length 347: 1643.794 ms\n",
      "Model: Base Quantized, Input Length 305: 1096.338 ms\n",
      "Model: Base Quantized, Input Length 278: 1145.357 ms\n",
      "Model: ONNX, Input Length 339: 805.228 ms\n",
      "Model: ONNX, Input Length 433: 1005.900 ms\n",
      "Model: ONNX, Input Length 385: 963.749 ms\n",
      "Model: ONNX, Input Length 317: 744.140 ms\n",
      "Model: ONNX, Input Length 327: 808.583 ms\n",
      "Model: ONNX, Input Length 583: 1041.391 ms\n",
      "Model: ONNX, Input Length 254: 608.289 ms\n",
      "Model: ONNX, Input Length 347: 972.523 ms\n",
      "Model: ONNX, Input Length 305: 736.827 ms\n",
      "Model: ONNX, Input Length 278: 770.168 ms\n",
      "Model: ONNX, Input Length 339: 984.791 ms\n",
      "Model: ONNX, Input Length 433: 1242.122 ms\n",
      "Model: ONNX, Input Length 385: 1099.958 ms\n",
      "Model: ONNX, Input Length 317: 1023.792 ms\n",
      "Model: ONNX, Input Length 327: 966.577 ms\n",
      "Model: ONNX, Input Length 583: 1237.793 ms\n",
      "Model: ONNX, Input Length 254: 890.170 ms\n",
      "Model: ONNX, Input Length 347: 1435.255 ms\n",
      "Model: ONNX, Input Length 305: 1102.506 ms\n",
      "Model: ONNX, Input Length 278: 1141.188 ms\n",
      "Model: ONNX, Input Length 339: 1439.033 ms\n",
      "Model: ONNX, Input Length 433: 1442.688 ms\n",
      "Model: ONNX, Input Length 385: 1187.058 ms\n",
      "Model: ONNX, Input Length 317: 951.433 ms\n",
      "Model: ONNX, Input Length 327: 1130.113 ms\n",
      "Model: ONNX, Input Length 583: 1539.484 ms\n",
      "Model: ONNX, Input Length 254: 941.384 ms\n",
      "Model: ONNX, Input Length 347: 1506.946 ms\n",
      "Model: ONNX, Input Length 305: 1110.091 ms\n",
      "Model: ONNX, Input Length 278: 1141.982 ms\n",
      "Model: ONNX, Input Length 339: 1403.348 ms\n",
      "Model: ONNX, Input Length 433: 1577.546 ms\n",
      "Model: ONNX, Input Length 385: 1453.663 ms\n",
      "Model: ONNX, Input Length 317: 1029.150 ms\n",
      "Model: ONNX, Input Length 327: 1051.138 ms\n",
      "Model: ONNX, Input Length 583: 1381.469 ms\n",
      "Model: ONNX, Input Length 254: 825.906 ms\n",
      "Model: ONNX, Input Length 347: 1252.645 ms\n",
      "Model: ONNX, Input Length 305: 826.659 ms\n",
      "Model: ONNX, Input Length 278: 898.490 ms\n",
      "Model: ONNX, Input Length 339: 1114.941 ms\n",
      "Model: ONNX, Input Length 433: 1269.830 ms\n",
      "Model: ONNX, Input Length 385: 1155.720 ms\n",
      "Model: ONNX, Input Length 317: 939.851 ms\n",
      "Model: ONNX, Input Length 327: 950.877 ms\n",
      "Model: ONNX, Input Length 583: 1378.982 ms\n",
      "Model: ONNX, Input Length 254: 765.853 ms\n",
      "Model: ONNX, Input Length 347: 1293.291 ms\n",
      "Model: ONNX, Input Length 305: 885.815 ms\n",
      "Model: ONNX, Input Length 278: 1069.837 ms\n",
      "Model: ONNX-OPT, Input Length 339: 1452.236 ms\n",
      "Model: ONNX-OPT, Input Length 433: 1601.752 ms\n",
      "Model: ONNX-OPT, Input Length 385: 1350.078 ms\n",
      "Model: ONNX-OPT, Input Length 317: 1048.060 ms\n",
      "Model: ONNX-OPT, Input Length 327: 1069.210 ms\n",
      "Model: ONNX-OPT, Input Length 583: 1321.292 ms\n",
      "Model: ONNX-OPT, Input Length 254: 793.970 ms\n",
      "Model: ONNX-OPT, Input Length 347: 1152.244 ms\n",
      "Model: ONNX-OPT, Input Length 305: 876.921 ms\n",
      "Model: ONNX-OPT, Input Length 278: 839.518 ms\n",
      "Model: ONNX-OPT, Input Length 339: 1246.412 ms\n",
      "Model: ONNX-OPT, Input Length 433: 1403.603 ms\n",
      "Model: ONNX-OPT, Input Length 385: 1350.907 ms\n",
      "Model: ONNX-OPT, Input Length 317: 924.965 ms\n",
      "Model: ONNX-OPT, Input Length 327: 1061.625 ms\n",
      "Model: ONNX-OPT, Input Length 583: 1362.046 ms\n",
      "Model: ONNX-OPT, Input Length 254: 759.607 ms\n",
      "Model: ONNX-OPT, Input Length 347: 1378.179 ms\n",
      "Model: ONNX-OPT, Input Length 305: 928.811 ms\n",
      "Model: ONNX-OPT, Input Length 278: 893.248 ms\n",
      "Model: ONNX-OPT, Input Length 339: 1154.534 ms\n",
      "Model: ONNX-OPT, Input Length 433: 1431.415 ms\n",
      "Model: ONNX-OPT, Input Length 385: 1404.288 ms\n",
      "Model: ONNX-OPT, Input Length 317: 939.945 ms\n",
      "Model: ONNX-OPT, Input Length 327: 1142.340 ms\n",
      "Model: ONNX-OPT, Input Length 583: 1366.801 ms\n",
      "Model: ONNX-OPT, Input Length 254: 814.134 ms\n",
      "Model: ONNX-OPT, Input Length 347: 1219.410 ms\n",
      "Model: ONNX-OPT, Input Length 305: 1004.038 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1003.990 ms\n",
      "Model: ONNX-OPT, Input Length 339: 1221.224 ms\n",
      "Model: ONNX-OPT, Input Length 433: 1517.772 ms\n",
      "Model: ONNX-OPT, Input Length 385: 1435.858 ms\n",
      "Model: ONNX-OPT, Input Length 317: 1053.474 ms\n",
      "Model: ONNX-OPT, Input Length 327: 1097.817 ms\n",
      "Model: ONNX-OPT, Input Length 583: 1355.347 ms\n",
      "Model: ONNX-OPT, Input Length 254: 861.018 ms\n",
      "Model: ONNX-OPT, Input Length 347: 1183.413 ms\n",
      "Model: ONNX-OPT, Input Length 305: 998.379 ms\n",
      "Model: ONNX-OPT, Input Length 278: 1087.469 ms\n",
      "Model: ONNX-OPT, Input Length 339: 1186.595 ms\n",
      "Model: ONNX-OPT, Input Length 433: 1522.493 ms\n",
      "Model: ONNX-OPT, Input Length 385: 1556.346 ms\n",
      "Model: ONNX-OPT, Input Length 317: 1171.793 ms\n",
      "Model: ONNX-OPT, Input Length 327: 1071.232 ms\n",
      "Model: ONNX-OPT, Input Length 583: 1275.947 ms\n",
      "Model: ONNX-OPT, Input Length 254: 721.922 ms\n",
      "Model: ONNX-OPT, Input Length 347: 1209.380 ms\n",
      "Model: ONNX-OPT, Input Length 305: 874.741 ms\n",
      "Model: ONNX-OPT, Input Length 278: 887.507 ms\n",
      "Model: ONNX Quantized, Input Length 339: 970.301 ms\n",
      "Model: ONNX Quantized, Input Length 433: 1030.272 ms\n",
      "Model: ONNX Quantized, Input Length 385: 946.087 ms\n",
      "Model: ONNX Quantized, Input Length 317: 709.861 ms\n",
      "Model: ONNX Quantized, Input Length 327: 769.326 ms\n",
      "Model: ONNX Quantized, Input Length 583: 1001.138 ms\n",
      "Model: ONNX Quantized, Input Length 254: 563.572 ms\n",
      "Model: ONNX Quantized, Input Length 347: 901.445 ms\n",
      "Model: ONNX Quantized, Input Length 305: 637.191 ms\n",
      "Model: ONNX Quantized, Input Length 278: 652.218 ms\n",
      "Model: ONNX Quantized, Input Length 339: 907.208 ms\n",
      "Model: ONNX Quantized, Input Length 433: 1047.882 ms\n",
      "Model: ONNX Quantized, Input Length 385: 1051.452 ms\n",
      "Model: ONNX Quantized, Input Length 317: 726.495 ms\n",
      "Model: ONNX Quantized, Input Length 327: 730.829 ms\n",
      "Model: ONNX Quantized, Input Length 583: 1160.794 ms\n",
      "Model: ONNX Quantized, Input Length 254: 657.705 ms\n",
      "Model: ONNX Quantized, Input Length 347: 958.484 ms\n",
      "Model: ONNX Quantized, Input Length 305: 692.378 ms\n",
      "Model: ONNX Quantized, Input Length 278: 674.681 ms\n",
      "Model: ONNX Quantized, Input Length 339: 903.130 ms\n",
      "Model: ONNX Quantized, Input Length 433: 1066.647 ms\n",
      "Model: ONNX Quantized, Input Length 385: 1051.056 ms\n",
      "Model: ONNX Quantized, Input Length 317: 851.047 ms\n",
      "Model: ONNX Quantized, Input Length 327: 822.823 ms\n",
      "Model: ONNX Quantized, Input Length 583: 1103.665 ms\n",
      "Model: ONNX Quantized, Input Length 254: 625.018 ms\n",
      "Model: ONNX Quantized, Input Length 347: 1017.062 ms\n",
      "Model: ONNX Quantized, Input Length 305: 802.997 ms\n",
      "Model: ONNX Quantized, Input Length 278: 696.234 ms\n",
      "Model: ONNX Quantized, Input Length 339: 930.013 ms\n",
      "Model: ONNX Quantized, Input Length 433: 1075.930 ms\n",
      "Model: ONNX Quantized, Input Length 385: 1029.854 ms\n",
      "Model: ONNX Quantized, Input Length 317: 701.736 ms\n",
      "Model: ONNX Quantized, Input Length 327: 760.307 ms\n",
      "Model: ONNX Quantized, Input Length 583: 1034.315 ms\n",
      "Model: ONNX Quantized, Input Length 254: 567.830 ms\n",
      "Model: ONNX Quantized, Input Length 347: 862.342 ms\n",
      "Model: ONNX Quantized, Input Length 305: 609.970 ms\n",
      "Model: ONNX Quantized, Input Length 278: 612.449 ms\n",
      "Model: ONNX Quantized, Input Length 339: 933.223 ms\n",
      "Model: ONNX Quantized, Input Length 433: 1068.622 ms\n",
      "Model: ONNX Quantized, Input Length 385: 1016.237 ms\n",
      "Model: ONNX Quantized, Input Length 317: 723.194 ms\n",
      "Model: ONNX Quantized, Input Length 327: 770.605 ms\n",
      "Model: ONNX Quantized, Input Length 583: 999.172 ms\n",
      "Model: ONNX Quantized, Input Length 254: 587.527 ms\n",
      "Model: ONNX Quantized, Input Length 347: 879.497 ms\n",
      "Model: ONNX Quantized, Input Length 305: 640.417 ms\n",
      "Model: ONNX Quantized, Input Length 278: 610.781 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 339: 881.830 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 433: 1056.086 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 385: 1010.541 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 317: 849.053 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 327: 941.996 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 583: 1089.020 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 254: 718.623 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 347: 916.156 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 305: 605.759 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 648.549 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 339: 918.466 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 433: 1050.462 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 385: 1139.004 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 317: 826.059 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 327: 906.315 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 583: 1125.577 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 254: 648.754 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 347: 936.625 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 305: 657.790 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 780.186 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 339: 1000.054 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 433: 1132.951 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 385: 1078.955 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 317: 721.979 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 327: 773.263 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 583: 1021.907 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 254: 576.817 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 347: 992.812 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 305: 663.212 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 641.934 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 339: 922.470 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 433: 1104.677 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 385: 971.637 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 317: 789.556 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 327: 883.420 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 583: 1008.924 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 254: 582.617 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 347: 1000.767 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 305: 668.635 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 670.203 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 339: 845.557 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 433: 1033.717 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 385: 922.932 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 317: 750.314 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 327: 744.787 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 583: 1075.334 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 254: 561.793 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 347: 953.486 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 305: 795.375 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 718.067 ms\n",
      "Loading: roberta-base race\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset race (/Users/michaelhermann/.cache/huggingface/datasets/race/all/0.1.0/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: race\n",
      "Preped data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 2498.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Base, Input Length 339: 1136.817 ms\n",
      "Model: Base, Input Length 433: 1446.437 ms\n",
      "Model: Base, Input Length 385: 1377.089 ms\n",
      "Model: Base, Input Length 317: 1141.790 ms\n",
      "Model: Base, Input Length 327: 1317.540 ms\n",
      "Model: Base, Input Length 583: 1690.949 ms\n",
      "Model: Base, Input Length 254: 987.686 ms\n",
      "Model: Base, Input Length 347: 1624.416 ms\n",
      "Model: Base, Input Length 305: 1117.442 ms\n",
      "Model: Base, Input Length 278: 1201.291 ms\n",
      "Model: Base, Input Length 339: 1518.966 ms\n",
      "Model: Base, Input Length 433: 1798.779 ms\n",
      "Model: Base, Input Length 385: 1682.984 ms\n",
      "Model: Base, Input Length 317: 1285.956 ms\n",
      "Model: Base, Input Length 327: 1483.394 ms\n",
      "Model: Base, Input Length 583: 1961.537 ms\n",
      "Model: Base, Input Length 254: 1140.689 ms\n",
      "Model: Base, Input Length 347: 1788.475 ms\n",
      "Model: Base, Input Length 305: 1460.456 ms\n",
      "Model: Base, Input Length 278: 1586.548 ms\n",
      "Model: Base, Input Length 339: 1839.404 ms\n",
      "Model: Base, Input Length 433: 2057.986 ms\n",
      "Model: Base, Input Length 385: 1830.118 ms\n",
      "Model: Base, Input Length 317: 1389.546 ms\n",
      "Model: Base, Input Length 327: 1553.181 ms\n",
      "Model: Base, Input Length 583: 1990.172 ms\n",
      "Model: Base, Input Length 254: 1085.388 ms\n",
      "Model: Base, Input Length 347: 1827.880 ms\n",
      "Model: Base, Input Length 305: 1392.247 ms\n",
      "Model: Base, Input Length 278: 1479.009 ms\n",
      "Model: Base, Input Length 339: 1527.214 ms\n",
      "Model: Base, Input Length 433: 1878.838 ms\n",
      "Model: Base, Input Length 385: 1757.494 ms\n",
      "Model: Base, Input Length 317: 1408.795 ms\n",
      "Model: Base, Input Length 327: 1342.986 ms\n",
      "Model: Base, Input Length 583: 2045.516 ms\n",
      "Model: Base, Input Length 254: 1396.207 ms\n",
      "Model: Base, Input Length 347: 1955.834 ms\n",
      "Model: Base, Input Length 305: 1458.005 ms\n",
      "Model: Base, Input Length 278: 1628.505 ms\n",
      "Model: Base, Input Length 339: 1734.013 ms\n",
      "Model: Base, Input Length 433: 1993.580 ms\n",
      "Model: Base, Input Length 385: 2057.019 ms\n",
      "Model: Base, Input Length 317: 1372.226 ms\n",
      "Model: Base, Input Length 327: 1441.586 ms\n",
      "Model: Base, Input Length 583: 1958.066 ms\n",
      "Model: Base, Input Length 254: 1104.160 ms\n",
      "Model: Base, Input Length 347: 2051.402 ms\n",
      "Model: Base, Input Length 305: 1320.606 ms\n",
      "Model: Base, Input Length 278: 1296.406 ms\n",
      "Model: Base Quantized, Input Length 339: 1694.939 ms\n",
      "Model: Base Quantized, Input Length 433: 2183.237 ms\n",
      "Model: Base Quantized, Input Length 385: 1935.470 ms\n",
      "Model: Base Quantized, Input Length 317: 1506.568 ms\n",
      "Model: Base Quantized, Input Length 327: 1529.632 ms\n",
      "Model: Base Quantized, Input Length 583: 1861.495 ms\n",
      "Model: Base Quantized, Input Length 254: 1062.408 ms\n",
      "Model: Base Quantized, Input Length 347: 1757.237 ms\n",
      "Model: Base Quantized, Input Length 305: 1173.298 ms\n",
      "Model: Base Quantized, Input Length 278: 1395.383 ms\n",
      "Model: Base Quantized, Input Length 339: 1783.091 ms\n",
      "Model: Base Quantized, Input Length 433: 2308.530 ms\n",
      "Model: Base Quantized, Input Length 385: 2001.612 ms\n",
      "Model: Base Quantized, Input Length 317: 1522.279 ms\n",
      "Model: Base Quantized, Input Length 327: 1670.716 ms\n",
      "Model: Base Quantized, Input Length 583: 2316.757 ms\n",
      "Model: Base Quantized, Input Length 254: 971.860 ms\n",
      "Model: Base Quantized, Input Length 347: 1711.337 ms\n",
      "Model: Base Quantized, Input Length 305: 1282.863 ms\n",
      "Model: Base Quantized, Input Length 278: 1373.677 ms\n",
      "Model: Base Quantized, Input Length 339: 1648.205 ms\n",
      "Model: Base Quantized, Input Length 433: 1978.439 ms\n",
      "Model: Base Quantized, Input Length 385: 1756.408 ms\n",
      "Model: Base Quantized, Input Length 317: 1336.238 ms\n",
      "Model: Base Quantized, Input Length 327: 1480.976 ms\n",
      "Model: Base Quantized, Input Length 583: 2148.423 ms\n",
      "Model: Base Quantized, Input Length 254: 1042.856 ms\n",
      "Model: Base Quantized, Input Length 347: 1743.107 ms\n",
      "Model: Base Quantized, Input Length 305: 1351.623 ms\n",
      "Model: Base Quantized, Input Length 278: 1608.046 ms\n",
      "Model: Base Quantized, Input Length 339: 1933.591 ms\n",
      "Model: Base Quantized, Input Length 433: 2078.862 ms\n",
      "Model: Base Quantized, Input Length 385: 2030.548 ms\n",
      "Model: Base Quantized, Input Length 317: 1524.478 ms\n",
      "Model: Base Quantized, Input Length 327: 1590.378 ms\n",
      "Model: Base Quantized, Input Length 583: 2035.355 ms\n",
      "Model: Base Quantized, Input Length 254: 1315.614 ms\n",
      "Model: Base Quantized, Input Length 347: 1982.886 ms\n",
      "Model: Base Quantized, Input Length 305: 1281.726 ms\n",
      "Model: Base Quantized, Input Length 278: 1549.334 ms\n",
      "Model: Base Quantized, Input Length 339: 1643.727 ms\n",
      "Model: Base Quantized, Input Length 433: 1935.102 ms\n",
      "Model: Base Quantized, Input Length 385: 1785.936 ms\n",
      "Model: Base Quantized, Input Length 317: 1387.987 ms\n",
      "Model: Base Quantized, Input Length 327: 1369.651 ms\n",
      "Model: Base Quantized, Input Length 583: 1884.225 ms\n",
      "Model: Base Quantized, Input Length 254: 1029.628 ms\n",
      "Model: Base Quantized, Input Length 347: 1688.300 ms\n",
      "Model: Base Quantized, Input Length 305: 1559.585 ms\n",
      "Model: Base Quantized, Input Length 278: 1305.511 ms\n",
      "Model: ONNX, Input Length 339: 884.956 ms\n",
      "Model: ONNX, Input Length 433: 1218.635 ms\n",
      "Model: ONNX, Input Length 385: 1113.516 ms\n",
      "Model: ONNX, Input Length 317: 935.796 ms\n",
      "Model: ONNX, Input Length 327: 925.518 ms\n",
      "Model: ONNX, Input Length 583: 1175.944 ms\n",
      "Model: ONNX, Input Length 254: 670.340 ms\n",
      "Model: ONNX, Input Length 347: 1071.034 ms\n",
      "Model: ONNX, Input Length 305: 846.718 ms\n",
      "Model: ONNX, Input Length 278: 789.731 ms\n",
      "Model: ONNX, Input Length 339: 1009.497 ms\n",
      "Model: ONNX, Input Length 433: 1272.054 ms\n",
      "Model: ONNX, Input Length 385: 1304.764 ms\n",
      "Model: ONNX, Input Length 317: 923.500 ms\n",
      "Model: ONNX, Input Length 327: 973.683 ms\n",
      "Model: ONNX, Input Length 583: 1290.471 ms\n",
      "Model: ONNX, Input Length 254: 829.176 ms\n",
      "Model: ONNX, Input Length 347: 1199.040 ms\n",
      "Model: ONNX, Input Length 305: 864.651 ms\n",
      "Model: ONNX, Input Length 278: 839.314 ms\n",
      "Model: ONNX, Input Length 339: 1072.417 ms\n",
      "Model: ONNX, Input Length 433: 1358.528 ms\n",
      "Model: ONNX, Input Length 385: 1130.126 ms\n",
      "Model: ONNX, Input Length 317: 938.152 ms\n",
      "Model: ONNX, Input Length 327: 1011.560 ms\n",
      "Model: ONNX, Input Length 583: 1272.049 ms\n",
      "Model: ONNX, Input Length 254: 756.455 ms\n",
      "Model: ONNX, Input Length 347: 1148.310 ms\n",
      "Model: ONNX, Input Length 305: 867.388 ms\n",
      "Model: ONNX, Input Length 278: 889.109 ms\n",
      "Model: ONNX, Input Length 339: 1092.963 ms\n",
      "Model: ONNX, Input Length 433: 1428.583 ms\n",
      "Model: ONNX, Input Length 385: 1388.524 ms\n",
      "Model: ONNX, Input Length 317: 1001.556 ms\n",
      "Model: ONNX, Input Length 327: 1237.555 ms\n",
      "Model: ONNX, Input Length 583: 1740.771 ms\n",
      "Model: ONNX, Input Length 254: 829.786 ms\n",
      "Model: ONNX, Input Length 347: 1136.163 ms\n",
      "Model: ONNX, Input Length 305: 984.266 ms\n",
      "Model: ONNX, Input Length 278: 920.241 ms\n",
      "Model: ONNX, Input Length 339: 1084.796 ms\n",
      "Model: ONNX, Input Length 433: 1341.427 ms\n",
      "Model: ONNX, Input Length 385: 1519.649 ms\n",
      "Model: ONNX, Input Length 317: 1090.722 ms\n",
      "Model: ONNX, Input Length 327: 1175.504 ms\n",
      "Model: ONNX, Input Length 583: 1319.818 ms\n",
      "Model: ONNX, Input Length 254: 770.548 ms\n",
      "Model: ONNX, Input Length 347: 1099.539 ms\n",
      "Model: ONNX, Input Length 305: 835.878 ms\n",
      "Model: ONNX, Input Length 278: 854.735 ms\n",
      "Model: ONNX-OPT, Input Length 339: 1160.227 ms\n",
      "Model: ONNX-OPT, Input Length 433: 1295.783 ms\n",
      "Model: ONNX-OPT, Input Length 385: 1097.175 ms\n",
      "Model: ONNX-OPT, Input Length 317: 997.029 ms\n",
      "Model: ONNX-OPT, Input Length 327: 1028.421 ms\n",
      "Model: ONNX-OPT, Input Length 583: 1247.141 ms\n",
      "Model: ONNX-OPT, Input Length 254: 696.065 ms\n",
      "Model: ONNX-OPT, Input Length 347: 1076.513 ms\n",
      "Model: ONNX-OPT, Input Length 305: 837.474 ms\n",
      "Model: ONNX-OPT, Input Length 278: 838.335 ms\n",
      "Model: ONNX-OPT, Input Length 339: 1054.328 ms\n",
      "Model: ONNX-OPT, Input Length 433: 1217.165 ms\n",
      "Model: ONNX-OPT, Input Length 385: 1200.164 ms\n",
      "Model: ONNX-OPT, Input Length 317: 919.701 ms\n",
      "Model: ONNX-OPT, Input Length 327: 922.883 ms\n",
      "Model: ONNX-OPT, Input Length 583: 1195.167 ms\n",
      "Model: ONNX-OPT, Input Length 254: 692.669 ms\n",
      "Model: ONNX-OPT, Input Length 347: 1061.646 ms\n",
      "Model: ONNX-OPT, Input Length 305: 817.410 ms\n",
      "Model: ONNX-OPT, Input Length 278: 831.215 ms\n",
      "Model: ONNX-OPT, Input Length 339: 1037.498 ms\n",
      "Model: ONNX-OPT, Input Length 433: 1206.102 ms\n",
      "Model: ONNX-OPT, Input Length 385: 1186.063 ms\n",
      "Model: ONNX-OPT, Input Length 317: 910.647 ms\n",
      "Model: ONNX-OPT, Input Length 327: 937.849 ms\n",
      "Model: ONNX-OPT, Input Length 583: 1225.969 ms\n",
      "Model: ONNX-OPT, Input Length 254: 685.498 ms\n",
      "Model: ONNX-OPT, Input Length 347: 1071.490 ms\n",
      "Model: ONNX-OPT, Input Length 305: 810.493 ms\n",
      "Model: ONNX-OPT, Input Length 278: 831.093 ms\n",
      "Model: ONNX-OPT, Input Length 339: 1050.566 ms\n",
      "Model: ONNX-OPT, Input Length 433: 1200.004 ms\n",
      "Model: ONNX-OPT, Input Length 385: 1135.952 ms\n",
      "Model: ONNX-OPT, Input Length 317: 913.228 ms\n",
      "Model: ONNX-OPT, Input Length 327: 924.473 ms\n",
      "Model: ONNX-OPT, Input Length 583: 1181.089 ms\n",
      "Model: ONNX-OPT, Input Length 254: 701.096 ms\n",
      "Model: ONNX-OPT, Input Length 347: 1097.925 ms\n",
      "Model: ONNX-OPT, Input Length 305: 824.419 ms\n",
      "Model: ONNX-OPT, Input Length 278: 838.150 ms\n",
      "Model: ONNX-OPT, Input Length 339: 1039.821 ms\n",
      "Model: ONNX-OPT, Input Length 433: 1210.201 ms\n",
      "Model: ONNX-OPT, Input Length 385: 1140.242 ms\n",
      "Model: ONNX-OPT, Input Length 317: 877.630 ms\n",
      "Model: ONNX-OPT, Input Length 327: 996.179 ms\n",
      "Model: ONNX-OPT, Input Length 583: 1235.896 ms\n",
      "Model: ONNX-OPT, Input Length 254: 705.415 ms\n",
      "Model: ONNX-OPT, Input Length 347: 1133.251 ms\n",
      "Model: ONNX-OPT, Input Length 305: 802.398 ms\n",
      "Model: ONNX-OPT, Input Length 278: 794.579 ms\n",
      "Model: ONNX Quantized, Input Length 339: 815.175 ms\n",
      "Model: ONNX Quantized, Input Length 433: 1000.623 ms\n",
      "Model: ONNX Quantized, Input Length 385: 864.719 ms\n",
      "Model: ONNX Quantized, Input Length 317: 686.503 ms\n",
      "Model: ONNX Quantized, Input Length 327: 720.088 ms\n",
      "Model: ONNX Quantized, Input Length 583: 926.650 ms\n",
      "Model: ONNX Quantized, Input Length 254: 516.545 ms\n",
      "Model: ONNX Quantized, Input Length 347: 843.062 ms\n",
      "Model: ONNX Quantized, Input Length 305: 632.175 ms\n",
      "Model: ONNX Quantized, Input Length 278: 646.910 ms\n",
      "Model: ONNX Quantized, Input Length 339: 759.944 ms\n",
      "Model: ONNX Quantized, Input Length 433: 988.925 ms\n",
      "Model: ONNX Quantized, Input Length 385: 846.103 ms\n",
      "Model: ONNX Quantized, Input Length 317: 676.089 ms\n",
      "Model: ONNX Quantized, Input Length 327: 723.081 ms\n",
      "Model: ONNX Quantized, Input Length 583: 928.432 ms\n",
      "Model: ONNX Quantized, Input Length 254: 537.449 ms\n",
      "Model: ONNX Quantized, Input Length 347: 848.843 ms\n",
      "Model: ONNX Quantized, Input Length 305: 619.473 ms\n",
      "Model: ONNX Quantized, Input Length 278: 639.864 ms\n",
      "Model: ONNX Quantized, Input Length 339: 773.671 ms\n",
      "Model: ONNX Quantized, Input Length 433: 968.474 ms\n",
      "Model: ONNX Quantized, Input Length 385: 832.730 ms\n",
      "Model: ONNX Quantized, Input Length 317: 662.184 ms\n",
      "Model: ONNX Quantized, Input Length 327: 707.465 ms\n",
      "Model: ONNX Quantized, Input Length 583: 949.114 ms\n",
      "Model: ONNX Quantized, Input Length 254: 543.633 ms\n",
      "Model: ONNX Quantized, Input Length 347: 853.191 ms\n",
      "Model: ONNX Quantized, Input Length 305: 626.729 ms\n",
      "Model: ONNX Quantized, Input Length 278: 655.646 ms\n",
      "Model: ONNX Quantized, Input Length 339: 774.600 ms\n",
      "Model: ONNX Quantized, Input Length 433: 956.541 ms\n",
      "Model: ONNX Quantized, Input Length 385: 834.339 ms\n",
      "Model: ONNX Quantized, Input Length 317: 650.904 ms\n",
      "Model: ONNX Quantized, Input Length 327: 702.322 ms\n",
      "Model: ONNX Quantized, Input Length 583: 887.726 ms\n",
      "Model: ONNX Quantized, Input Length 254: 515.347 ms\n",
      "Model: ONNX Quantized, Input Length 347: 866.235 ms\n",
      "Model: ONNX Quantized, Input Length 305: 720.078 ms\n",
      "Model: ONNX Quantized, Input Length 278: 687.953 ms\n",
      "Model: ONNX Quantized, Input Length 339: 803.349 ms\n",
      "Model: ONNX Quantized, Input Length 433: 899.923 ms\n",
      "Model: ONNX Quantized, Input Length 385: 839.948 ms\n",
      "Model: ONNX Quantized, Input Length 317: 657.376 ms\n",
      "Model: ONNX Quantized, Input Length 327: 747.490 ms\n",
      "Model: ONNX Quantized, Input Length 583: 914.257 ms\n",
      "Model: ONNX Quantized, Input Length 254: 507.907 ms\n",
      "Model: ONNX Quantized, Input Length 347: 834.418 ms\n",
      "Model: ONNX Quantized, Input Length 305: 601.116 ms\n",
      "Model: ONNX Quantized, Input Length 278: 632.842 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 339: 799.636 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 433: 943.007 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 385: 837.395 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 317: 658.945 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 327: 701.034 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 583: 936.801 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 254: 503.977 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 347: 815.187 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 305: 599.298 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 615.564 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 339: 767.719 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 433: 964.642 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 385: 843.908 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 317: 666.507 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 327: 706.615 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 583: 908.411 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 254: 501.620 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 347: 802.668 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 305: 602.330 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 636.512 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 339: 732.158 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 433: 955.041 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 385: 834.975 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 317: 652.841 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 327: 704.060 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 583: 911.156 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 254: 511.093 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 347: 811.655 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 305: 598.929 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 627.934 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 339: 746.146 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 433: 939.572 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 385: 827.190 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 317: 660.230 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 327: 705.183 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 583: 898.789 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 254: 501.182 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 347: 802.746 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 305: 598.872 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 623.272 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 339: 776.964 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 433: 970.089 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 385: 903.505 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 317: 646.955 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 327: 704.280 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 583: 940.914 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 254: 500.790 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 347: 797.747 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 305: 599.487 ms\n",
      "Model: ONNX-OPT Quantized, Input Length 278: 620.810 ms\n"
     ]
    }
   ],
   "source": [
    "data_amount = 100\n",
    "for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "    print(\"Loading: {} {}\".format(reader, adapter))\n",
    "    \n",
    "    #load adapter specific dataset\n",
    "    data_set_name = adapter\n",
    "    if data_set_name in [\"commonsense_qa\", \"social_i_qa\", \"multirc\"]:\n",
    "        continue\n",
    "    elif data_set_name == \"race\":\n",
    "        data = load_dataset(data_set_name, \"all\", split=f\"validation[:{data_amount}]\")\n",
    "    else: \n",
    "        data = load_dataset(data_set_name, split=f\"validation[:{data_amount}]\")\n",
    "    \n",
    "    print(f\"Loaded dataset: {data_set_name}\")\n",
    "\n",
    "    # build preped data\n",
    "    preped_data_set = []\n",
    "    for example in data:\n",
    "        if data_set_name == \"cosmos_qa\":\n",
    "            choices = [example[\"answer0\"], example[\"answer1\"], example[\"answer2\"], example[\"answer3\"]]\n",
    "            preped_data_set.append((example[\"question\"], example[\"context\"], choices))\n",
    "        elif data_set_name == \"quail\":\n",
    "            preped_data_set.append((example[\"question\"], example[\"context\"], example[\"answers\"]))\n",
    "        elif data_set_name == \"quartz\":\n",
    "            preped_data_set.append((example[\"question\"], example[\"para\"], example[\"choices\"][\"text\"]))\n",
    "        elif data_set_name ==\"race\":\n",
    "            preped_data_set.append((example[\"question\"], example[\"article\"], example[\"options\"]))\n",
    "            id_name = \"example_id\"\n",
    "            \n",
    "        else:\n",
    "            print(\"Error. Not implemented data_set. Dont know how to build preped_data_set.\")\n",
    "            Exception\n",
    "    print(\"Preped data\")\n",
    "\n",
    "    data_runs = 5\n",
    "    data_intervall = 10\n",
    "    \n",
    "    #load and eval base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(reader)\n",
    "    default_model = AutoModelWithHeads.from_pretrained(reader)\n",
    "    adapter_name = default_model.load_adapter(f\"AdapterHub/{reader}-pf-{adapter}\", source=\"hf\")\n",
    "    default_model.active_adapters = adapter_name\n",
    "    performance_log(mc_model_inference, \"Base\", default_model, tokenizer, preped_data_set, data_set_name, data_intervall, data_runs) \n",
    "    \n",
    "    #load and eval quant model\n",
    "    quantized_base_model = torch.quantization.quantize_dynamic(default_model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "    performance_log(mc_model_inference, \"Base Quantized\", quantized_base_model, tokenizer, preped_data_set, data_set_name, data_intervall, data_runs) \n",
    "    \n",
    "    #load onnx models\n",
    "    model_onnx, model_onnx_quant = repo_builder(reader, adapter)\n",
    "    onnx_models_list = load_model(model_onnx, model_onnx_quant, as_list=True)\n",
    "    onnx_models_name_helper_list = [\"ONNX\", \"ONNX-OPT\", \"ONNX Quantized\", \"ONNX-OPT Quantized\"]\n",
    "\n",
    "    # eval onnx models\n",
    "    for onnx_model, onnx_model_name in zip(onnx_models_list, onnx_models_name_helper_list):\n",
    "        performance_log(mc_onnx_inference, onnx_model_name, onnx_model, tokenizer, preped_data_set, data_set_name, data_intervall, data_runs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>time once (ms)</th>\n",
       "      <th>average_time 50 times (ms)</th>\n",
       "      <th>seq_length</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_set_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>364.140987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>Do i need to go for a legal divorce ? I wanted...</td>\n",
       "      <td>Why is this person asking about divorce ?</td>\n",
       "      <td>['If he gets married in the church he wo nt ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>cosmos_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Base</td>\n",
       "      <td>240.724087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67</td>\n",
       "      <td>I watched the first McCain / Obama debate last...</td>\n",
       "      <td>How would this person be classified ?</td>\n",
       "      <td>['None of the above choices .', 'Liberal', 'Co...</td>\n",
       "      <td>10</td>\n",
       "      <td>cosmos_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Base</td>\n",
       "      <td>454.405308</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117</td>\n",
       "      <td>So , while i was in the library in my old neig...</td>\n",
       "      <td>What did you do after realizing that your thin...</td>\n",
       "      <td>['I set about reporting the theft to the campu...</td>\n",
       "      <td>20</td>\n",
       "      <td>cosmos_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Base</td>\n",
       "      <td>268.949986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>At the beginning of the change there were jet ...</td>\n",
       "      <td>Why did jet airplanes allow us jump from one p...</td>\n",
       "      <td>['Because it was enough to add two or three co...</td>\n",
       "      <td>30</td>\n",
       "      <td>cosmos_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Base</td>\n",
       "      <td>261.901140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62</td>\n",
       "      <td>Another thing I do n't appreciate is the shoot...</td>\n",
       "      <td>Why might I have problems with Jon playing a s...</td>\n",
       "      <td>[\"Because a shooting game involves violence bu...</td>\n",
       "      <td>40</td>\n",
       "      <td>cosmos_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>ONNX-OPT Quantized</td>\n",
       "      <td>940.914154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>583</td>\n",
       "      <td>What is one of the most boring and tiresome wo...</td>\n",
       "      <td>What can we learn about responsibility?</td>\n",
       "      <td>[\"It's of secondary importance to discipline.\"...</td>\n",
       "      <td>50</td>\n",
       "      <td>race</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>ONNX-OPT Quantized</td>\n",
       "      <td>500.789881</td>\n",
       "      <td>NaN</td>\n",
       "      <td>254</td>\n",
       "      <td>Children have their own rules in playing games...</td>\n",
       "      <td>The writer believes that   _  .</td>\n",
       "      <td>['children should make better rules for their ...</td>\n",
       "      <td>60</td>\n",
       "      <td>race</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>ONNX-OPT Quantized</td>\n",
       "      <td>797.747374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>347</td>\n",
       "      <td>Before l tell you bow many hours a day people ...</td>\n",
       "      <td>According to the poll, the time people spend v...</td>\n",
       "      <td>['one to three hours', 'four to six hours', 'o...</td>\n",
       "      <td>70</td>\n",
       "      <td>race</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>ONNX-OPT Quantized</td>\n",
       "      <td>599.487305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>305</td>\n",
       "      <td>We are fortunate to be living in a time when a...</td>\n",
       "      <td>The writer's attitude toward the digital socie...</td>\n",
       "      <td>['critical', 'positive', 'neutral', 'negative']</td>\n",
       "      <td>80</td>\n",
       "      <td>race</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>ONNX-OPT Quantized</td>\n",
       "      <td>620.810270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>278</td>\n",
       "      <td>Bali is an Indonesia island that is rich in in...</td>\n",
       "      <td>When a person dies in Bali, it is a common pra...</td>\n",
       "      <td>['express deep sorrow at his death', 'celebrat...</td>\n",
       "      <td>90</td>\n",
       "      <td>race</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              model_name  time once (ms)  average_time 50 times (ms)  \\\n",
       "0                   Base      364.140987                         NaN   \n",
       "1                   Base      240.724087                         NaN   \n",
       "2                   Base      454.405308                         NaN   \n",
       "3                   Base      268.949986                         NaN   \n",
       "4                   Base      261.901140                         NaN   \n",
       "...                  ...             ...                         ...   \n",
       "2095  ONNX-OPT Quantized      940.914154                         NaN   \n",
       "2096  ONNX-OPT Quantized      500.789881                         NaN   \n",
       "2097  ONNX-OPT Quantized      797.747374                         NaN   \n",
       "2098  ONNX-OPT Quantized      599.487305                         NaN   \n",
       "2099  ONNX-OPT Quantized      620.810270                         NaN   \n",
       "\n",
       "      seq_length                                            context  \\\n",
       "0             92  Do i need to go for a legal divorce ? I wanted...   \n",
       "1             67  I watched the first McCain / Obama debate last...   \n",
       "2            117  So , while i was in the library in my old neig...   \n",
       "3             58  At the beginning of the change there were jet ...   \n",
       "4             62  Another thing I do n't appreciate is the shoot...   \n",
       "...          ...                                                ...   \n",
       "2095         583  What is one of the most boring and tiresome wo...   \n",
       "2096         254  Children have their own rules in playing games...   \n",
       "2097         347  Before l tell you bow many hours a day people ...   \n",
       "2098         305  We are fortunate to be living in a time when a...   \n",
       "2099         278  Bali is an Indonesia island that is rich in in...   \n",
       "\n",
       "                                               question  \\\n",
       "0             Why is this person asking about divorce ?   \n",
       "1                 How would this person be classified ?   \n",
       "2     What did you do after realizing that your thin...   \n",
       "3     Why did jet airplanes allow us jump from one p...   \n",
       "4     Why might I have problems with Jon playing a s...   \n",
       "...                                                 ...   \n",
       "2095            What can we learn about responsibility?   \n",
       "2096                    The writer believes that   _  .   \n",
       "2097  According to the poll, the time people spend v...   \n",
       "2098  The writer's attitude toward the digital socie...   \n",
       "2099  When a person dies in Bali, it is a common pra...   \n",
       "\n",
       "                                                choices  data_id data_set_name  \n",
       "0     ['If he gets married in the church he wo nt ha...        0     cosmos_qa  \n",
       "1     ['None of the above choices .', 'Liberal', 'Co...       10     cosmos_qa  \n",
       "2     ['I set about reporting the theft to the campu...       20     cosmos_qa  \n",
       "3     ['Because it was enough to add two or three co...       30     cosmos_qa  \n",
       "4     [\"Because a shooting game involves violence bu...       40     cosmos_qa  \n",
       "...                                                 ...      ...           ...  \n",
       "2095  [\"It's of secondary importance to discipline.\"...       50          race  \n",
       "2096  ['children should make better rules for their ...       60          race  \n",
       "2097  ['one to three hours', 'four to six hours', 'o...       70          race  \n",
       "2098    ['critical', 'positive', 'neutral', 'negative']       80          race  \n",
       "2099  ['express deep sorrow at his death', 'celebrat...       90          race  \n",
       "\n",
       "[2100 rows x 9 columns]"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"inference_time_mcq_2.csv\")\n",
    "df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure filesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = pd.read_csv(\"square_skills/impl_skills.csv\")\n",
    "# skill = \"multiple-choice\"\n",
    "# skills = load_skills(skill)\n",
    "\n",
    "skills = all_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_of_model(model):\n",
    "    # torch.save(model.state_dict(), \"temp.p\")\n",
    "    # size_of_model = os.path.getsize(\"temp.p\")/(1024*1024)\n",
    "    # print('Size (MB):', os.path.getsize(\"temp.p\")/(1024*1024))\n",
    "    # os.remove('temp.p')\n",
    "\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_of_model = (param_size + buffer_size) / 1024**2\n",
    "    return size_of_model\n",
    "\n",
    "def save_df(df_new, path_to_logger_file = \"logger_all.csv\"):\n",
    "    if os.path.exists(path_to_logger_file):\n",
    "        df_fin = pd.concat([pd.read_csv(path_to_logger_file), df_new])\n",
    "        df_fin.to_csv(path_to_logger_file,index=False)\n",
    "    else: \n",
    "        df_new.to_csv(path_to_logger_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: bert-base-uncased boolq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3083.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423.32056427001953\n",
      "388.361328125\n",
      "Loading: roberta-base boolq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4957.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481.16434478759766\n",
      "446.2051086425781\n",
      "Loading: bert-base-uncased cosmos_qa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4683.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423.31763076782227\n",
      "388.361328125\n",
      "Loading: roberta-base cosmos_qa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 6042.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481.1614112854004\n",
      "446.2051086425781\n",
      "Loading: bert-base-uncased drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 5154.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421.06763458251953\n",
      "388.361328125\n",
      "Loading: roberta-base drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4245.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478.91141510009766\n",
      "446.2051086425781\n",
      "Loading: bert-base-uncased hotpotqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 5136.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421.06763458251953\n",
      "388.361328125\n",
      "Loading: roberta-base hotpotqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 5389.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478.91141510009766\n",
      "446.2051086425781\n",
      "Loading: bert-base-uncased multirc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4431.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423.32056427001953\n",
      "388.361328125\n",
      "Loading: roberta-base multirc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4322.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481.16434478759766\n",
      "446.2051086425781\n",
      "Loading: bert-base-uncased newsqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 5272.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421.06763458251953\n",
      "388.361328125\n",
      "Loading: roberta-base newsqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4057.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478.91141510009766\n",
      "446.2051086425781\n",
      "Loading: bert-base-uncased quail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4918.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423.31763076782227\n",
      "388.361328125\n",
      "Loading: roberta-base quail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4850.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481.1614112854004\n",
      "446.2051086425781\n",
      "Loading: roberta-base quartz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3687.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481.1614112854004\n",
      "446.2051086425781\n",
      "Loading: bert-base-uncased quoref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3666.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421.06763458251953\n",
      "388.361328125\n",
      "Loading: roberta-base quoref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4131.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478.91141510009766\n",
      "446.2051086425781\n",
      "Loading: bert-base-uncased race\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4655.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423.31763076782227\n",
      "388.361328125\n",
      "Loading: roberta-base race\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3831.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481.1614112854004\n",
      "446.2051086425781\n",
      "Loading: bert-base-uncased squad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3829.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421.06763458251953\n",
      "388.361328125\n",
      "Loading: roberta-base squad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4761.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478.91141510009766\n",
      "446.2051086425781\n",
      "Loading: bert-base-uncased squad_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 2830.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421.06763458251953\n",
      "388.361328125\n",
      "Loading: roberta-base squad_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3894.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478.91141510009766\n",
      "446.2051086425781\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"reader\", \"adapter\", \"base\", \"base_quant\", \"onnx\", \"onnx_quant\"])\n",
    "\n",
    "for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "    print(\"Loading: {} {}\".format(reader, adapter))\n",
    "\n",
    "\n",
    "    #load base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(reader)\n",
    "    default_model = AutoModelWithHeads.from_pretrained(reader)\n",
    "    adapter_name = default_model.load_adapter(f\"AdapterHub/{reader}-pf-{adapter}\", source=\"hf\")\n",
    "    default_model.active_adapters = adapter_name\n",
    "    # get base model size\n",
    "    default_model_size = get_size_of_model(default_model)\n",
    "    print(default_model_size)\n",
    "    \n",
    "    #get quant model size\n",
    "    quantized_base_model = torch.quantization.quantize_dynamic(default_model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "    quantized_base_model_size = get_size_of_model(quantized_base_model)\n",
    "    print(quantized_base_model_size)\n",
    "\n",
    "    try:\n",
    "        #load onnx model\n",
    "        onnx_model_size = os.path.getsize(f\"onnx/{reader}-pf-{adapter}-onnx/model.onnx\")/(1024*1024)\n",
    "        print(onnx_model_size)\n",
    "\n",
    "    except:\n",
    "        print(\"error while exporting onnx\")\n",
    "        onnx_model_size = \"error\"\n",
    "    \n",
    "    try:\n",
    "        # get onnx quant size \n",
    "        onnx_quant_model_size = os.path.getsize(f\"onnx/{reader}-pf-{adapter}-onnx/model_quant.onnx\")/(1024*1024)\n",
    "        print(onnx_quant_model_size)\n",
    "\n",
    "    except:\n",
    "        print(\"error while exporting onnx quant\")\n",
    "        onnx_quant_model_size = \"error\"\n",
    "\n",
    "        \n",
    "    df.loc[len(df)] = [reader, adapter, default_model_size, quantized_base_model_size, onnx_model_size, onnx_quant_model_size]\n",
    "save_df(df, path_to_logger_file=\"file_size_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Accuracy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate all extractive qa model on squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_name = \"squad\"\n",
    "data = load_dataset(data_set_name, split=\"validation[:500]\")\n",
    "metric = evaluate.load(data_set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_evaluate(inference_func, model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs):\n",
    "    examples = list(zip(data[\"question\"], data[\"context\"]))\n",
    "    predictions = []\n",
    "    for example in examples:\n",
    "        _, task_outputs, _, _ = inference_func(model, tokenizer, [example], preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "        predictions.append(task_outputs[\"answers\"][0][0][\"answer\"])\n",
    "    \n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in zip(data[\"id\"], predictions)]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in data]\n",
    "    \n",
    "    score = metric.compute(predictions=formatted_predictions, references=references)\n",
    "\n",
    "    return score[\"f1\"], score[\"exact_match\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "    print(f\"Loading: {reader} {adapter}\")\n",
    "\n",
    "    #load base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(reader)\n",
    "    default_model = AutoModelWithHeads.from_pretrained(reader)\n",
    "    adapter_name = default_model.load_adapter(f\"AdapterHub/{reader}-pf-{adapter}\", source=\"hf\")\n",
    "    default_model.active_adapters = adapter_name\n",
    "\n",
    "    # Test acc for base model\n",
    "    f1, exact = squad_evaluate(base_qa, default_model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "    result.append((\"Base\", skill, reader, adapter, f1, exact, data_set_name))\n",
    "\n",
    "    #load onnx models\n",
    "    model_onnx, model_onnx_quant = repo_builder(reader, adapter)\n",
    "    onnx_models_list = load_model(model_onnx, model_onnx_quant, as_list=True)\n",
    "    onnx_models_name_helper_list = [\"ONNX\", \"ONNX-OPT\", \"Quantized ONNX\", \"Quantized ONNX - OPT\"]\n",
    "\n",
    "    # Test acc for onnx models\n",
    "    for onnx_model, onnx_model_name in zip(onnx_models_list, onnx_models_name_helper_list):\n",
    "        f1, exact = squad_evaluate(question_answering, onnx_model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "        result.append((onnx_model_name, skill, reader, adapter, f1, exact, data_set_name))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, columns=[\"name\", \"skill\", \"reader\", \"adapter\", \"f1\", \"exact\", \"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(df, \"accuracy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate extractive qa model on specific adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_scoring(pred_list, true_val_list):\n",
    "    hit = 0\n",
    "    for pred, true_val in zip(pred_list, true_val_list):\n",
    "        if pred in true_val:\n",
    "            hit += 1\n",
    "    return hit/len(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapter_evaluate(adapter, inference_func, model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs):\n",
    "    if adapter == \"drop\":\n",
    "        context_name = \"passage\"\n",
    "        id_name = \"query_id\"\n",
    "        answers_name = \"answers_spans\"\n",
    "    else:\n",
    "        context_name = \"context\"\n",
    "        id_name = \"id\"\n",
    "        answers_name = \"answers\"\n",
    "\n",
    "    examples = list(zip(data[\"question\"], data[context_name]))\n",
    "    \n",
    "    predictions = []\n",
    "    for example in examples:\n",
    "        _, task_outputs, _, _ = inference_func(model, tokenizer, [example], preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "        predictions.append(task_outputs[\"answers\"][0][0][\"answer\"])\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "runs = 250\n",
    "for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "    print(f\"Loading: {reader} {adapter}\")\n",
    "\n",
    "    #load adapter specific dataset\n",
    "    data_set_name = adapter\n",
    "    if data_set_name in [\"newsqa\", \"hotpot_qa\"]:\n",
    "        continue\n",
    "    else: \n",
    "        data = load_dataset(data_set_name, split=f\"validation[:{runs}]\")\n",
    "        print(f\"Loaded dataset: {data_set_name}\")\n",
    "    \n",
    "    #load base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(reader)\n",
    "    default_model = AutoModelWithHeads.from_pretrained(reader)\n",
    "    adapter_name = default_model.load_adapter(f\"AdapterHub/{reader}-pf-{adapter}\", source=\"hf\")\n",
    "    default_model.active_adapters = adapter_name\n",
    "    \n",
    "    # Get base results\n",
    "    base_model_result = adapter_evaluate(adapter, base_qa, default_model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "\n",
    "    #load and eval quant model \n",
    "    quantized_model = torch.quantization.quantize_dynamic(default_model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "    quant_base_model_result = adapter_evaluate(adapter, base_qa, quantized_model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "    scoring = accuracy_scoring(base_model_result, quant_base_model_result)\n",
    "    result.append((\"Quantized Base Model\", skill, reader, adapter, scoring, data_set_name, runs)) \n",
    "    \n",
    "    #load onnx models\n",
    "    model_onnx, model_onnx_quant = repo_builder(reader, adapter)\n",
    "    onnx_models_list = load_model(model_onnx, model_onnx_quant, as_list=True)\n",
    "    onnx_models_name_helper_list = [\"ONNX\", \"ONNX-OPT\", \"Quantized ONNX\", \"Quantized ONNX - OPT\"] \n",
    "    \n",
    "    for onnx_model, onnx_model_name in zip(onnx_models_list, onnx_models_name_helper_list):\n",
    "        onnx = adapter_evaluate(adapter, question_answering, onnx_model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "\n",
    "        scoring = accuracy_scoring(base_model_result, onnx)\n",
    "\n",
    "        result.append((onnx_model_name, skill, reader, adapter, scoring, data_set_name, runs))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, columns=[\"base_name\", \"onnx_name\", \"skill\", \"reader\", \"adapter\", \"result\" \"dataset\", \"runs\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(df, \"sim_base_to_onnx.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate categorical qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = pd.read_csv(\"square_skills/impl_skills.csv\")\n",
    "skill = \"categorical\"\n",
    "skills = load_skills(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_base_inference(model, tokenizer, question, context):\n",
    "    \n",
    "    raw_input = [[context, question]]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    answer_idx = torch.argmax(outputs.logits)\n",
    "    \n",
    "    return bool(answer_idx)\n",
    "\n",
    "def categorical_onnx_inference(onnx_model, tokenizer, question, context):\n",
    "\n",
    "    inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors=\"np\")\n",
    "    inputs = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\n",
    "\n",
    "    outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\n",
    "\n",
    "    return bool(np.argmax(outputs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolq_evaluate(model, tokenizer, data, inference_type):\n",
    "    result = []\n",
    "    for test_no in range(len(data)):\n",
    "        question = data[test_no][\"question\"]\n",
    "        context = data[test_no][\"passage\"]\n",
    "\n",
    "        answer = inference_type(model, tokenizer, question, context)\n",
    "        result.append(answer)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolq_accuracy_scoring(base_list, pred_list):\n",
    "    hit = 0\n",
    "    for base_pred, pred in zip(base_list, pred_list):\n",
    "        if base_pred == pred:\n",
    "            hit += 1\n",
    "    return hit/len(base_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "runs = 250\n",
    "\n",
    "data_set_name = \"boolq\"\n",
    "data = load_dataset(data_set_name, split=f\"validation[:{runs}]\")\n",
    "\n",
    "for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "    print(f\"Loading: {reader} {adapter}\")\n",
    "    \n",
    "    #load base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(reader)\n",
    "    default_model = AutoModelWithHeads.from_pretrained(reader)\n",
    "    adapter_name = default_model.load_adapter(f\"AdapterHub/{reader}-pf-{adapter}\", source=\"hf\")\n",
    "    default_model.active_adapters = adapter_name\n",
    "\n",
    "    # Get base results\n",
    "    base_model_result = boolq_evaluate(default_model, tokenizer, data, categorical_base_inference)\n",
    "\n",
    "    #load quant model\n",
    "    quantized_base_model = torch.quantization.quantize_dynamic(default_model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "    \n",
    "    #eval quant model\n",
    "    quant_base_model_result = boolq_evaluate(default_model, tokenizer, data, categorical_base_inference)\n",
    "    scoring = boolq_accuracy_scoring(base_model_result, quant_base_model_result)\n",
    "    result.append((\"Quantized Base Model\", skill, reader, adapter, scoring, data_set_name, runs)) \n",
    "    \n",
    "    \n",
    "    #load onnx models\n",
    "    model_onnx, model_onnx_quant = repo_builder(reader, adapter)\n",
    "    onnx_models_list = load_model(model_onnx, model_onnx_quant, as_list=True)\n",
    "    onnx_models_name_helper_list = [\"ONNX\", \"ONNX-OPT\", \"Quantized ONNX\", \"Quantized ONNX - OPT\"]\n",
    "\n",
    "    # eval onnx models\n",
    "    for onnx_model, onnx_model_name in zip(onnx_models_list, onnx_models_name_helper_list):\n",
    "        onnx_result = boolq_evaluate(onnx_model, tokenizer, data, categorical_onnx_inference)\n",
    "        scoring = boolq_accuracy_scoring(base_model_result, onnx_result)\n",
    "        result.append((onnx_model_name, skill, reader, adapter, scoring, data_set_name, runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, columns=[\"base_name\", \"onnx_name\", \"skill\", \"reader\", \"adapter\", \"result\" \"dataset\", \"runs\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(df, \"sim_base_to_onnx.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate mcq qa model on specific adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_model_inference(model, tokenizer, preped_data_set):\n",
    "    result = []\n",
    "    i = 0\n",
    "    for example in preped_data_set:\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "        question, context, choices = example[0], example[1], example[2]\n",
    "        \n",
    "        outputs = []\n",
    "        raw_input = [[context, question + \" \" + choice] for choice in choices]\n",
    "        inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        answer_idx = torch.argmax(outputs.logits)\n",
    "        result.append(choices[answer_idx])\n",
    "    return result\n",
    "\n",
    "def onnx_inference(onnx_model, tokenizer, preped_data_set):\n",
    "    result = []\n",
    "    i = 0\n",
    "    for example in preped_data_set:\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "        question, context, choices= example[0], example[1], example[2]\n",
    "\n",
    "        raw_input = [[context, question + \" \" + choice] for choice in choices]\n",
    "        inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"np\")\n",
    "\n",
    "        inputs['input_ids'] =  np.expand_dims(inputs['input_ids'], axis=0)\n",
    "        inputs['attention_mask'] =  np.expand_dims(inputs['attention_mask'], axis=0)\n",
    "\n",
    "        if \"token_type_ids\" in inputs: #roberta does not use this\n",
    "            inputs['token_type_ids'] = np.expand_dims(inputs['token_type_ids'], axis=0)\n",
    "\n",
    "        outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\n",
    "\n",
    "        answer_idx = np.argmax(outputs[0])\n",
    "        result.append(choices[answer_idx])\n",
    "    return result\n",
    "\n",
    "def accuracy_scoring(base_list, pred_list):\n",
    "    hit = 0\n",
    "    for base_pred, pred in zip(base_list, pred_list):\n",
    "        if base_pred == pred:\n",
    "            hit += 1\n",
    "    return hit/len(base_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = pd.read_csv(\"square_skills/impl_skills.csv\")\n",
    "skill = \"multiple-choice\"\n",
    "skills = load_skills(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: bert-base-uncased cosmos_qa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cosmos_qa (/Users/michaelhermann/.cache/huggingface/datasets/cosmos_qa/default/0.1.0/3e18538cbfdb2c04189b16642715f0f6da3e97ed5df0aadcec3641245b2cf157)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: cosmos_qa\n",
      "Preped data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4104.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing base inference. \n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Getting scoring for quant base model.\n",
      "Doing ONNX loading. \n",
      "Doing bert-base-uncased, cosmos_qa for ONNX\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing bert-base-uncased, cosmos_qa for ONNX-OPT\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing bert-base-uncased, cosmos_qa for Quantized ONNX\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing bert-base-uncased, cosmos_qa for Quantized ONNX - OPT\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Loading: roberta-base cosmos_qa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cosmos_qa (/Users/michaelhermann/.cache/huggingface/datasets/cosmos_qa/default/0.1.0/3e18538cbfdb2c04189b16642715f0f6da3e97ed5df0aadcec3641245b2cf157)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: cosmos_qa\n",
      "Preped data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4637.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing base inference. \n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Getting scoring for quant base model.\n",
      "Doing ONNX loading. \n",
      "Doing roberta-base, cosmos_qa for ONNX\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing roberta-base, cosmos_qa for ONNX-OPT\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing roberta-base, cosmos_qa for Quantized ONNX\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing roberta-base, cosmos_qa for Quantized ONNX - OPT\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Loading: bert-base-uncased multirc\n",
      "Loading: roberta-base multirc\n",
      "Loading: bert-base-uncased quail\n",
      "Loading: roberta-base quail\n",
      "Loading: roberta-base quartz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset quartz (/Users/michaelhermann/.cache/huggingface/datasets/quartz/default/0.1.0/6e5195fb88ecd7a75eda5d8f3549c262c8b15267366f38f9c153f40da92724a6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: quartz\n",
      "Preped data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3484.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing base inference. \n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Getting scoring for quant base model.\n",
      "Doing ONNX loading. \n",
      "Doing roberta-base, quartz for ONNX\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing roberta-base, quartz for ONNX-OPT\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing roberta-base, quartz for Quantized ONNX\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing roberta-base, quartz for Quantized ONNX - OPT\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Loading: bert-base-uncased race\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset race (/Users/michaelhermann/.cache/huggingface/datasets/race/all/0.1.0/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preped data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 2391.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing base inference. \n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Getting scoring for quant base model.\n",
      "Doing ONNX loading. \n",
      "Doing bert-base-uncased, race for ONNX\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing bert-base-uncased, race for ONNX-OPT\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing bert-base-uncased, race for Quantized ONNX\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing bert-base-uncased, race for Quantized ONNX - OPT\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Loading: roberta-base race\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset race (/Users/michaelhermann/.cache/huggingface/datasets/race/all/0.1.0/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preped data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3155.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing base inference. \n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Getting scoring for quant base model.\n",
      "Doing ONNX loading. \n",
      "Doing roberta-base, race for ONNX\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing roberta-base, race for ONNX-OPT\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing roberta-base, race for Quantized ONNX\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n",
      "Doing roberta-base, race for Quantized ONNX - OPT\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "Done with scoring\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "runs = 250\n",
    "for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "    print(f\"Loading: {reader} {adapter}\")\n",
    "\n",
    "    #load adapter specific dataset\n",
    "    data_set_name = adapter\n",
    "    preped_data_set = []\n",
    "    if data_set_name in [\"commonsense_qa\", \"social_i_qa\", \"multirc\", \"quail\"]:\n",
    "        continue\n",
    "    elif data_set_name == \"race\":\n",
    "        data = load_dataset(data_set_name, \"all\", split=f\"validation[:{runs}]\")\n",
    "    else: \n",
    "        data = load_dataset(data_set_name, split=f\"validation[:{runs}]\")\n",
    "    print(f\"Loaded dataset: {data_set_name}\")\n",
    "    \n",
    "    # build preped data\n",
    "    for example in data:\n",
    "        if data_set_name == \"cosmos_qa\":\n",
    "            choices = [example[\"answer0\"], example[\"answer1\"], example[\"answer2\"], example[\"answer3\"]]\n",
    "            preped_data_set.append((example[\"question\"], example[\"context\"], choices))\n",
    "        elif data_set_name == \"quail\":\n",
    "            preped_data_set.append((example[\"question\"], example[\"context\"], example[\"answers\"]))\n",
    "        elif data_set_name == \"quartz\":\n",
    "            preped_data_set.append((example[\"question\"], example[\"para\"], example[\"choices\"][\"text\"]))\n",
    "        elif data_set_name ==\"race\":\n",
    "            preped_data_set.append((example[\"question\"], example[\"article\"], example[\"options\"]))\n",
    "            \n",
    "        else:\n",
    "            print(\"Error. Not implemented data_set. Dont know how to build preped_data_set.\")\n",
    "            Exception\n",
    "    print(\"Preped data\")\n",
    "    \n",
    "    #  load base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    base_model = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "    adapter_name = base_model.load_adapter(f\"AdapterHub/bert-base-uncased-pf-{adapter}\", source=\"hf\")\n",
    "    base_model.active_adapters = adapter_name\n",
    "    \n",
    "    # Get base results\n",
    "    print(\"Doing base inference. \")\n",
    "    base_model_result = mc_model_inference(base_model, tokenizer, preped_data_set)\n",
    "\n",
    "    #load and eval quant model \n",
    "    quantized_model = torch.quantization.quantize_dynamic(base_model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "    quant_base_model_result = mc_model_inference(quantized_model, tokenizer, preped_data_set)\n",
    "    print(\"Getting scoring for quant base model.\")\n",
    "    scoring = accuracy_scoring(base_model_result, quant_base_model_result)\n",
    "    result.append((\"Quantized Base Model\", skill, reader, adapter, scoring, data_set_name, runs)) \n",
    "    \n",
    "    #load onnx models\n",
    "    print(\"Doing ONNX loading. \")\n",
    "    model_onnx, model_onnx_quant = repo_builder(reader, adapter)\n",
    "    onnx_models_list = load_model(model_onnx, model_onnx_quant, as_list=True)\n",
    "    onnx_models_name_helper_list = [\"ONNX\", \"ONNX-OPT\", \"Quantized ONNX\", \"Quantized ONNX - OPT\"] \n",
    "    \n",
    "    for onnx_model, onnx_model_name in zip(onnx_models_list, onnx_models_name_helper_list):\n",
    "        print(f\"Doing {reader}, {adapter} for {onnx_model_name}\")\n",
    "        onnx_results = onnx_inference(onnx_model, tokenizer, preped_data_set)\n",
    "        scoring = accuracy_scoring(base_model_result, onnx_results)\n",
    "        print(\"Done with scoring\")\n",
    "        result.append((onnx_model_name, skill, reader, adapter, scoring, data_set_name, runs))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_name</th>\n",
       "      <th>onnx_name</th>\n",
       "      <th>skill</th>\n",
       "      <th>reader</th>\n",
       "      <th>adapter</th>\n",
       "      <th>resultdataset</th>\n",
       "      <th>runs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quantized Base Model</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>0.956</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ONNX</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>1.000</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ONNX-OPT</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>1.000</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quantized ONNX</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>0.760</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantized ONNX - OPT</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>0.760</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Quantized Base Model</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>0.956</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ONNX</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>0.208</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ONNX-OPT</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>0.208</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Quantized ONNX</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>0.220</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Quantized ONNX - OPT</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>0.220</td>\n",
       "      <td>cosmos_qa</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Quantized Base Model</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>quartz</td>\n",
       "      <td>0.752</td>\n",
       "      <td>quartz</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ONNX</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>quartz</td>\n",
       "      <td>0.564</td>\n",
       "      <td>quartz</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ONNX-OPT</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>quartz</td>\n",
       "      <td>0.564</td>\n",
       "      <td>quartz</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Quantized ONNX</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>quartz</td>\n",
       "      <td>0.532</td>\n",
       "      <td>quartz</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Quantized ONNX - OPT</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>quartz</td>\n",
       "      <td>0.532</td>\n",
       "      <td>quartz</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Quantized Base Model</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>race</td>\n",
       "      <td>0.948</td>\n",
       "      <td>race</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ONNX</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>race</td>\n",
       "      <td>1.000</td>\n",
       "      <td>race</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ONNX-OPT</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>race</td>\n",
       "      <td>1.000</td>\n",
       "      <td>race</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Quantized ONNX</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>race</td>\n",
       "      <td>0.784</td>\n",
       "      <td>race</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Quantized ONNX - OPT</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>race</td>\n",
       "      <td>0.784</td>\n",
       "      <td>race</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Quantized Base Model</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>race</td>\n",
       "      <td>0.948</td>\n",
       "      <td>race</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ONNX</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>race</td>\n",
       "      <td>0.268</td>\n",
       "      <td>race</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ONNX-OPT</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>race</td>\n",
       "      <td>0.268</td>\n",
       "      <td>race</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Quantized ONNX</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>race</td>\n",
       "      <td>0.272</td>\n",
       "      <td>race</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantized ONNX - OPT</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>race</td>\n",
       "      <td>0.272</td>\n",
       "      <td>race</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               base_name        onnx_name              skill     reader  \\\n",
       "0   Quantized Base Model  multiple-choice  bert-base-uncased  cosmos_qa   \n",
       "1                   ONNX  multiple-choice  bert-base-uncased  cosmos_qa   \n",
       "2               ONNX-OPT  multiple-choice  bert-base-uncased  cosmos_qa   \n",
       "3         Quantized ONNX  multiple-choice  bert-base-uncased  cosmos_qa   \n",
       "4   Quantized ONNX - OPT  multiple-choice  bert-base-uncased  cosmos_qa   \n",
       "5   Quantized Base Model  multiple-choice       roberta-base  cosmos_qa   \n",
       "6                   ONNX  multiple-choice       roberta-base  cosmos_qa   \n",
       "7               ONNX-OPT  multiple-choice       roberta-base  cosmos_qa   \n",
       "8         Quantized ONNX  multiple-choice       roberta-base  cosmos_qa   \n",
       "9   Quantized ONNX - OPT  multiple-choice       roberta-base  cosmos_qa   \n",
       "10  Quantized Base Model  multiple-choice       roberta-base     quartz   \n",
       "11                  ONNX  multiple-choice       roberta-base     quartz   \n",
       "12              ONNX-OPT  multiple-choice       roberta-base     quartz   \n",
       "13        Quantized ONNX  multiple-choice       roberta-base     quartz   \n",
       "14  Quantized ONNX - OPT  multiple-choice       roberta-base     quartz   \n",
       "15  Quantized Base Model  multiple-choice  bert-base-uncased       race   \n",
       "16                  ONNX  multiple-choice  bert-base-uncased       race   \n",
       "17              ONNX-OPT  multiple-choice  bert-base-uncased       race   \n",
       "18        Quantized ONNX  multiple-choice  bert-base-uncased       race   \n",
       "19  Quantized ONNX - OPT  multiple-choice  bert-base-uncased       race   \n",
       "20  Quantized Base Model  multiple-choice       roberta-base       race   \n",
       "21                  ONNX  multiple-choice       roberta-base       race   \n",
       "22              ONNX-OPT  multiple-choice       roberta-base       race   \n",
       "23        Quantized ONNX  multiple-choice       roberta-base       race   \n",
       "24  Quantized ONNX - OPT  multiple-choice       roberta-base       race   \n",
       "\n",
       "    adapter resultdataset  runs  \n",
       "0     0.956     cosmos_qa   250  \n",
       "1     1.000     cosmos_qa   250  \n",
       "2     1.000     cosmos_qa   250  \n",
       "3     0.760     cosmos_qa   250  \n",
       "4     0.760     cosmos_qa   250  \n",
       "5     0.956     cosmos_qa   250  \n",
       "6     0.208     cosmos_qa   250  \n",
       "7     0.208     cosmos_qa   250  \n",
       "8     0.220     cosmos_qa   250  \n",
       "9     0.220     cosmos_qa   250  \n",
       "10    0.752        quartz   250  \n",
       "11    0.564        quartz   250  \n",
       "12    0.564        quartz   250  \n",
       "13    0.532        quartz   250  \n",
       "14    0.532        quartz   250  \n",
       "15    0.948          race   250  \n",
       "16    1.000          race   250  \n",
       "17    1.000          race   250  \n",
       "18    0.784          race   250  \n",
       "19    0.784          race   250  \n",
       "20    0.948          race   250  \n",
       "21    0.268          race   250  \n",
       "22    0.268          race   250  \n",
       "23    0.272          race   250  \n",
       "24    0.272          race   250  "
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(result, columns=[\"base_name\", \"onnx_name\", \"skill\", \"reader\", \"adapter\", \"result\" \"dataset\", \"runs\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(df, \"sim_base_to_onnx.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapterhub_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dff476688330005cfc33f1ee0f15c13ae533c265ccd041ab146cdb98ecc6219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
