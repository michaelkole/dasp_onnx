{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import AutoModelWithHeads, AutoTokenizer\n",
    "\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from onnxruntime import InferenceSession\n",
    "import onnxruntime\n",
    "\n",
    "\n",
    "import time\n",
    "from typing import Tuple, Union\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_skills(skill_type, path=\"square_skills/impl_skills.csv\"):\n",
    "    all_skills = pd.read_csv(path)\n",
    "    skills = all_skills[all_skills[\"Type\"] == skill_type]\n",
    "    return skills"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure extractive qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = pd.read_csv(\"square_skills/impl_skills.csv\")\n",
    "skill = \"span-extraction\"\n",
    "skills = load_skills(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(\n",
    "            start_: np.ndarray,\n",
    "            end_: np.ndarray,\n",
    "            topk: int,\n",
    "            max_answer_len: int,\n",
    "            undesired_tokens_: np.ndarray,\n",
    "    ) -> Tuple:\n",
    "    \"\"\"\n",
    "    Take the output of any :obj:`ModelForQuestionAnswering` and\n",
    "        will generate probabilities for each span to be the\n",
    "        actual answer.\n",
    "    In addition, it filters out some unwanted/impossible cases\n",
    "    like answer len being greater than max_answer_len or\n",
    "    answer end position being before the starting position.\n",
    "    The method supports output the k-best answer through\n",
    "    the topk argument.\n",
    "    Args:\n",
    "        start_ (:obj:`np.ndarray`): Individual start\n",
    "            probabilities for each token.\n",
    "        end (:obj:`np.ndarray`): Individual end_ probabilities\n",
    "            for each token.\n",
    "        topk (:obj:`int`): Indicates how many possible answer\n",
    "            span(s) to extract from the model output.\n",
    "        max_answer_len (:obj:`int`): Maximum size of the answer\n",
    "            to extract from the model\"s output.\n",
    "        undesired_tokens_ (:obj:`np.ndarray`): Mask determining\n",
    "            tokens that can be part of the answer\n",
    "    \"\"\"\n",
    "    # Ensure we have batch axis\n",
    "    if start_.ndim == 1:\n",
    "        start_ = start_[None]\n",
    "\n",
    "    if end_.ndim == 1:\n",
    "        end_ = end_[None]\n",
    "\n",
    "    # Compute the score of each tuple(start_, end_) to be the real answer\n",
    "    outer = np.matmul(np.expand_dims(start_, -1), np.expand_dims(end_, 1))\n",
    "\n",
    "    # Remove candidate with end_ < start_ and end_ - start_ > max_answer_len\n",
    "    candidates = np.tril(np.triu(outer), max_answer_len - 1)\n",
    "\n",
    "    #  Inspired by Chen & al. (https://github.com/facebookresearch/DrQA)\n",
    "    scores_flat = candidates.flatten()\n",
    "    if topk == 1:\n",
    "        idx_sort = [np.argmax(scores_flat)]\n",
    "    elif len(scores_flat) < topk:\n",
    "        idx_sort = np.argsort(-scores_flat)\n",
    "    else:\n",
    "        idx = np.argpartition(-scores_flat, topk)[0:topk]\n",
    "        idx_sort = idx[np.argsort(-scores_flat[idx])]\n",
    "\n",
    "    starts_, ends_ = np.unravel_index(idx_sort, candidates.shape)[1:]\n",
    "    desired_spans = np.isin(starts_, undesired_tokens_.nonzero()) & np.isin(\n",
    "        ends_, undesired_tokens_.nonzero()\n",
    "    )\n",
    "    starts_ = starts_[desired_spans]\n",
    "    ends_ = ends_[desired_spans]\n",
    "    scores_ = candidates[0, starts_, ends_]\n",
    "\n",
    "    return starts_, ends_, scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_predict(\n",
    "            model, input, tokenizer, preprocessing_kwargs, model_kwargs, batch_size=1, disable_gpu=True, output_features=False\n",
    "    ) -> Union[dict, Tuple[dict, dict]]:\n",
    "        \"\"\"\n",
    "        Inference on the input.\n",
    "        Args:\n",
    "         request: the request with the input and optional kwargs\n",
    "         output_features: return the features of the input.\n",
    "            Necessary if, e.g., attention mask is needed for post-processing.\n",
    "        Returns:\n",
    "             The model outputs and optionally the input features\n",
    "        \"\"\"\n",
    "\n",
    "        all_predictions = []\n",
    "        preprocessing_kwargs[\"padding\"] = preprocessing_kwargs.get(\n",
    "            \"padding\", True\n",
    "        )\n",
    "        preprocessing_kwargs[\"truncation\"] = preprocessing_kwargs.get(\n",
    "            \"truncation\", True\n",
    "        )\n",
    "        model.to(\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available() and not disable_gpu\n",
    "            else \"cpu\"\n",
    "        )\n",
    "\n",
    "        features = tokenizer(\n",
    "            input, return_tensors=\"pt\", **preprocessing_kwargs\n",
    "        )\n",
    "\n",
    "        for start_idx in range(0, len(input), batch_size):\n",
    "            with torch.no_grad():\n",
    "                input_features = {\n",
    "                    k: features[k][start_idx: start_idx + batch_size]\n",
    "                    for k in features.keys()\n",
    "                }\n",
    "                predictions = model(**input_features, **model_kwargs)\n",
    "                all_predictions.append(predictions)\n",
    "\n",
    "        keys = all_predictions[0].keys()\n",
    "        final_prediction = {}\n",
    "        for key in keys:\n",
    "            # HuggingFace outputs for \"attentions\" and more is\n",
    "            # returned as tuple of tensors\n",
    "            # Tuple of tuples only exists for \"past_key_values\"\n",
    "            # which is only relevant for generation.\n",
    "            # Generation should NOT use this function\n",
    "            if isinstance(all_predictions[0][key], tuple):\n",
    "                tuple_of_lists = list(\n",
    "                    zip(\n",
    "                        *[\n",
    "                            [\n",
    "                                torch.stack(p).cpu()\n",
    "                                if isinstance(p, tuple)\n",
    "                                else p.cpu()\n",
    "                                for p in tpl[key]\n",
    "                            ]\n",
    "                            for tpl in all_predictions\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "                final_prediction[key] = tuple(torch.cat(l) for l in tuple_of_lists)\n",
    "            else:\n",
    "                final_prediction[key] = torch.cat(\n",
    "                    [p[key].cpu() for p in all_predictions]\n",
    "                )\n",
    "        if output_features:\n",
    "            return final_prediction, features\n",
    "\n",
    "        return final_prediction\n",
    "\n",
    "def base_qa(model, tokenizer, input, preprocessing_kwargs, task_kwargs, model_kwargs):\n",
    "    \"\"\"\n",
    "    Span-based question answering for a given question and context.\n",
    "    We expect the input to use the (question, context) format for the text pairs.\n",
    "    Args:\n",
    "        request: the prediction request\n",
    "    \"\"\"    \n",
    "    preprocessing_kwargs[\"truncation\"] = \"only_second\"\n",
    "    features = tokenizer(\n",
    "        input, return_tensors=\"pt\", **preprocessing_kwargs\n",
    "    )\n",
    "    predictions, features = base_predict(model, input, tokenizer, preprocessing_kwargs, model_kwargs, output_features=True)\n",
    "\n",
    "    task_outputs = {\n",
    "        \"answers\": [],\n",
    "        \"attributions\": [],\n",
    "        \"adversarial\": {\n",
    "            \"indices\": [],\n",
    "        },  # for hotflip, input_reduction and topk\n",
    "    }\n",
    "\n",
    "    for idx, (start, end, (_, context)) in enumerate(\n",
    "            zip(predictions[\"start_logits\"], predictions[\"end_logits\"], input)\n",
    "    ):\n",
    "        # Ensure padded tokens & question tokens cannot\n",
    "        # belong to the set of candidate answers.\n",
    "        question_tokens = np.abs(np.array([s != 1 for s in features.sequence_ids(idx)]) - 1)\n",
    "        # Unmask CLS token for \"no answer\"\n",
    "        question_tokens[0] = 1\n",
    "        undesired_tokens = question_tokens & features[\"attention_mask\"][idx].numpy()\n",
    "\n",
    "        # Generate mask\n",
    "        undesired_tokens_mask = undesired_tokens == 0.0\n",
    "\n",
    "        # Make sure non-context indexes in the tensor cannot\n",
    "        # contribute to the softmax\n",
    "        start = np.where(undesired_tokens_mask, -10000.0, start)\n",
    "        end = np.where(undesired_tokens_mask, -10000.0, end)\n",
    "\n",
    "        start = np.exp(start - np.log(np.sum(np.exp(start), axis=-1, keepdims=True)))\n",
    "        end = np.exp(end - np.log(np.sum(np.exp(end), axis=-1, keepdims=True)))\n",
    "\n",
    "        # Get score for \"no answer\" then mask for decoding step (CLS token\n",
    "        no_answer_score = (start[0] * end[0]).item()\n",
    "        start[0] = end[0] = 0.0\n",
    "\n",
    "        starts, ends, scores = decode(\n",
    "            start,\n",
    "            end,\n",
    "            task_kwargs.get(\"topk\", 1),\n",
    "            task_kwargs.get(\"max_answer_len\", 128),\n",
    "            undesired_tokens,\n",
    "        )\n",
    "\n",
    "        enc = features[idx]\n",
    "        original_ans_start = enc.token_to_word(starts[0])\n",
    "        original_ans_end = enc.token_to_word(ends[0])\n",
    "        answers = [\n",
    "            {\n",
    "                \"score\": score.item(),\n",
    "                \"start\": enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0],\n",
    "                \"end\": enc.word_to_chars(enc.token_to_word(e), sequence_index=1)[1],\n",
    "                \"answer\": context[\n",
    "                            enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0]: enc.word_to_chars(\n",
    "                                enc.token_to_word(e), sequence_index=1\n",
    "                            )[1]\n",
    "                            ],\n",
    "            }\n",
    "            for s, e, score in zip(starts, ends, scores)\n",
    "        ]\n",
    "        if task_kwargs.get(\"show_null_answers\", True):\n",
    "            answers.append({\"score\": no_answer_score, \"start\": 0, \"end\": 0, \"answer\": \"\"})\n",
    "        answers = sorted(answers, key=lambda x: x[\"score\"], reverse=True)[: task_kwargs.get(\"topk\", 1)]\n",
    "        task_outputs[\"answers\"].append(answers)\n",
    "\n",
    "    return predictions, task_outputs, original_ans_start, original_ans_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from SQuARE ONNX QA Pipeline (note: some features like explainability and attack mode have been removed)\n",
    "def question_answering(model_qa, tokenizer, input, preprocessing_kwargs, task_kwargs, model_kwargs):\n",
    "    \"\"\"\n",
    "    Span-based question answering for a given question and context.\n",
    "    We expect the input to use the (question, context) format for the text pairs.\n",
    "    Args:\n",
    "        request: the prediction request\n",
    "    \"\"\"    \n",
    "    preprocessing_kwargs[\"truncation\"] = \"only_second\"\n",
    "\n",
    "    features = tokenizer(\n",
    "        input, return_tensors=\"np\", **preprocessing_kwargs\n",
    "    )\n",
    "    onnx_inputs = {key: np.array(features[key], dtype=np.int64) for key in features}\n",
    "    \n",
    "    predictions_onnx = model_qa.run(input_feed=onnx_inputs, output_names=None)\n",
    "    predictions = {\n",
    "        \"start_logits\": predictions_onnx[0],\n",
    "        \"end_logits\": predictions_onnx[1]\n",
    "    }\n",
    "\n",
    "    task_outputs = {\n",
    "        \"answers\": [],\n",
    "        \"attributions\": [],\n",
    "        \"adversarial\": {\n",
    "            \"indices\": [],\n",
    "        },  # for hotflip, input_reduction and topk\n",
    "    }\n",
    "\n",
    "    for idx, (start, end, (_, context)) in enumerate(\n",
    "            zip(predictions[\"start_logits\"], predictions[\"end_logits\"], input)\n",
    "    ):\n",
    "        # Ensure padded tokens & question tokens cannot\n",
    "        # belong to the set of candidate answers.\n",
    "        question_tokens = np.abs(np.array([s != 1 for s in features.sequence_ids(idx)]) - 1)\n",
    "        # Unmask CLS token for \"no answer\"\n",
    "        question_tokens[0] = 1\n",
    "        undesired_tokens = question_tokens & features[\"attention_mask\"][idx]\n",
    "\n",
    "        # Generate mask\n",
    "        undesired_tokens_mask = undesired_tokens == 0.0\n",
    "\n",
    "        # Make sure non-context indexes in the tensor cannot\n",
    "        # contribute to the softmax\n",
    "        start = np.where(undesired_tokens_mask, -10000.0, start)\n",
    "        end = np.where(undesired_tokens_mask, -10000.0, end)\n",
    "\n",
    "        start = np.exp(start - np.log(np.sum(np.exp(start), axis=-1, keepdims=True)))\n",
    "        end = np.exp(end - np.log(np.sum(np.exp(end), axis=-1, keepdims=True)))\n",
    "\n",
    "        # Get score for \"no answer\" then mask for decoding step (CLS token\n",
    "        no_answer_score = (start[0] * end[0]).item()\n",
    "        start[0] = end[0] = 0.0\n",
    "\n",
    "        starts, ends, scores = decode(\n",
    "            start,\n",
    "            end,\n",
    "            task_kwargs.get(\"topk\", 1),\n",
    "            task_kwargs.get(\"max_answer_len\", 128),\n",
    "            undesired_tokens,\n",
    "        )\n",
    "\n",
    "        enc = features[idx]\n",
    "        original_ans_start = enc.token_to_word(starts[0])\n",
    "        original_ans_end = enc.token_to_word(ends[0])\n",
    "        answers = [\n",
    "            {\n",
    "                \"score\": score.item(),\n",
    "                \"start\": enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0],\n",
    "                \"end\": enc.word_to_chars(enc.token_to_word(e), sequence_index=1)[1],\n",
    "                \"answer\": context[\n",
    "                            enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0]: enc.word_to_chars(\n",
    "                                enc.token_to_word(e), sequence_index=1\n",
    "                            )[1]\n",
    "                            ],\n",
    "            }\n",
    "            for s, e, score in zip(starts, ends, scores)\n",
    "        ]\n",
    "        if task_kwargs.get(\"show_null_answers\", True):\n",
    "            answers.append({\"score\": no_answer_score, \"start\": 0, \"end\": 0, \"answer\": \"\"})\n",
    "        answers = sorted(answers, key=lambda x: x[\"score\"], reverse=True)[: task_kwargs.get(\"topk\", 1)]\n",
    "        task_outputs[\"answers\"].append(answers)\n",
    "\n",
    "    return predictions, task_outputs, original_ans_start, original_ans_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_kwargs = {\"padding\": True, \"truncation\": True}\n",
    "\n",
    "task_kwargs = {\"show_null_answers\": False, \"topk\": 1, \"max_answer_len\": 128}\n",
    "\n",
    "model_kwargs = {\"\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_onnx, model_onnx_quant, as_list=False):\n",
    "    local_onnx_model = onnxruntime.InferenceSession(model_onnx, providers=[\"CPUExecutionProvider\"])\n",
    "    local_onnx_model_quant = onnxruntime.InferenceSession(model_onnx_quant, providers=[\"CPUExecutionProvider\"])\n",
    "    \n",
    "    so = onnxruntime.SessionOptions()\n",
    "    so.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    \n",
    "    local_onnx_model_opt = onnxruntime.InferenceSession(model_onnx, so)\n",
    "    local_onnx_model_quant_opt = onnxruntime.InferenceSession(model_onnx_quant, so)\n",
    "    \n",
    "    if as_list:\n",
    "        return [local_onnx_model, local_onnx_model_opt, local_onnx_model_quant, local_onnx_model_quant_opt]\n",
    "    return local_onnx_model, local_onnx_model_opt, local_onnx_model_quant, local_onnx_model_quant_opt\n",
    "\n",
    "def repo_builder(reader, adapter):\n",
    "    repo_id = f\"UKP-SQuARE/{reader}-pf-{adapter}-onnx\"\n",
    "    filename_onnx = \"model.onnx\"\n",
    "    filename_onnx_quant = \"model_quant.onnx\"\n",
    "\n",
    "    model_onnx = hf_hub_download(repo_id=repo_id, filename=filename_onnx)\n",
    "    model_onnx_quant = hf_hub_download(repo_id=repo_id, filename=filename_onnx_quant)\n",
    "\n",
    "    return model_onnx, model_onnx_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_torch(model, inputs):\n",
    "#     with torch.no_grad():\n",
    "#         model(**inputs)\n",
    "\n",
    "# def run_onnx(qa_model, onnx_inputs):\n",
    "#     qa_model.run(output_names=[\"start_logits\", \"end_logits\"], input_feed=dict(onnx_inputs))   \n",
    "\n",
    "# def get_time_duration(func, model, inputs): \n",
    "#     st= time.time()\n",
    "#     func(model, inputs)\n",
    "#     et = time.time()\n",
    "#     return 1000 * (et - st)\n",
    "\n",
    "def save_df(df_new, path_to_logger_file = \"logger_all.csv\"):\n",
    "\n",
    "    if os.path.exists(path_to_logger_file):\n",
    "        df_fin = pd.concat([pd.read_csv(path_to_logger_file), df_new])\n",
    "        df_fin.to_csv(path_to_logger_file,index=False)\n",
    "    else: \n",
    "        df_new.to_csv(path_to_logger_file,index=False)\n",
    "\n",
    "# def measure_time(perf_type, tokenizer, question, context, model):\n",
    "#     if perf_type == \"base\":\n",
    "#         inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True)\n",
    "#         mode = run_torch\n",
    "#         # time_once = get_time_duration(run_torch, model, inputs)\n",
    "    \n",
    "#     elif perf_type == \"seq_length\":\n",
    "#         inputs = tokenizer(question, context, return_tensors=\"np\", truncation=True)\n",
    "#         inputs = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\n",
    "#         mode = run_onnx\n",
    "#         # time_once = get_time_duration(run_onnx, model, inputs) \n",
    "    \n",
    "#     time_once = get_time_duration(mode, model, inputs) \n",
    "\n",
    "#     return time_once\n",
    "\n",
    "# def performance_log(perf_type, name, model, tokenizer, data, data_intervall = 0): \n",
    "#     df = pd.DataFrame(columns=[\"model_name\", \"time once (ms)\", \"average_time 50 times (ms)\", \"seq_length\", \"context\", \"question\", \"data_id\"])\n",
    "    \n",
    "#     for i in range(0, len(data[\"context\"]), data_intervall):\n",
    "#         context = data[\"context\"][i]\n",
    "#         question = data[\"question\"][i]\n",
    "#         time_duration = measure_time(perf_type, tokenizer, question, context, model)\n",
    "        \n",
    "#         seq_length = len(context.split()) # TODO -> reduce stopwords? Real Tokenization?\n",
    "        \n",
    "#         df.loc[len(df)] = [name, time_duration, \"\", seq_length, context, question, data[\"id\"][i]]\n",
    "        \n",
    "#         print(\"Model: {}, Input Length {}: {:.3f} ms\".format(name, seq_length, time_duration))\n",
    "#     save_df(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate all extractive qa model on squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/Users/michaelhermann/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    }
   ],
   "source": [
    "data_set_name = \"squad\"\n",
    "data = load_dataset(data_set_name, split=\"validation[:100]\")\n",
    "metric = evaluate.load(data_set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_evaluate(inference_func, model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs):\n",
    "    examples = list(zip(data[\"question\"], data[\"context\"]))\n",
    "    predictions = []\n",
    "    for example in examples:\n",
    "        _, task_outputs, _, _ = inference_func(model, tokenizer, [example], preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "        predictions.append(task_outputs[\"answers\"][0][0][\"answer\"])\n",
    "    \n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in zip(data[\"id\"], predictions)]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in data]\n",
    "    \n",
    "    score = metric.compute(predictions=formatted_predictions, references=references)\n",
    "\n",
    "    return score[\"f1\"], score[\"exact_match\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "    print(f\"Loading: {reader} {adapter}\")\n",
    "\n",
    "    #load base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(reader)\n",
    "    default_model = AutoModelWithHeads.from_pretrained(reader)\n",
    "    adapter_name = default_model.load_adapter(f\"AdapterHub/{reader}-pf-{adapter}\", source=\"hf\")\n",
    "    default_model.active_adapters = adapter_name\n",
    "\n",
    "    # Test acc for base model\n",
    "    f1, exact = squad_evaluate(base_qa, default_model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "    result.append((\"Base\", skill, reader, adapter, f1, exact, data_set_name))\n",
    "\n",
    "    #load onnx models\n",
    "    model_onnx, model_onnx_quant = repo_builder(reader, adapter)\n",
    "    onnx_models_list = load_model(model_onnx, model_onnx_quant, as_list=True)\n",
    "    onnx_models_name_helper_list = [\"ONNX\", \"ONNX-OPT\", \"Quantized ONNX\", \"Quantized ONNX - OPT\"]\n",
    "\n",
    "    # Test acc for onnx models\n",
    "    for onnx_model, onnx_model_name in zip(onnx_models_list, onnx_models_name_helper_list):\n",
    "        f1, exact = squad_evaluate(question_answering, onnx_model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "        result.append((onnx_model_name, skill, reader, adapter, f1, exact, data_set_name))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, columns=[\"name\", \"skill\", \"reader\", \"adapter\", \"f1\", \"exact\", \"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(df, \"accuracy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate extractive qa model on specific adapter - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = []\n",
    "# for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "#     print(f\"Loading: {reader} {adapter}\")\n",
    "\n",
    "#     #load adapter specific dataset\n",
    "#     data_set_name = adapter\n",
    "#     if data_set_name in [\"newsqa\", \"hotpot_qa\"]:\n",
    "#         continue\n",
    "#     else: \n",
    "#         data = load_dataset(data_set_name, split=\"validation[:100]\")\n",
    "\n",
    "#     metric = evaluate.load(data_set_name)\n",
    "\n",
    "#     #load base model\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(reader)\n",
    "#     default_model = AutoModelWithHeads.from_pretrained(reader)\n",
    "#     adapter_name = default_model.load_adapter(f\"AdapterHub/{reader}-pf-{adapter}\", source=\"hf\")\n",
    "#     default_model.active_adapters = adapter_name\n",
    "\n",
    "#     # Test acc for base model\n",
    "#     f1, exact = squad_evaluate(base_qa, default_model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "#     result.append((\"Base\", skill, reader, adapter, f1, exact, data_set_name))\n",
    "\n",
    "#     #load onnx models\n",
    "#     model_onnx, model_onnx_quant = repo_builder(reader, adapter)\n",
    "#     onnx_models_list = load_model(model_onnx, model_onnx_quant, as_list=True)\n",
    "#     onnx_models_name_helper_list = [\"ONNX\", \"ONNX-OPT\", \"Quantized ONNX\", \"Quantized ONNX - OPT\"]\n",
    "\n",
    "#     # Test acc for onnx models\n",
    "#     for onnx_model, onnx_model_name in zip(onnx_models_list, onnx_models_name_helper_list):\n",
    "#         f1, exact = squad_evaluate(question_answering, onnx_model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "#         result.append((onnx_model_name, skill, reader, adapter, f1, exact, data_set_name))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, columns=[\"name\", \"skill\", \"reader\", \"adapter\", \"f1\", \"exact\", \"dataset\"])\n",
    "\n",
    "save_df(df, \"accuracy.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate categorical qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = pd.read_csv(\"square_skills/impl_skills.csv\")\n",
    "skill = \"categorical\"\n",
    "skills = load_skills(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset boolq (/Users/michaelhermann/.cache/huggingface/datasets/boolq/default/0.1.0/bf0dd57da941c50de94ae3ce3cef7fea48c08f337a4b7aac484e9dddc5aa24e5)\n"
     ]
    }
   ],
   "source": [
    "data_set_name = \"boolq\"\n",
    "data = load_dataset(data_set_name, split=\"validation[:100]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 4100.67it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "adapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-boolq\", source=\"hf\")\n",
    "model.active_adapters = adapter_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_base_inference(model, tokenizer, question, context):\n",
    "    \n",
    "    raw_input = [[context, question]]\n",
    "    inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    answer_idx = torch.argmax(outputs.logits)\n",
    "\n",
    "    return bool(answer_idx)\n",
    "    \n",
    "def onnx_inference(onnx_model, tokenizer, question, context):\n",
    "\n",
    "    inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors=\"np\")\n",
    "    inputs = {key: np.array(inputs[key], dtype=np.int64) for key in inputs}\n",
    "\n",
    "    outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\n",
    "\n",
    "    return bool(np.argmax(outputs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolq_evaluate(model, tokenizer, data, inference_type):\n",
    "    correct = 0\n",
    "    for test_no in range(len(data)):\n",
    "        question = data[test_no][\"question\"]\n",
    "        correct_answer = data[test_no][\"answer\"]\n",
    "        context = data[test_no][\"passage\"]\n",
    "\n",
    "        answer = inference_type(model, tokenizer, question, context)\n",
    "\n",
    "        if answer == correct_answer:\n",
    "            correct += 1\n",
    "\n",
    "    return correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: bert-base-uncased boolq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/bert/adapter_model.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 2799.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: roberta-base boolq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 5870.26it/s]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for reader, adapter in zip(skills[\"Reader Model\"], skills[\"Reader Adapter\"]):\n",
    "    print(f\"Loading: {reader} {adapter}\")\n",
    "\n",
    "    #load base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(reader)\n",
    "    default_model = AutoModelWithHeads.from_pretrained(reader)\n",
    "    adapter_name = default_model.load_adapter(f\"AdapterHub/{reader}-pf-{adapter}\", source=\"hf\")\n",
    "    default_model.active_adapters = adapter_name\n",
    "\n",
    "    # Test acc for base model\n",
    "    exact = boolq_evaluate(default_model, tokenizer, data, categorical_base_inference)\n",
    "    result.append((\"Base\", skill, reader, adapter, \"\", exact, data_set_name))\n",
    "\n",
    "    #load onnx models\n",
    "    model_onnx, model_onnx_quant = repo_builder(reader, adapter)\n",
    "    onnx_models_list = load_model(model_onnx, model_onnx_quant, as_list=True)\n",
    "    onnx_models_name_helper_list = [\"ONNX\", \"ONNX-OPT\", \"Quantized ONNX\", \"Quantized ONNX - OPT\"]\n",
    "\n",
    "\n",
    "    for onnx_model, onnx_model_name in zip(onnx_models_list, onnx_models_name_helper_list):\n",
    "        exact = boolq_evaluate(onnx_model, tokenizer, data, onnx_inference)\n",
    "        result.append((onnx_model_name, skill, reader, adapter, \"\", exact, data_set_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, columns=[\"name\", \"skill\", \"reader\", \"adapter\", \"f1\", \"exact\", \"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>skill</th>\n",
       "      <th>reader</th>\n",
       "      <th>adapter</th>\n",
       "      <th>f1</th>\n",
       "      <th>exact</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>categorical</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>boolq</td>\n",
       "      <td></td>\n",
       "      <td>0.73</td>\n",
       "      <td>boolq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ONNX</td>\n",
       "      <td>categorical</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>boolq</td>\n",
       "      <td></td>\n",
       "      <td>0.66</td>\n",
       "      <td>boolq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ONNX-OPT</td>\n",
       "      <td>categorical</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>boolq</td>\n",
       "      <td></td>\n",
       "      <td>0.66</td>\n",
       "      <td>boolq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quantized ONNX</td>\n",
       "      <td>categorical</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>boolq</td>\n",
       "      <td></td>\n",
       "      <td>0.47</td>\n",
       "      <td>boolq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantized ONNX - OPT</td>\n",
       "      <td>categorical</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>boolq</td>\n",
       "      <td></td>\n",
       "      <td>0.47</td>\n",
       "      <td>boolq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Base</td>\n",
       "      <td>categorical</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>boolq</td>\n",
       "      <td></td>\n",
       "      <td>0.78</td>\n",
       "      <td>boolq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ONNX</td>\n",
       "      <td>categorical</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>boolq</td>\n",
       "      <td></td>\n",
       "      <td>0.75</td>\n",
       "      <td>boolq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ONNX-OPT</td>\n",
       "      <td>categorical</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>boolq</td>\n",
       "      <td></td>\n",
       "      <td>0.75</td>\n",
       "      <td>boolq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Quantized ONNX</td>\n",
       "      <td>categorical</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>boolq</td>\n",
       "      <td></td>\n",
       "      <td>0.74</td>\n",
       "      <td>boolq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Quantized ONNX - OPT</td>\n",
       "      <td>categorical</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>boolq</td>\n",
       "      <td></td>\n",
       "      <td>0.74</td>\n",
       "      <td>boolq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name        skill             reader adapter f1  exact  \\\n",
       "0                  Base  categorical  bert-base-uncased   boolq      0.73   \n",
       "1                  ONNX  categorical  bert-base-uncased   boolq      0.66   \n",
       "2              ONNX-OPT  categorical  bert-base-uncased   boolq      0.66   \n",
       "3        Quantized ONNX  categorical  bert-base-uncased   boolq      0.47   \n",
       "4  Quantized ONNX - OPT  categorical  bert-base-uncased   boolq      0.47   \n",
       "5                  Base  categorical       roberta-base   boolq      0.78   \n",
       "6                  ONNX  categorical       roberta-base   boolq      0.75   \n",
       "7              ONNX-OPT  categorical       roberta-base   boolq      0.75   \n",
       "8        Quantized ONNX  categorical       roberta-base   boolq      0.74   \n",
       "9  Quantized ONNX - OPT  categorical       roberta-base   boolq      0.74   \n",
       "\n",
       "  dataset  \n",
       "0   boolq  \n",
       "1   boolq  \n",
       "2   boolq  \n",
       "3   boolq  \n",
       "4   boolq  \n",
       "5   boolq  \n",
       "6   boolq  \n",
       "7   boolq  \n",
       "8   boolq  \n",
       "9   boolq  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(df, \"accuracy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapterhub_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dff476688330005cfc33f1ee0f15c13ae533c265ccd041ab146cdb98ecc6219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
