{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive QA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AutoAdapterModel' from 'transformers' (/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mevaluate\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m QuestionAnsweringPipeline, AutoAdapterModel, AutoModelWithHeads, AutoTokenizer, AutoConfig\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m OnnxConfig, validate_model_outputs, export\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbert\u001b[39;00m \u001b[39mimport\u001b[39;00m BertOnnxConfig\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AutoAdapterModel' from 'transformers' (/Users/michaelhermann/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import QuestionAnsweringPipeline, AutoAdapterModel, AutoModelWithHeads, AutoTokenizer, AutoConfig\n",
    "from transformers.onnx import OnnxConfig, validate_model_outputs, export\n",
    "from transformers.models.bert import BertOnnxConfig\n",
    "\n",
    "import onnx\n",
    "# from onnxruntime.quantization import quantize_dynamic, QuantType #import error\n",
    "from onnxruntime import InferenceSession\n",
    "# from onnx_opcounter import calculate_params\n",
    "\n",
    "import os\n",
    "import time\n",
    "from typing import Mapping, OrderedDict, Tuple, Union\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(question, context, tokenizer):\n",
    "    inputs = tokenizer(question, context, padding=True, truncation=True, return_tensors=\"np\")\n",
    "    return inputs\n",
    "\n",
    "def postprocessing(outputs, inputs, tokenizer):\n",
    "    start_scores = outputs[0]\n",
    "    end_scores = outputs[1]\n",
    "    ans_start = np.argmax(start_scores)\n",
    "    ans_end = np.argmax(end_scores)+1\n",
    "    return tokenizer.decode(inputs['input_ids'][0, ans_start:ans_end])\n",
    "\n",
    "def onnx_inference(onnx_path, tokenizer, question, context):\n",
    "    onnx_model = get_onnx_model(onnx_path)\n",
    "\n",
    "    inputs = preprocessing(question, context, tokenizer)\n",
    "    outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)\n",
    "    answer = postprocessing(outputs, inputs, tokenizer)\n",
    "    return answer\n",
    "\n",
    "def get_onnx_model(onnx_path):\n",
    "    return InferenceSession(\n",
    "        str(onnx_path), providers=[\"CPUExecutionProvider\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interoperability of frameworks and hardware optimization\n"
     ]
    }
   ],
   "source": [
    "# API Parameters\n",
    "base_model = 'bert-base-uncased'\n",
    "head = 'AdapterHub/bert-base-uncased-pf-drop'\n",
    "context = 'ONNX is an open format built to represent machine learning models. The key benefits of using ONNX are interoperability of frameworks and HARDware optimization.'\n",
    "question = 'What are advantages of ONNX?'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# TODO Replace hardcoded string with directory structure\n",
    "# onnx_path = \"/\".join(\"onnx\", base_model, head + \".onnx\")\n",
    "onnx_path = Path(\"onnx/dropbert/model.onnx\")\n",
    "answer = onnx_inference(onnx_path, tokenizer, question, context)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(\n",
    "            start_: np.ndarray,\n",
    "            end_: np.ndarray,\n",
    "            topk: int,\n",
    "            max_answer_len: int,\n",
    "            undesired_tokens_: np.ndarray,\n",
    "    ) -> Tuple:\n",
    "    \"\"\"\n",
    "    Take the output of any :obj:`ModelForQuestionAnswering` and\n",
    "        will generate probabilities for each span to be the\n",
    "        actual answer.\n",
    "    In addition, it filters out some unwanted/impossible cases\n",
    "    like answer len being greater than max_answer_len or\n",
    "    answer end position being before the starting position.\n",
    "    The method supports output the k-best answer through\n",
    "    the topk argument.\n",
    "    Args:\n",
    "        start_ (:obj:`np.ndarray`): Individual start\n",
    "            probabilities for each token.\n",
    "        end (:obj:`np.ndarray`): Individual end_ probabilities\n",
    "            for each token.\n",
    "        topk (:obj:`int`): Indicates how many possible answer\n",
    "            span(s) to extract from the model output.\n",
    "        max_answer_len (:obj:`int`): Maximum size of the answer\n",
    "            to extract from the model's output.\n",
    "        undesired_tokens_ (:obj:`np.ndarray`): Mask determining\n",
    "            tokens that can be part of the answer\n",
    "    \"\"\"\n",
    "    # Ensure we have batch axis\n",
    "    if start_.ndim == 1:\n",
    "        start_ = start_[None]\n",
    "\n",
    "    if end_.ndim == 1:\n",
    "        end_ = end_[None]\n",
    "\n",
    "    # Compute the score of each tuple(start_, end_) to be the real answer\n",
    "    outer = np.matmul(np.expand_dims(start_, -1), np.expand_dims(end_, 1))\n",
    "\n",
    "    # Remove candidate with end_ < start_ and end_ - start_ > max_answer_len\n",
    "    candidates = np.tril(np.triu(outer), max_answer_len - 1)\n",
    "\n",
    "    #  Inspired by Chen & al. (https://github.com/facebookresearch/DrQA)\n",
    "    scores_flat = candidates.flatten()\n",
    "    if topk == 1:\n",
    "        idx_sort = [np.argmax(scores_flat)]\n",
    "    elif len(scores_flat) < topk:\n",
    "        idx_sort = np.argsort(-scores_flat)\n",
    "    else:\n",
    "        idx = np.argpartition(-scores_flat, topk)[0:topk]\n",
    "        idx_sort = idx[np.argsort(-scores_flat[idx])]\n",
    "\n",
    "    starts_, ends_ = np.unravel_index(idx_sort, candidates.shape)[1:]\n",
    "    desired_spans = np.isin(starts_, undesired_tokens_.nonzero()) & np.isin(\n",
    "        ends_, undesired_tokens_.nonzero()\n",
    "    )\n",
    "    starts_ = starts_[desired_spans]\n",
    "    ends_ = ends_[desired_spans]\n",
    "    scores_ = candidates[0, starts_, ends_]\n",
    "\n",
    "    return starts_, ends_, scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from SQuARE ONNX QA Pipeline (note: some features like explainability and attack mode have been removed)\n",
    "def question_answering(model_path, tokenizer, input, preprocessing_kwargs, task_kwargs):\n",
    "    \"\"\"\n",
    "    Span-based question answering for a given question and context.\n",
    "    We expect the input to use the (question, context) format for the text pairs.\n",
    "    Args:\n",
    "        request: the prediction request\n",
    "    \"\"\"    \n",
    "    model_qa = InferenceSession(\n",
    "        str(model_path), providers=[\"CPUExecutionProvider\"]\n",
    "    )\n",
    "    \n",
    "    preprocessing_kwargs[\"truncation\"] = \"only_second\"\n",
    "\n",
    "    features = tokenizer(\n",
    "        input, return_tensors=\"np\", **preprocessing_kwargs\n",
    "    )\n",
    "\n",
    "    predictions_onnx = model_qa.run(input_feed=dict(features), output_names=None)\n",
    "    predictions = {\n",
    "        \"start_logits\": predictions_onnx[0],\n",
    "        \"end_logits\": predictions_onnx[1]\n",
    "    }\n",
    "\n",
    "    task_outputs = {\n",
    "        \"answers\": [],\n",
    "        \"attributions\": [],\n",
    "        \"adversarial\": {\n",
    "            \"indices\": [],\n",
    "        },  # for hotflip, input_reduction and topk\n",
    "    }\n",
    "\n",
    "    for idx, (start, end, (_, context)) in enumerate(\n",
    "            zip(predictions[\"start_logits\"], predictions[\"end_logits\"], input)\n",
    "    ):\n",
    "        # Ensure padded tokens & question tokens cannot\n",
    "        # belong to the set of candidate answers.\n",
    "        question_tokens = np.abs(np.array([s != 1 for s in features.sequence_ids(idx)]) - 1)\n",
    "        # Unmask CLS token for 'no answer'\n",
    "        question_tokens[0] = 1\n",
    "        undesired_tokens = question_tokens & features[\"attention_mask\"][idx]\n",
    "\n",
    "        # Generate mask\n",
    "        undesired_tokens_mask = undesired_tokens == 0.0\n",
    "\n",
    "        # Make sure non-context indexes in the tensor cannot\n",
    "        # contribute to the softmax\n",
    "        start = np.where(undesired_tokens_mask, -10000.0, start)\n",
    "        end = np.where(undesired_tokens_mask, -10000.0, end)\n",
    "\n",
    "        start = np.exp(start - np.log(np.sum(np.exp(start), axis=-1, keepdims=True)))\n",
    "        end = np.exp(end - np.log(np.sum(np.exp(end), axis=-1, keepdims=True)))\n",
    "\n",
    "        # Get score for 'no answer' then mask for decoding step (CLS token\n",
    "        no_answer_score = (start[0] * end[0]).item()\n",
    "        start[0] = end[0] = 0.0\n",
    "\n",
    "        starts, ends, scores = decode(\n",
    "            start,\n",
    "            end,\n",
    "            task_kwargs.get(\"topk\", 1),\n",
    "            task_kwargs.get(\"max_answer_len\", 128),\n",
    "            undesired_tokens,\n",
    "        )\n",
    "\n",
    "        enc = features[idx]\n",
    "        original_ans_start = enc.token_to_word(starts[0])\n",
    "        original_ans_end = enc.token_to_word(ends[0])\n",
    "        answers = [\n",
    "            {\n",
    "                \"score\": score.item(),\n",
    "                \"start\": enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0],\n",
    "                \"end\": enc.word_to_chars(enc.token_to_word(e), sequence_index=1)[1],\n",
    "                \"answer\": context[\n",
    "                            enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0]: enc.word_to_chars(\n",
    "                                enc.token_to_word(e), sequence_index=1\n",
    "                            )[1]\n",
    "                            ],\n",
    "            }\n",
    "            for s, e, score in zip(starts, ends, scores)\n",
    "        ]\n",
    "        if task_kwargs.get(\"show_null_answers\", True):\n",
    "            answers.append({\"score\": no_answer_score, \"start\": 0, \"end\": 0, \"answer\": \"\"})\n",
    "        answers = sorted(answers, key=lambda x: x[\"score\"], reverse=True)[: task_kwargs.get(\"topk\", 1)]\n",
    "        task_outputs[\"answers\"].append(answers)\n",
    "\n",
    "    return predictions, task_outputs, original_ans_start, original_ans_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_inference(model_path, tokenizer, input, preprocessing_kwargs, task_kwargs):\n",
    "    \"\"\"\n",
    "    Span-based question answering for a given question and context.\n",
    "    We expect the input to use the (question, context) format for the text pairs.\n",
    "    Args:\n",
    "        request: the prediction request\n",
    "    \"\"\"    \n",
    "    model_qa = InferenceSession(\n",
    "        str(model_path), providers=[\"CPUExecutionProvider\"]\n",
    "    )\n",
    "    \n",
    "    preprocessing_kwargs[\"truncation\"] = \"only_second\"\n",
    "\n",
    "    task_outputs = {\n",
    "        \"answers\": [],\n",
    "        \"attributions\": [],\n",
    "        \"adversarial\": {\n",
    "            \"indices\": [],\n",
    "        },  # for hotflip, input_reduction and topk\n",
    "    }\n",
    "\n",
    "    for example in input:\n",
    "        features = tokenizer(\n",
    "            [example], return_tensors=\"np\", **preprocessing_kwargs\n",
    "        )\n",
    "\n",
    "        predictions_onnx = model_qa.run(input_feed=dict(features), output_names=None)\n",
    "        predictions = {\n",
    "            \"start_logits\": predictions_onnx[0],\n",
    "            \"end_logits\": predictions_onnx[1]\n",
    "        }\n",
    "\n",
    "        for idx, (start, end) in enumerate(\n",
    "                zip(predictions[\"start_logits\"], predictions[\"end_logits\"])\n",
    "        ):\n",
    "            context = example[1]\n",
    "\n",
    "            # Ensure padded tokens & question tokens cannot\n",
    "            # belong to the set of candidate answers.\n",
    "            question_tokens = np.abs(np.array([s != 1 for s in features.sequence_ids(idx)]) - 1)\n",
    "            # Unmask CLS token for 'no answer'\n",
    "            question_tokens[0] = 1\n",
    "            undesired_tokens = question_tokens & features[\"attention_mask\"][idx]\n",
    "\n",
    "            # Generate mask\n",
    "            undesired_tokens_mask = undesired_tokens == 0.0\n",
    "\n",
    "            # Make sure non-context indexes in the tensor cannot\n",
    "            # contribute to the softmax\n",
    "            start = np.where(undesired_tokens_mask, -10000.0, start)\n",
    "            end = np.where(undesired_tokens_mask, -10000.0, end)\n",
    "\n",
    "            start = np.exp(start - np.log(np.sum(np.exp(start), axis=-1, keepdims=True)))\n",
    "            end = np.exp(end - np.log(np.sum(np.exp(end), axis=-1, keepdims=True)))\n",
    "\n",
    "            # Get score for 'no answer' then mask for decoding step (CLS token\n",
    "            no_answer_score = (start[0] * end[0]).item()\n",
    "            start[0] = end[0] = 0.0\n",
    "\n",
    "            starts, ends, scores = decode(\n",
    "                start,\n",
    "                end,\n",
    "                task_kwargs.get(\"topk\", 1),\n",
    "                task_kwargs.get(\"max_answer_len\", 128),\n",
    "                undesired_tokens,\n",
    "            )\n",
    "\n",
    "            enc = features[idx]\n",
    "            original_ans_start = enc.token_to_word(starts[0])\n",
    "            original_ans_end = enc.token_to_word(ends[0])\n",
    "            answers = [\n",
    "                {\n",
    "                    \"score\": score.item(),\n",
    "                    \"start\": enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0],\n",
    "                    \"end\": enc.word_to_chars(enc.token_to_word(e), sequence_index=1)[1],\n",
    "                    \"answer\": context[\n",
    "                                enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0]: enc.word_to_chars(\n",
    "                                    enc.token_to_word(e), sequence_index=1\n",
    "                                )[1]\n",
    "                                ],\n",
    "                }\n",
    "                for s, e, score in zip(starts, ends, scores)\n",
    "            ]\n",
    "            if task_kwargs.get(\"show_null_answers\", True):\n",
    "                answers.append({\"score\": no_answer_score, \"start\": 0, \"end\": 0, \"answer\": \"\"})\n",
    "            answers = sorted(answers, key=lambda x: x[\"score\"], reverse=True)[: task_kwargs.get(\"topk\", 1)]\n",
    "            task_outputs[\"answers\"].append(answers)\n",
    "\n",
    "        # return predictions, task_outputs, original_ans_start, original_ans_end\n",
    "        \n",
    "    return 0, task_outputs, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who was the chancelor of Germany?\n",
      "Angela Merkel (score: 0.24)\n",
      "\n",
      "Who is the current chancelor of Germany?\n",
      "Olaf Scholz (score: 0.29)\n",
      "\n",
      "Whats the name of Angela Merkel's party?\n",
      "CDU (score: 0.51)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessing_kwargs = {\n",
    "    'padding': True, 'truncation': True \n",
    "}\n",
    "\n",
    "task_kwargs = {\n",
    "    \"show_null_answers\": False,\n",
    "    'topk': 1,\n",
    "    'max_answer_len': 128\n",
    "}\n",
    "\n",
    "context = \"Angela Merkel (CDU) was the chancelor of Germany. The current chancelor is Olaf Scholz (SPD).\"\n",
    "inputs = [[\"Who was the chancelor of Germany?\", context], [\"Who is the current chancelor of Germany?\", context], [\"Whats the name of Angela Merkel's party?\", context]]\n",
    "\n",
    "_, task_outputs, _, _ = question_answering(\"onnx/dropbert/model.onnx\", tokenizer, inputs, preprocessing_kwargs, task_kwargs)\n",
    "\n",
    "for i, q in enumerate(task_outputs[\"answers\"]):\n",
    "    print(inputs[i][0])\n",
    "    for a in q:\n",
    "        print(\"%s (score: %.2f)\" % (a['answer'], a['score']))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing(f):\n",
    "    def wrap(*args, **kwargs):\n",
    "        time1 = time.time()\n",
    "        ret = f(*args, **kwargs)\n",
    "        time2 = time.time()\n",
    "        print('{:s} function took {:.3f} ms'.format(f.__name__, (time2-time1)*1000.0))\n",
    "\n",
    "        return ret\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/Users/michaelhermann/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"squad\", split='validation[:100]')\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "@timing\n",
    "def evaluate_em(name, model_path, data, tokenizer, preprocessing_kwargs, task_kwargs):\n",
    "    examples = list(zip(data[\"question\"], data[\"context\"]))\n",
    "    _, task_outputs, _, _ = single_inference(model_path, tokenizer, examples, preprocessing_kwargs, task_kwargs)\n",
    "    \n",
    "    predictions = [output[0][\"answer\"] for output in task_outputs[\"answers\"]]\n",
    "\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in zip(data[\"id\"], predictions)]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in data]\n",
    "    score = metric.compute(predictions=formatted_predictions, references=references)\n",
    "    print(\"{} exact match: {:.1f}%\".format(name, score['exact_match']))\n",
    "    print(\"{} f1: {:.1f}%\".format(name, score['f1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_predict(\n",
    "            model, input, tokenizer, preprocessing_kwargs, model_kwargs, batch_size=1, disable_gpu=True, output_features=False\n",
    "    ) -> Union[dict, Tuple[dict, dict]]:\n",
    "        \"\"\"\n",
    "        Inference on the input.\n",
    "        Args:\n",
    "         request: the request with the input and optional kwargs\n",
    "         output_features: return the features of the input.\n",
    "            Necessary if, e.g., attention mask is needed for post-processing.\n",
    "        Returns:\n",
    "             The model outputs and optionally the input features\n",
    "        \"\"\"\n",
    "\n",
    "        all_predictions = []\n",
    "        preprocessing_kwargs[\"padding\"] = preprocessing_kwargs.get(\n",
    "            \"padding\", True\n",
    "        )\n",
    "        preprocessing_kwargs[\"truncation\"] = preprocessing_kwargs.get(\n",
    "            \"truncation\", True\n",
    "        )\n",
    "        model.to(\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available() and not disable_gpu\n",
    "            else \"cpu\"\n",
    "        )\n",
    "\n",
    "        features = tokenizer(\n",
    "            input, return_tensors=\"pt\", **preprocessing_kwargs\n",
    "        )\n",
    "\n",
    "        for start_idx in range(0, len(input), batch_size):\n",
    "            with torch.no_grad():\n",
    "                input_features = {\n",
    "                    k: features[k][start_idx: start_idx + batch_size]\n",
    "                    for k in features.keys()\n",
    "                }\n",
    "                predictions = model(**input_features, **model_kwargs)\n",
    "                all_predictions.append(predictions)\n",
    "\n",
    "        keys = all_predictions[0].keys()\n",
    "        final_prediction = {}\n",
    "        for key in keys:\n",
    "            # HuggingFace outputs for 'attentions' and more is\n",
    "            # returned as tuple of tensors\n",
    "            # Tuple of tuples only exists for 'past_key_values'\n",
    "            # which is only relevant for generation.\n",
    "            # Generation should NOT use this function\n",
    "            if isinstance(all_predictions[0][key], tuple):\n",
    "                tuple_of_lists = list(\n",
    "                    zip(\n",
    "                        *[\n",
    "                            [\n",
    "                                torch.stack(p).cpu()\n",
    "                                if isinstance(p, tuple)\n",
    "                                else p.cpu()\n",
    "                                for p in tpl[key]\n",
    "                            ]\n",
    "                            for tpl in all_predictions\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "                final_prediction[key] = tuple(torch.cat(l) for l in tuple_of_lists)\n",
    "            else:\n",
    "                final_prediction[key] = torch.cat(\n",
    "                    [p[key].cpu() for p in all_predictions]\n",
    "                )\n",
    "        if output_features:\n",
    "            return final_prediction, features\n",
    "\n",
    "        return final_prediction\n",
    "\n",
    "\n",
    "def base_qa(model, tokenizer, input, preprocessing_kwargs, task_kwargs, model_kwargs):\n",
    "    \"\"\"\n",
    "    Span-based question answering for a given question and context.\n",
    "    We expect the input to use the (question, context) format for the text pairs.\n",
    "    Args:\n",
    "        request: the prediction request\n",
    "    \"\"\"    \n",
    "    preprocessing_kwargs[\"truncation\"] = \"only_second\"\n",
    "    features = tokenizer(\n",
    "        input, return_tensors=\"pt\", **preprocessing_kwargs\n",
    "    )\n",
    "    predictions, features = base_predict(model, input, tokenizer, preprocessing_kwargs, model_kwargs, output_features=True)\n",
    "\n",
    "    task_outputs = {\n",
    "        \"answers\": [],\n",
    "        \"attributions\": [],\n",
    "        \"adversarial\": {\n",
    "            \"indices\": [],\n",
    "        },  # for hotflip, input_reduction and topk\n",
    "    }\n",
    "\n",
    "    for idx, (start, end, (_, context)) in enumerate(\n",
    "            zip(predictions[\"start_logits\"], predictions[\"end_logits\"], input)\n",
    "    ):\n",
    "        # Ensure padded tokens & question tokens cannot\n",
    "        # belong to the set of candidate answers.\n",
    "        question_tokens = np.abs(np.array([s != 1 for s in features.sequence_ids(idx)]) - 1)\n",
    "        # Unmask CLS token for 'no answer'\n",
    "        question_tokens[0] = 1\n",
    "        undesired_tokens = question_tokens & features[\"attention_mask\"][idx].numpy()\n",
    "\n",
    "        # Generate mask\n",
    "        undesired_tokens_mask = undesired_tokens == 0.0\n",
    "\n",
    "        # Make sure non-context indexes in the tensor cannot\n",
    "        # contribute to the softmax\n",
    "        start = np.where(undesired_tokens_mask, -10000.0, start)\n",
    "        end = np.where(undesired_tokens_mask, -10000.0, end)\n",
    "\n",
    "        start = np.exp(start - np.log(np.sum(np.exp(start), axis=-1, keepdims=True)))\n",
    "        end = np.exp(end - np.log(np.sum(np.exp(end), axis=-1, keepdims=True)))\n",
    "\n",
    "        # Get score for 'no answer' then mask for decoding step (CLS token\n",
    "        no_answer_score = (start[0] * end[0]).item()\n",
    "        start[0] = end[0] = 0.0\n",
    "\n",
    "        starts, ends, scores = decode(\n",
    "            start,\n",
    "            end,\n",
    "            task_kwargs.get(\"topk\", 1),\n",
    "            task_kwargs.get(\"max_answer_len\", 128),\n",
    "            undesired_tokens,\n",
    "        )\n",
    "\n",
    "        enc = features[idx]\n",
    "        original_ans_start = enc.token_to_word(starts[0])\n",
    "        original_ans_end = enc.token_to_word(ends[0])\n",
    "        answers = [\n",
    "            {\n",
    "                \"score\": score.item(),\n",
    "                \"start\": enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0],\n",
    "                \"end\": enc.word_to_chars(enc.token_to_word(e), sequence_index=1)[1],\n",
    "                \"answer\": context[\n",
    "                            enc.word_to_chars(enc.token_to_word(s), sequence_index=1)[0]: enc.word_to_chars(\n",
    "                                enc.token_to_word(e), sequence_index=1\n",
    "                            )[1]\n",
    "                            ],\n",
    "            }\n",
    "            for s, e, score in zip(starts, ends, scores)\n",
    "        ]\n",
    "        if task_kwargs.get(\"show_null_answers\", True):\n",
    "            answers.append({\"score\": no_answer_score, \"start\": 0, \"end\": 0, \"answer\": \"\"})\n",
    "        answers = sorted(answers, key=lambda x: x[\"score\"], reverse=True)[: task_kwargs.get(\"topk\", 1)]\n",
    "        task_outputs[\"answers\"].append(answers)\n",
    "\n",
    "    return predictions, task_outputs, original_ans_start, original_ans_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 5633.72it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "adapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-squad\", source=\"hf\")\n",
    "model.active_adapters = adapter_name\n",
    "\n",
    "model_kwargs = {\n",
    "    \"\": {}\n",
    "}\n",
    "\n",
    "\n",
    "@timing\n",
    "def base_evaluate(name, model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs):\n",
    "    examples = list(zip(data[\"question\"], data[\"context\"]))\n",
    "    predictions = []\n",
    "    for example in examples:\n",
    "        _, task_outputs, _, _ = base_qa(model, tokenizer, [example], preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "        predictions.append(task_outputs[\"answers\"][0][0][\"answer\"])\n",
    "\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in zip(data[\"id\"], predictions)]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in data]\n",
    "    score = metric.compute(predictions=formatted_predictions, references=references)\n",
    "    print(\"{} exact match: {:.1f}%\".format(name, score['exact_match']))\n",
    "    print(\"{} f1: {:.1f}%\".format(name, score['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model exact match: 87.0%\n",
      "Base Model f1: 91.9%\n",
      "base_evaluate function took 8676.083 ms\n",
      "\n"
     ]
    },
    {
     "ename": "NoSuchFile",
     "evalue": "[ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from onnx/squadbert/model.onnx failed:Load model onnx/squadbert/model.onnx failed. File doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m base_evaluate(\u001b[39m\"\u001b[39m\u001b[39mBase Model\u001b[39m\u001b[39m\"\u001b[39m, model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[0;32m----> 3\u001b[0m evaluate_em(\u001b[39m\"\u001b[39;49m\u001b[39mONNX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39monnx/squadbert/model.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m, data, tokenizer, preprocessing_kwargs, task_kwargs)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[1;32m      5\u001b[0m evaluate_em(\u001b[39m\"\u001b[39m\u001b[39mQuantized ONNX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39monnx/squadbert/model_quant.onnx\u001b[39m\u001b[39m\"\u001b[39m, data, tokenizer, preprocessing_kwargs, task_kwargs)\n",
      "Cell \u001b[0;32mIn [22], line 4\u001b[0m, in \u001b[0;36mtiming.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m      3\u001b[0m     time1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m     ret \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m      5\u001b[0m     time2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m function took \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, (time2\u001b[39m-\u001b[39mtime1)\u001b[39m*\u001b[39m\u001b[39m1000.0\u001b[39m))\n",
      "Cell \u001b[0;32mIn [23], line 8\u001b[0m, in \u001b[0;36mevaluate_em\u001b[0;34m(name, model_path, data, tokenizer, preprocessing_kwargs, task_kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m@timing\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_em\u001b[39m(name, model_path, data, tokenizer, preprocessing_kwargs, task_kwargs):\n\u001b[1;32m      7\u001b[0m     examples \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(data[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m], data[\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[0;32m----> 8\u001b[0m     _, task_outputs, _, _ \u001b[39m=\u001b[39m single_inference(model_path, tokenizer, examples, preprocessing_kwargs, task_kwargs)\n\u001b[1;32m     10\u001b[0m     predictions \u001b[39m=\u001b[39m [output[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m task_outputs[\u001b[39m\"\u001b[39m\u001b[39manswers\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m     12\u001b[0m     formatted_predictions \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: k, \u001b[39m\"\u001b[39m\u001b[39mprediction_text\u001b[39m\u001b[39m\"\u001b[39m: v} \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(data[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m], predictions)]\n",
      "Cell \u001b[0;32mIn [20], line 8\u001b[0m, in \u001b[0;36msingle_inference\u001b[0;34m(model_path, tokenizer, input, preprocessing_kwargs, task_kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msingle_inference\u001b[39m(model_path, tokenizer, \u001b[39minput\u001b[39m, preprocessing_kwargs, task_kwargs):\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    Span-based question answering for a given question and context.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m    We expect the input to use the (question, context) format for the text pairs.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m        request: the prediction request\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m    \n\u001b[0;32m----> 8\u001b[0m     model_qa \u001b[39m=\u001b[39m InferenceSession(\n\u001b[1;32m      9\u001b[0m         \u001b[39mstr\u001b[39;49m(model_path), providers\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mCPUExecutionProvider\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     preprocessing_kwargs[\u001b[39m\"\u001b[39m\u001b[39mtruncation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39monly_second\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     task_outputs \u001b[39m=\u001b[39m {\n\u001b[1;32m     15\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39manswers\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m     16\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mattributions\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m         },  \u001b[39m# for hotflip, input_reduction and topk\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     }\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:347\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m disabled_optimizers \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mdisabled_optimizers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisabled_optimizers\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_inference_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    348\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m~/Source/DASP/code/dasp_onnx/adapterhub_env/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:384\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    382\u001b[0m session_options \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess_options \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess_options \u001b[39melse\u001b[39;00m C\u001b[39m.\u001b[39mget_default_session_options()\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_path:\n\u001b[0;32m--> 384\u001b[0m     sess \u001b[39m=\u001b[39m C\u001b[39m.\u001b[39;49mInferenceSession(session_options, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model_path, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_config_from_model)\n\u001b[1;32m    385\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     sess \u001b[39m=\u001b[39m C\u001b[39m.\u001b[39mInferenceSession(session_options, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_bytes, \u001b[39mFalse\u001b[39;00m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_config_from_model)\n",
      "\u001b[0;31mNoSuchFile\u001b[0m: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from onnx/squadbert/model.onnx failed:Load model onnx/squadbert/model.onnx failed. File doesn't exist"
     ]
    }
   ],
   "source": [
    "base_evaluate(\"Base Model\", model, data, tokenizer, preprocessing_kwargs, task_kwargs, model_kwargs)\n",
    "print()\n",
    "evaluate_em(\"ONNX\", \"onnx/squadbert/model.onnx\", data, tokenizer, preprocessing_kwargs, task_kwargs)\n",
    "print()\n",
    "evaluate_em(\"Quantized ONNX\", \"onnx/squadbert/model_quant.onnx\", data, tokenizer, preprocessing_kwargs, task_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Data from Laptop as my PC is using GPU during base model inference :s <br>\n",
    "\n",
    "n = 20 Single Inference (CPU)\n",
    "- Base: 90%, 98%, 30s\n",
    "- ONNX: 90%, 98%, 10s\n",
    "- Quantized: 65%, 70%, 8s \n",
    "\n",
    "n=100 Single Inference (CPU)\n",
    "- Base: 87%, 92%, 93s\n",
    "- ONNX: 87%, 92%, 33s\n",
    "- Quantized: 81%, 86%, 27s "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('adapterhub_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dff476688330005cfc33f1ee0f15c13ae533c265ccd041ab146cdb98ecc6219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
